[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "PseudoLab Causal Inference Team",
    "section": "",
    "text": "about\nì•ˆë…•í•˜ì„¸ìš”. ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì…ë‹ˆë‹¤.\në°ì´í„°ë¥¼ í†µí•œ ë¬¸ì œí•´ê²°ë ¥ì„ ë†’ì´ê¸° ìœ„í•´ Causal Inferenceë¥¼ í•¨ê»˜ í•™ìŠµí•˜ê³  ìˆì–´ìš”âœŒï¸\ní•œêµ­ì–´ ìë£Œê°€ ë§ì§€ ì•Šì€ ì¸ê³¼ì¶”ë¡ ì„ ë§ì€ ë¶„ë“¤ì´ ì‰½ê²Œ ì ‘í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ ê¸°ì—¬í•˜ê³ ì í•©ë‹ˆë‹¤!\n\n\n\nì¸ê³¼ì¶”ë¡ íŒ€ ë¹Œë”\n\n\n\nì´ë¦„\nì†Œì†\nì†Œê°œ\n\n\n\n\nì‹ ì§„ìˆ˜\ní¬ë˜í”„í†¤ Data Analyst\nGithub / LinkedIn\n\n\n\n\n\nStudy : ì¸ê³¼ì¶”ë¡  with Entertainment\n\n\n\nì´ë¦„\nì†Œì†\nì†Œê°œ\n\n\n\n\nê¹€ì†Œí¬\ní‹°ë¹™ Data Analyst\nLinkedIn\n\n\nê¹€ì§€ì—°\nì—”ì”¨ì†Œí”„íŠ¸ Data Analyst\n\n\n\në°•ì‹œì˜¨\në„¥ìŠ¨ì½”ë¦¬ì•„ Data Analyst\n\n\n\në°•ë³‘ìˆ˜\në„¥ìŠ¨ì½”ë¦¬ì•„ Data Analyst\nLinkedIn\n\n\në°•ì´ì‚­\ní•˜ì´ë¸ŒIM Data Analyst\nLinkedIn\n\n\nìœ ì •í˜„\në„¥ìŠ¨ì½”ë¦¬ì•„ Data Analyst\nLinkedIn\n\n\nì„ì¢…ì–¸\në„¥ìŠ¨ì½”ë¦¬ì•„ Data Analyst\n\n\n\nì¡°ìŠ¬ì§€\në„·ë§ˆë¸”ì—í”„ì•¤ì”¨ Data Analyst\nLinkedIn\n\n\n\n\n\nStudy : ì¸ê³¼ì¶”ë¡  ë¼ì´ë¸ŒëŸ¬ë¦¬\n\n\n\nì´ë¦„\nì†Œì†\nì†Œê°œ\n\n\n\n\në‚¨ê¶ë¯¼ìƒ\nê°€ì§œì—°êµ¬ì†Œ Data Analyst\n\n\n\në°•ìƒìš°\nê°€ì§œì—°êµ¬ì†Œ Data Analyst\n\n\n\nì´í™ê·œ\nê°€ì§œì—°êµ¬ì†Œ Data Analyst\n\n\n\nì •í˜¸ì¬\në¡¯ë°ìºí”¼íƒˆ Credit Analyst\nLinkedIn\n\n\nì¡°ì¸ì„œ\nê°€ì§œì—°êµ¬ì†Œ Data Analyst\n\n\n\n\n\n\nResearch : ì¸ê³¼ì¶”ë¡  ë…¼ë¬¸ì“°ê¸°\n\n\nTech Support\n\n\n\nì´ë¦„\nì†Œì†\nì†Œê°œ\n\n\n\n\nê¹€ìƒëˆ\në² ê°€ìŠ¤ Data Analyst\n\n\n\nìµœì€í¬\në„¥ìŠ¨ì½”ë¦¬ì•„ Data Analyst\n\n\n\ní™ì„±ì² \nê°€ì§œì—°êµ¬ì†Œ Data Analyst"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Library/causal_inference_library.html",
    "href": "posts/Introduction_to_causal_inference_Library/causal_inference_library.html",
    "title": "01. Causal Inference ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë¦¬",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì…ë‹ˆë‹¤.\në³¸ê²©ì ìœ¼ë¡œ Causal Inference ì±•í„° ê³µë¶€ì— ì•ì„œ, í•™ìŠµì— í•„ìš”í•œ Libraryì™€ íŠœí† ë¦¬ì–¼ì„ ì •ë¦¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤.\nì•ìœ¼ë¡œ Causal Inference ê´€ë ¨ëœ íŒ¨í‚¤ì§€ë¥¼ ê¾¸ì¤€íˆ ì¶”ê°€í•  ì˜ˆì •ì´ë‹ˆ, í˜¹ì‹œë‚˜ í•´ë‹¹ í˜ì´ì§€ì—\nì •ë¦¬ë˜ì§€ ì•Šì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ëŒ“ê¸€ë¡œ ë‹¬ì•„ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤~!Â  (Update 22.06.18)\n\n\n\n\nìˆœì„œ\nì–¸ì–´\në¼ì´ë¸ŒëŸ¬ë¦¬ ëª…\nì„¤ëª… & ë§í¬\n\n\n\n\n1\nPython\ncausality\nPython Causality ë¼ì´ë¸ŒëŸ¬ë¦¬ (Observational Datasets ê¸°ë°˜) Github ë§í¬\n\n\n2\nPython\nMicrosoft - DoWhy\nCausal Inference End-to-End ë¼ì´ë¸ŒëŸ¬ë¦¬ (4ë‹¨ê³„ë¡œ êµ¬ì„±) Github ë§í¬/Dowhy ì„¤ëª…ìë£Œ\n\n\n3\nPython\nMicrosoft - EconML\nHeterogeneous treatment effects ì¶”ì • ë¼ì´ë¸ŒëŸ¬ë¦¬ Github ë§í¬\n\n\n4\nPython\nUber - CausalML\nUplift Modeling & MLê³¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ Github ë§í¬\n\n\n5\nPython\nsensemakr\nPython Sensitivity Analysis ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ëª… ë§í¬\n\n\n6\nPython\ncdt\nCausal Discovery ë¼ì´ë¸ŒëŸ¬ë¦¬ (PC, Skeleton) Github ë§í¬\n\n\n7\nR\nGoogle - CausalImpact\nGoogleì—ì„œ ë² ì´ì§€ì•ˆ Time Series ëª¨ë¸ì„ ì‚¬ìš©í•œ R ê¸°ë°˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Github ë§í¬\n\n\n8\nR\nDagitty\nDAG ì‹œê°í™” ë° ëª¨ë¸ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ Github ë§í¬ / ì‹œê°í™” ì—°ìŠµ\n\n\n9\nR\nbnlearn\nDAG ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ Github ë§í¬ /ê´€ë ¨ ë…¼ë¬¸ ë° ì½”ë“œ ì†Œê°œ\n\n\n10\nR\nsensemakr / tipr\nR Sensitivity Analysis ë¼ì´ë¸ŒëŸ¬ë¦¬ sensemakr ì„¤ëª… ë§í¬ / tipr Github ë§í¬\n\n\n11\nR\nMatchlt\nMatching (PSM)ë¼ì´ë¸ŒëŸ¬ë¦¬ Github ë§í¬\n\n\n12\nê³µí†µ\ncausaldata\nCausal inference ì±…ì— ìˆëŠ” ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ Github ë§í¬\n\n\n13\nR\ntlverse\ncausal data science with the tlverse software ecosystem https://tlverse.org/tlverse-handbook/\n\n\n\n\nCausal Inference ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ íŠœí† ë¦¬ì–¼ì— ëŒ€í•œ ì •ë¦¬ëŠ”\nê°€ì§œì—°êµ¬ì†Œ Causal Inference ë…¸ì…˜ í˜ì´ì§€ì—ì„œë„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\nCitationBibTeX citation:@online{2023,\n  author = {, everything},\n  title = {01\\textbackslash. {Causal} {Inference} {ë¼ì´ë¸ŒëŸ¬ë¦¬} {ì •ë¦¬}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\neverything. 2023. â€œ01\\. Causal Inference ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë¦¬.â€\nNovember 14, 2023."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Causal_Discovery/Causal_Discovery.html",
    "href": "posts/Introduction_to_causal_inference_Causal_Discovery/Causal_Discovery.html",
    "title": "12. Causal Discovery from Observational Data",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì˜ ê¹€ìƒëˆ, ê¹€ì¤€ì˜ì…ë‹ˆë‹¤.Â \nIntroduction to Causal Inference ê°•ì˜ì˜ 11ë²ˆì§¸ ì±•í„°ì´ë©°, í•´ë‹¹ ì±•í„°ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\nContents\n\nIndependence-Based Causal Discovery\nSemi-Parametric Causal Discovery\n\nâ—¦ ê°•ì˜ ì˜ìƒ ë§í¬ : Chapter 11. causal discovery from observational data\nÂ  Â  ì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!\n\nCausal Discovery ì˜ë¯¸\n\nCausal Discovery : Data â†’ Causal graph\nëª‡ ê°€ì§€ ê°€ì • í•˜ì—ì„œ, causal discoveryë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ causal graphë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ\n\n\n\n\n1. Independence-Based Causal Discovery\nAssumption\n\nMarkov assumption\nA node is dependent only on its descendants in the graph\n\n\nì˜ˆì‹œ) ì•„ë˜ ê·¸ë¦¼ì—ì„œ \\(X_4\\)ëŠ” \\(X_3\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤ë©´ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ê³¼ëŠ” ë…ë¦½(ì¡°ê±´ë¶€ ë…ë¦½)\n\nâ†’ \\(P(X_4|X_3,X_2,X_1) = P(X_4|X_3)\\)\n\n\n\n\nFaithfulness\nNode that are causally connected in a particular way in the graph are probabilistically dependent\n\nFaithfulness Conterexample\n\n\n\n\n\\[ \\begin{aligned} B &:= \\alpha A \\\\ C &:= \\gamma A \\\\ D &:= \\beta B + \\delta C \\\\ \\newline D &= (\\alpha \\beta + \\gamma \\delta) A \\end{aligned}  \n\\]\n\\(\\alpha \\beta = - \\gamma \\delta\\) ì¼ ë•Œ, \\(D = 0\\)ì´ ë¨\nì¦‰, A â†’ B â†’ Dì˜ íš¨ê³¼ì™€ A â†’ C â†’ Dì˜ íš¨ê³¼ê°€ ì„œë¡œ ë°˜ëŒ€ì¼ ê²½ìš° íš¨ê³¼ê°€ ìƒì‡„ë¨\nA, D ê°„ì— ê´€ê³„ê°€ ì—†ë‹¤ëŠ” ì˜ëª»ëœ ê²°ë¡ ì„ ë„ì¶œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ëŸ¬í•œ ê²½ìš°ëŠ” ì—†ë‹¤ê³  ê°€ì •í•¨\n\n\nCausal sufficiency\nthere are no unobserved confounders of any of the variables in the graph\n\n\nAcyclicity\nstill assuming there are no cycles in the graph\n\n\n\n\nMarkov equivalent class\n\nconditional independence êµ¬ì¡°ê°€ ê°™ì€ DAGì˜ ì§‘í•©ì„ ì˜ë¯¸í•¨\nDAGì—ì„œ skeletonê³¼ v-structure(immorality)ê°€ ê°™ì€ ê²½ìš°\n\nChain/Fork\n\n\nMarkov : \\(X_1 \\perp\\!\\!\\!\\perp X_3 | X_2\\)\nMinimality : \\(X_1 \\not\\!\\perp\\!\\!\\!\\perp X_2 \\text{ and }X_2 \\not\\!\\perp\\!\\!\\!\\perp X_3\\)\nFaithfulness : \\(X_1 \\not\\!\\perp\\!\\!\\!\\perp X_3\\)\nchain/forkì— í•´ë‹¹í•˜ëŠ” ì„¸ ê·¸ë˜í”„ ëª¨ë‘ ê·¸ë˜í”„ì˜ í˜•íƒœëŠ” ë‹¤ë¥´ì§€ë§Œ, ê°™ì€ conditional independence êµ¬ì¡°ë¥¼ ê°–ìŒ\n\nImmorality\n\n\n\n\nMarkov : \\(X_1 \\not\\!\\perp\\!\\!\\!\\perp X_3 | X_2\\)\nMinimality : \\(X_1 \\not\\!\\perp\\!\\!\\!\\perp X_2 \\text{ and }X_2 \\not\\!\\perp\\!\\!\\!\\perp X_3\\)\nFaithfulness : \\(X_1 \\perp\\!\\!\\!\\perp X_3\\)\nImmoralityì˜ ê²½ìš° ê°™ì€ conditional independent êµ¬ì¡°ë¥¼ ê°–ëŠ” ê·¸ë˜í”„ëŠ” í•œ ê°œë°–ì— ì—†ìŒ\n\nSkeletons\n\n\n\n\n\n\n\n\n\n\n\nchain/fork êµ¬ì¡°ì˜ ê²½ìš° ê°™ì€ conditional independence êµ¬ì¡°ë¥¼ ê°–ê³ , ê°™ì€ skeletonì„ ê°–ìŒ\ncomplete ê·¸ë˜í”„ì˜ ì˜ ê²½ìš° chain êµ¬ì¡°ì—ì„œ \\(X_1 \\rightarrow X_3\\) edgeë¥¼ ì¶”ê°€í•  ë•Œ, ì´ì „ê³¼ ë‹¤ë¥¸ conditional independence êµ¬ì¡°ë¥¼ ê°–ê³ , ë‹¤ë¥¸ skeletonì„ ê°–ìŒ\n\n\nMarkov Equivalence via Immoral Skeletons\ntwo graphs are markov equivalent if and only if they have the same skeleton and same immoralities\n\n\nEssential graph(CPDAG): skeleton, immoralityê°€ ê°™ì€ ê·¸ë˜í”„ë¥¼ ì˜ë¯¸í•¨\nMarkov equivalent classë¥¼ ë§Œì¡±í•˜ëŠ” DAGëŠ” CPDAGë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\nPC algorithm\n\në°ì´í„°ë¡œë¶€í„° CPDAGë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜\n\n\n1. Identify the skeleton\n\n\në…¸ë“œ ê°„ ì™„ì „ì—°ê²°ëœ complete graph ìƒì„±\ncomplete graphì—ì„œ ë‘ ë…¸ë“œ ê°„ unconditionally independentì¸ ê²½ìš° edge ì œê±°\n\n\\(A,\\, B\\)ëŠ” immoralityì´ë¯€ë¡œ \\(A\\perp\\!\\!\\!\\perp B\\), edge ì œê±°\n\nëª¨ë“  ìŒ \\((X, Y)\\)ì— ëŒ€í•´ \\(X\\perp\\!\\!\\!\\perp Y|{Z}\\)ì¸ ê²½ìš° edge ì œê±°\n\n\\(C\\)ì— ëŒ€í•´ conditioning í–ˆì„ ë•Œ, conditionally independentë¼ëŠ” ì˜ë¯¸ëŠ” ë‘ ë…¸ë“œ ê°„ ì§ì ‘ ì—°ê²°ëœ edgeê°€ ì—†ë‹¤ëŠ” ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆìŒ\n\\(A\\)ì™€ \\(E\\)ì— ì§ì ‘ ì—°ê²°ëœ edgeê°€ ì‚¬ë¼ì§\në‚˜ë¨¸ì§€ ë…¸ë“œë„ ë™ì¼í•œ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ìƒê°í•´ë³´ë©´ (c)ê·¸ë˜í”„ì²˜ëŸ¼ true graphì˜ skeletonì„ ì‹ë³„í•  ìˆ˜ ìˆìŒ\n\n\n2. Identify immoralities and orient themÂ \n\n\nImmoralityì˜ ê²½ìš° conditional independent êµ¬ì¡°ë¥¼ ê°–ëŠ” ê·¸ë˜í”„ëŠ” í•œ ê°œë°–ì— ì—†ìœ¼ë¯€ë¡œ ë°©í–¥ì„ ì‹ë³„í•  ìˆ˜ ìˆìŒ\nImmoralityì˜ ê²½ìš° \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) ì¸ ê²½ìš°ì— í•´ë‹¹í•¨\n\\(A\\perp\\!\\!\\!\\perp B|{C}\\), \\(A\\perp\\!\\!\\!\\perp D|{C}\\), \\(B\\perp\\!\\!\\!\\perp C|{E}\\), \\(A\\perp\\!\\!\\!\\perp B|{C}\\), \\(D\\perp\\!\\!\\!\\perp E|{C}\\) ì— ëŒ€í•´ì„œ, ê°ê° ë‹¤ testí•´ë³´ê³ conditionally dependentê°€ ë˜ëŠ” ìŒì„ ì°¾ìŒ\n\n3. Orient qualifying edges that are incident on colliders\n\n\në‚¨ì€ edge ì¤‘ì—ì„œ ë°©í–¥ì„ ì§€ì •í–ˆì„ ë•Œ, immoralityê°€ ë˜ì§€ ì•ŠëŠ” ë°©í–¥ìœ¼ë¡œ ì§€ì •\n\\(D \\rightarrow C\\)ì˜ ê²½ìš° \\(A \\rightarrow C \\leftarrow D\\)ë¡œ immoralityì— í•´ë‹¹í•˜ë¯€ë¡œ, \\(C \\rightarrow D\\) ë¡œ ë°©í–¥ì„ ì§€ì •í•´ì¤˜ì•¼ í•¨\n\nCausal discoveryì— ê´€í•œ ë‹¤ë¥¸ ë°©ë²•\n\n\n\nì¸ê³¼ì¶”ë¡ ì˜ ë°ì´í„°ê³¼í•™ [session 18-3] ê°•ì˜ìë£Œ\n\n\n\nFCI : without assuming causal sufficiency\nCCD : without assuming acylicity\n\n\n\nPC ì•Œê³ ë¦¬ì¦˜ í•œê³„ì \n\nê° ë…¸ë“œ ìŒì˜ conditional independence testì— ì˜ì¡´í•˜ë¯€ë¡œ ê³„ì‚°ëŸ‰ì´ ë§ìŒ\nì •í™•í•œ testë¥¼ í•˜ê¸° ìœ„í•´ì„œëŠ” ë°ì´í„°ê°€ ë§ì•„ì•¼ í•¨\n\n\n\n2. Semi-Parametric Causal Discovery\n\nIssues in independence based causal discovery\n\nFaithfulness assumption í•„ìš”\nLarge samples for conditional independence tests\nMarkov equivalence classë§Œ identify\n\n\n\nNo identifiability without Parametric Assumptions\n1. Markov perspective\n\n\nMarkov equivalentì¸ ê²½ìš°, conditional independenceê°€ \\(Xâ†’Y\\)ì™€ \\(Xâ†Y\\)êµ¬ë³„ì— ë„ì›€ X\nBestëŠ” essential graph X-Y identification\n\n2. SCM perspective\n\nNon-Identifiability of Two-Node Graphs(proposition): For every joint distribution P(x,y) on two real-valued random variables, there is an SCM in either direction that generates data consistent with P(x,y)\n\n\n\nParametric form SCMì— assumptionsì„ í•œë‹¤ë©´, êµ¬ë³„ ê°€ëŠ¥!\nì¦‰, parametric form needs assumptions!\n\nLinear Non-Gaussian Assumption\n\nMarkov Completeness Theoremì— ì˜í•´ linear with Gaussian noiseí˜•íƒœëŠ” ê·¸ë˜í”„ êµ¬ë³„ì´ ë¶ˆê°€ëŠ¥\nê·¸ë ‡ì§€ë§Œ non-Gaussianì¸ ê²½ìš° êµ¬ë³„ ê°€ëŠ¥!\n\n\\[Y : = f(x) + U\\]\n\\(X\\perp\\!\\!\\!\\perp U\\), and U is non-Gaussian random variable.\n\nIdentifiability in linear non-Gaussian setting theorem:\n\n\n\nGraphic intuition\n\n\n\n\nLiNGAM\nê°€ì •\n\nData generating process is linear\nNo unobserved confounders\nâ‡’ DAG ì¶©ì¡±\nNoise follows independent non-Gaussian distribution.\n\ncomplete causal structure can be estimated without prior information\ndiscover direction of causality\n\nâ‡’ Linear, Non-Gaussian, Acyclic Model\n\n\nICA(Independent Component Analysis)\n\n: A statistical technique used for estimating mixing matrix A s.t x=Ae, x is observed and A and e are not.\nâ†’ identify linear model\n\në…ë¦½ ì„±ë¶„ ë¶„ì„\nDimension reduction\në°ì´í„°ê°€ í†µê³„ì ìœ¼ë¡œ ë…ë¦½ì´ê³  ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šì„ ë•Œ, ë…ë¦½ì„±ì´ ìµœëŒ€ê°€ ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì¶•ì„ ì‚¼ìŒ\nâ‡’ ê°€ì¥ ë…ë¦½ì ì¸ ì¶•ì„ ì°¾ìŒ\n\në§Œì¼ Gaussianì´ë©´ covariance matrixê°€ í•­ìƒ ê°™ìŒ\n\n\n\nhttps://www.jmlr.org/papers/volume7/shimizu06a/shimizu06a.pdf\n\n\n\n\nNonlinear Models\nNonlinear additive noise setting\n\n\n\nhttps://papers.nips.cc/paper/2008/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf\n\n\n\nNonlinearì˜ ì—­í• ì€ non-Gaussianityì™€ ìœ ì‚¬causal direction identify ê°€ëŠ¥\nâ‡’ ê´€ì¸¡ë³€ìˆ˜ë“¤ì˜ symmetryë¥¼ ê¹¸\nnonlinear+additive noise yields identifiable models\n\nPost-Nonlinear setting\n\nâ†’ noiseê°€ additiveë¡œ ë“¤ì–´ê°€ì§€ ì•ŠëŠ”ë‹¤ë©´?\n\ngeneralization of the nonlinear additive model (mild assumption)\n\n\n\n\n\nhttps://arxiv.org/ftp/arxiv/papers/1205/1205.2599.pdf\n\n\n\nì°¸ê³  ìë£ŒÂ \nì¸ê³¼ì¶”ë¡ ì˜ ë°ì´í„°ê³¼í•™ ê°•ì˜ : https://youtu.be/h1eMKb4iCTk\n\n\n\n\n\n\nCitationBibTeX citation:@online{& junyoung2023,\n  author = {\\& Junyoung, Sangdon},\n  title = {12\\textbackslash. {Causal} {Discovery} from {Observational}\n    {Data}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n& Junyoung, Sangdon. 2023. â€œ12\\. Causal Discovery from\nObservational Data.â€ November 14, 2023."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html",
    "title": "10. Instrumental Variables",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì˜ ë‚¨ê¶ë¯¼ìƒ, ì •í˜¸ì¬ì…ë‹ˆë‹¤.\nIntroduction to Causal Inference ê°•ì˜ì˜ ì•„í™‰ë²ˆì§¸ ì±•í„°ì´ë©°, í•´ë‹¹ ì±•í„°ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\nê°•ì˜ ì˜ìƒ ë§í¬ : https://youtu.be/B0SRWteGoOw\nì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#contents",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#contents",
    "title": "10. Instrumental Variables",
    "section": "Contents",
    "text": "Contents\n\nIntro\nWhat is an Instrument?\nNo Nonparametric Identification of the ATE\nWarm-Up: Binary Linear Setting\nContinuous Linear Setting\nNonparametric Identification of the LATE\nIV in More General Settings"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#intro",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#intro",
    "title": "10. Instrumental Variables",
    "section": "1. Intro",
    "text": "1. Intro\nQ : ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ ìš”ì¸ì´ ìˆì„ ë•Œ ì–´ë–»ê²Œ ì¸ê³¼ê´€ê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?\nA : Frontdoor adjustment - Chap.5\n\nì¤‘ê°„ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ê³¼íš¨ê³¼ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•\n\nA : Unconfounded children criterion - Chap.5\n\ní•˜ë‚˜ì˜ conditioning setìœ¼ë¡œ ì²˜ì¹˜ë³€ìˆ˜ Tì˜ ìì† ì¤‘ì— ê²°ê³¼ë³€ìˆ˜ Yì˜ ì¡°ìƒì¸ ê²ƒë“¤ë¡œ í†µí•˜ëŠ” backdoor pathë¥¼ ëª¨ë‘ ë§‰ëŠ” ë°©ë²•\n\nA : Some other fancy application of do-calculus - Chap.5\n\nê·¸ë˜í”„ê°€ ì•„ë‹Œ statistical quantityë¥¼ ì´ìš©í•œ ì¼ë°˜ì ì¸ ë°©ë²•\n\nA : Set identification (bounds) - Chap.7\n\nê´€ì°°ë˜ì§€ ì•Šì€ êµë€ ìš”ì¸ì˜ intervalì„ ì¢í˜€ë³´ëŠ” ë°©ë²•\n\nA : Sensitivity analysis - Chap.7\n\nê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì´ ì¡´ì¬í• ë•Œ ì •ëŸ‰ì ìœ¼ë¡œ ì˜í–¥ë ¥ì„ íŒë‹¨í•˜ëŠ” ë°©ë²•\n\nA : Instrumental Variables(â˜†)\n\në‹¤ë¥¸ ë³€ìˆ˜ë¡œ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì˜ ì˜í–¥ì„ ì—†ì• ëŠ” ë°©ë²•"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#what-is-an-instrument",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#what-is-an-instrument",
    "title": "10. Instrumental Variables",
    "section": "2. What is an Instrument?",
    "text": "2. What is an Instrument?\në„êµ¬ ë³€ìˆ˜ë€, ì²˜ì¹˜ë³€ìˆ˜ Tì™€ì—ëŠ” ì˜í–¥ì„ ì£¼ë©´ì„œ ê·¸ ì´ì™¸ì˜ ë³€ìˆ˜ì—ëŠ” ì˜í–¥ì„ ì£¼ê±°ë‚˜ ë°›ì§€ ì•ŠëŠ” ë³€ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\n2.1 Assumption\n1. Relevance\n\në„êµ¬ë³€ìˆ˜ ZëŠ” ì²˜ì¹˜ë³€ìˆ˜ Tì— ì¸ê³¼ì  ì˜í–¥(ìƒê´€ì„± ì¡´ì¬)ì„ ë¼ì¹©ë‹ˆë‹¤.\n\n\n2. Exclusion Restriction\n\n\n\n\n\n\n\n\n\n\n\nê²°ê³¼ë³€ìˆ˜ Yì— ëŒ€í•œ ë„êµ¬ë³€ìˆ˜ Z ì¸ê³¼ì  ì˜í–¥ì€ ì²˜ì¹˜ë³€ìˆ˜ Tì— ì˜í•´ ì™„ì „íˆ ì¤‘ì¬ë©ë‹ˆë‹¤.\në„êµ¬ ë³€ìˆ˜ Zê°€ ê²°ê³¼ë³€ìˆ˜ Yì— ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ìœ„í•´ì„  ë°˜ë“œì‹œ ì²˜ì¹˜ë³€ìˆ˜ Të¥¼ í†µí•´ì•¼ í•©ë‹ˆë‹¤.\n\n3. Instrumental Unconfoundedness\n\në„êµ¬ë³€ìˆ˜ Zì—ì„œ ê²°ê³¼ë³€ìˆ˜ Yê°„ì˜ backdoor pathsëŠ” ì—†ìŠµë‹ˆë‹¤.\në„êµ¬ë³€ìˆ˜ Zì™€ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ ìš”ì¸ Uì˜ ê´€ê³„ëŠ” ì—†ìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\në„êµ¬ë³€ìˆ˜ Zì™€ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ ìš”ì¸ Uì˜ ê´€ê³„ê°€ ì—†ìœ¼ë¯€ë¡œ ê´€ì°°ëœ êµë€ ë³€ìˆ˜(W)ë¥¼ ì°¨ë‹¨í•˜ì—¬ Instrumental variablesë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#no-nonparametric-identification-of-the-ate",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#no-nonparametric-identification-of-the-ate",
    "title": "10. Instrumental Variables",
    "section": "3. No Nonparametric Identification of the ATE",
    "text": "3. No Nonparametric Identification of the ATE\nQ : ë„êµ¬ ë³€ìˆ˜ê°€ ì¸ê³¼ê´€ê³„ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆë‹¤ë©´, ì™œ Chapter 6. Non-parametiric Identificationì—ì„œ ë³´ì§€ ì•Šì•˜ì„ê¹Œìš”?\nA : ë„êµ¬ë³€ìˆ˜ëŠ” Non-parametiric Identification ë°©ë²•ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\nê°€ì •ì´ í•„ìš” ì—†ì„ë•Œ Non-parametiric Identificationì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ë° ë„êµ¬ë³€ìˆ˜ì—ëŠ” 3ê°€ì •ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n\n&gt; (FYI) Nonparametric Identificationì„ ë§Œì¡±í•˜ëŠ” ì¡°ê±´\n\n\n[ì²˜ì¹˜ë³€ìˆ˜ T]ì™€ [ê²°ê³¼ë³€ìˆ˜ Yì˜ ancestorì´ë©´ì„œ ì²˜ì¹˜ë³€ìˆ˜ Tì˜ ìì‹ë…¸ë“œì¸ ì–´ëŠ ë…¸ë“œ]ì™€ì˜ pathëŠ” ì°¨ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#warm-up-binary-linear-setting",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#warm-up-binary-linear-setting",
    "title": "10. Instrumental Variables",
    "section": "4. Warm-Up: Binary Linear Setting",
    "text": "4. Warm-Up: Binary Linear Setting\n\nAssumption\n\n\\(Y:= \\delta T + \\alpha_u U\\)\n\nSetting\n\nì²˜ì¹˜ë³€ìˆ˜ Tì™€ ë„êµ¬ë³€ìˆ˜ ZëŠ” binary\n\nAssociational difference for the Z-Y relationship :\n\n\\(E[Y | Z = 1] - E[Y | Z = 0]\\)\n\\(= E[\\delta T + \\alpha_u U | Z = 1] -E[\\delta T + \\alpha_u U| Z = 0]\\) â† exclusion restriction(2ë²ˆì§¸ê°€ì •) and linear outcome assumptions\n\\(= \\delta(E[T | Z = 1] - E[T | Z = 0]) + \\alpha_u (E[U|Z=1] - E[U|Z=0])\\)\n\\(= \\delta(E[T | Z = 1] - E[T | Z = 0]) + \\alpha_u (E[U] - E[U])\\) â† instrumental unconfoundedness assumption(3ë²ˆì§¸ê°€ì •)\n\\(= \\delta(E[T | Z = 1] - E[T | Z = 0])\\)\n\nWald estimand :\n\n\\(\\delta=\\frac{Cov(Y,Z) }{Cov(T,Z) }\\)\nRelevance Assumptionìœ¼ë¡œ \\(E[T | Z = 1] \\not= E[T | Z = 0]\\) ì„ ë§Œì¡±í•©ë‹ˆë‹¤.\n\në„êµ¬ë³€ìˆ˜ Zì—ì„œ ê²°ê³¼ë³€ìˆ˜ Yë¡œ ê°€ëŠ” backdoor pathëŠ” ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê° pathì˜ ì˜í–¥ì„ ì‚´í´ë´…ë‹ˆë‹¤.\në˜í•œ Causal effectê°€ ìˆëŠ” directed pathë¥¼ ê³„ìˆ˜ë“¤ì˜ ê³±ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ë¥¼ ëŒ€ì…í•˜ë©´ \\(\\delta = \\frac{\\alpha_z \\delta}{\\alpha_z}\\) ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nWald estimator :\n\n\\(\\hat\\delta=\\frac{\\frac{1}{n_1}\\sum_{i:z_i=1}Y_i -\\frac{1}{n_0}\\sum_{i:z_i=0}Y_i }{\\frac{1}{n_1}\\sum_{i:z_i=1}T_i -\\frac{1}{n_0}\\sum_{i:z_i=0}T_i }\\)\nZâ†’Yì˜ ì˜í–¥ì„ êµ¬í•œ í›„ Zâ†’Tì˜ ì˜í–¥ì„ ë‚˜ëˆ„ì–´ì„œ ê³„ì‚°í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#continuous-linear-setting",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#continuous-linear-setting",
    "title": "10. Instrumental Variables",
    "section": "5. Continuous Linear Setting",
    "text": "5. Continuous Linear Setting\n\nAssumption\n\n\\(Y:= \\delta T + \\alpha_u U\\)\n\nSetting\n\nì²˜ì¹˜ë³€ìˆ˜ Tì™€ ë„êµ¬ë³€ìˆ˜ ZëŠ” continuous\n\nAssociational difference for the Z-Y relationship :\n\n\\(Cov(Y,Z) = E[YZ]E[Y]E[Z]\\)\n\\(= E[(\\delta T + \\alpha_u U )Z] -E[\\delta T + \\alpha_u U]E[ Z]\\) â† exclusion restriction(2ë²ˆì§¸ê°€ì •) and linear outcome assumptions\n\\(= \\delta E[TZ] + \\alpha_u E[UZ] - \\delta E[T]E[Z] - \\alpha_u E[U]E[Z]\\)\n\\(= \\delta (E[TZ] - E[T]E[Z]) + \\alpha_u (E[UZ] - E[U]E[Z])\\)\n\\(= \\delta Cov(T,Z) + \\alpha_u Cov(U,Z)\\)\n\\(= \\delta Cov(T,Z)\\) â† instrumental unconfoundedness assumption(3ë²ˆì§¸ê°€ì •)\n\nWald estimand :\n\n\\(\\delta=\\frac{Cov(Y,Z) }{Cov(T,Z) }\\)\nRelevance Assumptionìœ¼ë¡œ \\(Cov(T,Z) \\not=0\\) ì„ ë§Œì¡±í•©ë‹ˆë‹¤.\n\nWald estimator :\n\n\\(\\hat\\delta=\\frac{\\hat{Cov}(Y,Z) }{\\hat{Cov}(T,Z) }\\)\n\nTwo-stage least squares estimator\n\nLinearly regress \\(T\\) on \\(Z\\) to estimate \\(E[T | Z ]\\) . This gives us the projection of \\(T\\) onto \\(Z\\): \\(\\hat T\\).\nLinearly regress \\(Y\\) on \\(\\hat T\\) to estimate \\(E[\\hat T | Z ]\\) . Obtain our estimate \\(\\hat \\delta\\) as the fitted coefficient in front of \\(\\hat T\\).\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat T\\)ëŠ” \\(U\\)ì— ëŒ€í•œ í•¨ìˆ˜ê°€ ì•„ë‹ˆë¯€ë¡œ \\(U\\)ì—ì„œ \\(T\\)ë¡œê°€ëŠ” pathê°€ ì‚¬ë¼ì ¸ì„œ backdoor pathê°€ ì œê±°ë©ë‹ˆë‹¤.\nâ€» ì˜ˆì‹œ\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as sm\n\nnp.random.seed(12345) # ë™ì¼í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ì‹œë“œ ì„¤ì •\nnum = 10000 # ë°ì´í„° ìˆ˜\n\nU = np.random.normal(size = num) # Unobserved Factors\nZ = np.random.normal(size = num) # Instrumental Variable\n\n# TëŠ” Uì™€ Zì˜ ìì‹ë…¸ë“œ\nT = 3.0*U + 6.0*Z + np.random.normal(size = num) # Treatment\n\n# YëŠ” Tì™€ Uì˜ ì§€ì‹ë…¸ë“œ\nY = 15.0*U + 9.0*T + np.random.normal(size = num) # Outcome\n\ndata = pd.DataFrame({'T' : T, 'U' : U, 'Y' : Y})\n\në‹¨ìˆœë¹„êµ\n\n# ë‹¨ìˆœ ë¹„êµ\nsod_model = sm.ols('Y ~ T', data).fit()\n# êµë€ ë³€ìˆ˜ë¥¼ ëˆ„ë½í•˜ì˜€ê¸°ì— ì¸ê³¼ íš¨ê³¼ê°€ ì˜ëª» ì¶”ì •ë¨\nsod_model.summary().tables[1]\n\n\n\n\n\n\n\n\n\n\n\n\nÂ \ncoefÂ \nstd err\nÂ t\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-0.1277\n0.134\n-0.953\n0.341\n-0.390\n0.135\n\n\nT\n9.9905\n0.020\n505.906\n0.000\n9.952\n10.029\n\n\n\nêµë€ë³€ìˆ˜ë¥¼ í†µì œí•˜ì§€ ì•Šì•˜ì„ ë•Œ ì¸ê³¼íš¨ê³¼ëŠ” 9.9905ë¡œ ë‚˜ì˜´ 0.9905ë§Œí¼ì˜ errorê°€ ë°œìƒí•©ë‹ˆë‹¤.\n\\(Y = Intercept + T + e\\)\n\në„êµ¬ë³€ìˆ˜ í™œìš©\n\n\\[ (T â†’ Y)ì˜ ì¸ê³¼íš¨ê³¼ = \\frac {(Z â†’ Y) pathì˜ ì˜í–¥} {(Z â†’ T) pathì˜ ì˜í–¥} \\]\n# first stage model(Z -&gt; T)\nZ_to_T = sm.ols('T ~ Z', data).fit()\n# reduced model(Z -&gt; Y)\nZ_to_Y = sm.ols('Y ~ Z', data).fit()\n# first stage model\nZ_to_T.summary().tables[1]\n\n\n\n\n\n\n\n\n\n\n\n\nÂ \ncoefÂ \nstd err\nÂ t\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-0.0349\n0.032\n-1.103\n0.270\n-0.097\n0.027\n\n\nT\n6.0207\n0.032\n189.711\n0.000\n5.958\n6.083\n\n\n\n# reduced model\nZ_to_Y.summary().tables[1]\n\n\n\n\n\n\n\n\n\n\n\n\nÂ \ncoefÂ \nstd err\nÂ t\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-0.4725\n0.429\n-1.100\n0.271\n-1.314\n0.369\n\n\nT\n54.3108\n0.431\n126.122\n0.000\n53.467\n55.155\n\n\n\nZ_to_Y.params.Z/Z_to_T.params.Z\nê³„ì‚°ê²°ê³¼ 9.020754651013464ë¡œ ì‹¤ì œ ì¸ê³¼íš¨ê³¼ì¸ 9ì— ê·¼ì‚¬í•©ë‹ˆë‹¤.\n\n2SLS\n\nì¸ê³¼ íš¨ê³¼ì˜ ìœ ì˜ì„±, ì‹ ë¢°êµ¬ê°„ì„ í™•ì¸í•˜ê¸° ìœ„í•´ 2SLSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n# first stage\nt_hat = Z_to_T.predict()\ndata['T_hat'] =  t_hat\n\n# second stage\nThat_to_Y = sm.ols('Y ~ T_hat', data).fit()\n# second stage model\nThat_to_Y.summary().tables[1]\n\n\n\n\n\n\n\n\n\n\n\n\nÂ \ncoefÂ \nstd err\nÂ t\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n-0.1575\n0.429\n-0.367\n0.714\n-0.999\n0.684\n\n\nT\n9.0208\n0.072\n126.122\n0.000\n8.881\n9.161\n\n\n\nThat_to_Y.params.T_hat\nì¶”ì • ì¸ê³¼íš¨ê³¼ëŠ” 9.020754651013458ì´ë©°, p-valueê°€ ë§¤ìš° ì‘ì•„ ì¸ê³¼íš¨ê³¼ê°€ ìœ ì˜í•˜ë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#nonparametric-identification-of-the-late",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#nonparametric-identification-of-the-late",
    "title": "10. Instrumental Variables",
    "section": "6. Nonparametric Identification of the LATE",
    "text": "6. Nonparametric Identification of the LATE\nì•ì—ì„œ Linear settingì—ì„œ ATEë¥¼ ê³„ì‚°í•˜ëŠ” ë²•ì„ ë´¤ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì´ê±´ ë„ˆë¬´ ê°•ë ¥í•œ ê°€ì •ì…ë‹ˆë‹¤.\nnonparametricí•œ ìƒí™©(ë¶„í¬ì— ëŒ€í•œ ê°€ì •ì´ ì—†ëŠ” ìƒí™©)ì—ì„œ ATEë¥¼ êµ¬í•  ìˆ˜ëŠ” ì—†ì„ê¹Œìš”? ì™„ì „í•œ ATEëŠ” ì•„ë‹ˆì§€ë§Œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n\nIV ê´€ë ¨ Notation\në³¸ê²©ì ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ì•ì„œ ëª‡ ê°€ì§€ í‘œê¸°ë²•ì„ ì •ë¦¬í•©ì‹œë‹¤\n\n\\(Z\\): ë„êµ¬ë³€ìˆ˜ (instrumental variable)\n\\(T\\): ì²˜ì¹˜ë³€ìˆ˜ (treatment variable)\n\\(Y\\): ê²°ê³¼, ì¢…ì†ë³€ìˆ˜ (dependent variable)\n\n\\(Z \\rightarrow T \\rightarrow Y\\)ë¡œ ê°€ëŠ” ì¸ê³¼ê°€ ìˆë‹¤ê³  í•  ë•Œ,\n\n\\(Y(1) \\triangleq Y(T=1)\\)\n\\(T(1) \\triangleq T(Z=1)\\)\n\\(Y(Z=1)\\)ì²˜ëŸ¼ \\(Z\\)ì— interveneí–ˆì„ ë•Œì˜ potential outcome \\(Y\\)ì˜ ê°’ì€ ë³„ë‹¤ë¥¸ ì¶•ì•½í˜•ì„ ê°€ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n\n\nPrincipal Strata\n\\(Z\\)ê°€ \\(T\\)ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ì£¼ëŠëƒì— ë”°ë¼ ë°ì´í„°ë¥¼ 4ê°€ì§€ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nComplier: \\(Z\\)ê°€ ì‹œí‚¤ëŠ”ëŒ€ë¡œ í•˜ëŠ” ì‚¬ëŒë“¤.\n\n\\[ T(1)=T(Z=1)=1 \\qquad T(0)=T(Z=0)=0 \\]\n\nDefier: \\(Z\\)ê°€ ì‹œí‚¤ëŠ” ê±° ë°˜ëŒ€ë¡œë§Œ í•˜ëŠ” ì²­ê°œêµ¬ë¦¬ë“¤.\n\\[ T(1)=T(Z=1)=0 \\qquad T(0)=T(Z=0)=1 \\]\nAlways-taker: \\(Z\\)ì— ìƒê´€ ì—†ì´ treatmentë¥¼ ë°›ëŠ” ì‚¬ëŒë“¤.\n\\[ T(1)=T(Z=1)=1 \\qquad T(0)=T(Z=0)=1 \\]\nNever-taker: \\(Z\\)ì— ìƒê´€ ì—†ì´ treatmentë¥¼ ì•ˆ ë°›ëŠ” ì‚¬ëŒë“¤.\n\\[ T(1)=T(Z=1)=0 \\qquad T(0)=T(Z=0)=0 \\]\n\nğŸ’¡ ì´ ë•Œ, ìœ„ì— 2ê°œ ê·¸ë£¹ê³¼ ì•„ë˜ 2ê°œ ê·¸ë£¹ì€ ë‹¤ë¥¸ causal graphë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n\n\n\nLATE êµ¬í•˜ê¸°\nIVë¥¼ ì¨ë„, unobserved confoundingì´ ìˆë‹¤ë©´ nonparametricí•œ ìƒí™©ì—ì„œì˜ ATEë¥¼ êµ¬í•  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.\ní•˜ì§€ë§Œ ì•½ê°„ì˜ ê°€ì •ì„ ì¶”ê°€í•´ì„œ LATE (Local Average Treatment Effect) ë˜ëŠ” CACE (Complier Average Causal Effect)ë¼ê³  ë¶ˆë¦¬ëŠ” ê±¸ êµ¬í•  ìˆ˜ëŠ” ìˆìŠµë‹ˆë‹¤.\nğŸ’¡ LATE (ë˜ëŠ” CACE)ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.\n\\(\\mathbb{E}[Y(T=1)-Y(T=0)\\:|\\:T(Z=1)=1,\\:T(Z=0)=0]\\)\nLATEë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” monotonicity(ë‹¨ì¡°ì„±)ì´ë¼ëŠ” ê°€ì •ì„ ë§ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤.\n\\(\\forall i, \\space T_i(Z=1) \\geq T_i(Z=0)\\)\në‹¤ë¥´ê²Œ ë§í•˜ìë©´, ìš°ë¦¬ ë°ì´í„°ì— defierê°€ ì—†ë‹¤ëŠ” ê°€ì •ì…ë‹ˆë‹¤.\nRelevance, exclusion restriction, instrumental unconfoundedness + monotonicity ê°€ì •ì´ ë§Œì¡±ë  ë•Œ, LATEëŠ” Wald estimandì™€ ê°™ìŠµë‹ˆë‹¤.\n\\(\\mathbb{E}[Y(1)-Y(0)\\:|\\:T(1)=1,\\:T(0)=0]= \\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{\\mathbb{E}[T|Z=1]-\\mathbb{E}[T|Z=0]}\\)\n\n\ní˜¹ì‹œ ìœ ë„ ê³¼ì •ì´ ì•Œê³  ì‹¶ë‚˜ìš”?\n\n\\(Z\\)ê°€ \\(Y\\)ì— ë¼ì¹˜ëŠ” causal effectëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\\(E[Y(Z=1)-Y(Z=0)]\\)\nìš°ë¦¬ê°€ ì•ì„œ ë°°ì› ë˜ 4ê°€ì§€ ê·¸ë£¹ì„ ìƒê°í•´ë³¼ê¹Œìš”? \\(Z\\)ì™€ \\(T\\)ì˜ ê°’ì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì´ í’€ì–´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\(\\begin{aligned} \\mathbb{E}[â€¦] &= \\mathbb{E}[â€¦|T(1)=1,T(0)=0]\\:P(T(1)=1,T(0)=0) \\quad (complier) \\\\ &+ \\mathbb{E}[â€¦|T(1)=0,T(0)=1]\\:P(T(1)=0,T(0)=1) \\quad (defier) \\\\ &+ \\mathbb{E}[â€¦|T(1)=1,T(0)=1]\\:P(T(1)=1,T(0)=1) \\quad (always-taker) \\\\ &+ \\mathbb{E}[â€¦|T(1)=0,T(0)=0]\\:P(T(1)=0,T(0)=0) \\quad (never-taker) \\end{aligned}\\)\nì—¬ê¸°ì„œ ëª‡ ê°€ì§€ í•­ì€ ìë™ìœ¼ë¡œ ì†Œê±°ë©ë‹ˆë‹¤.\n\\(\\begin{aligned} \\mathbb{E}[â€¦] &= \\mathbb{E}[â€¦|T(1)=1,T(0)=0]\\:P(T(1)=1,T(0)=0) \\quad (complier) \\\\ &+ \\mathbb{E}[â€¦|T(1)=0,T(0)=1]\\:P(T(1)=0,T(0)=1) \\quad (*defier) \\\\ &+ \\mathbb{E}[â€¦|T(1)=1,T(0)=1]\\:P(T(1)=1,T(0)=1) \\quad (**always-taker) \\\\ &+ \\mathbb{E}[â€¦|T(1)=0,T(0)=0]\\:P(T(1)=0,T(0)=0) \\quad (**never-taker) \\end{aligned}\\)\n*Defierì˜ ê²½ìš°, monotonicity ê°€ì •ì— ì˜í•´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê¹”ë”í•˜ê²Œ ë¬´ì‹œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n**Always-takerì™€ **Never-takerì˜ ê²½ìš°, \\(Z\\)ì™€ \\(T\\) ì‚¬ì´ì— (ë‚˜ì•„ê°€ \\(Z\\)ì™€ \\(Y\\) ì‚¬ì´ì—) causal effectê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ í•­ë“¤ë„ ê¹”ë”í•˜ê²Œ ë¬´ì‹œí•©ì‹œë‹¤.\nì´ë¥¼ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\(\\mathbb{E}[Y(Z=1)-Y(Z=0)|T(1)=1,T(0)=0]=\\frac{\\mathbb{E}[Y(Z=1)-Y(Z=0)]}{P(T(1)=1,T(0)=0)}\\)\nì´ ë•Œ, complierëŠ” \\(Z\\)ì˜ ê°’ê³¼ \\(T\\)ì˜ ê°’ì´ ê°™ìœ¼ë¯€ë¡œ, ì¢Œí•­ì˜ \\(Y(Z=0)\\)ê³¼ \\(Y(Z=1)\\)ë¥¼ \\(Y(T=0)\\)ì™€ \\(Y(T=1)\\)ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  instrumental unconfoundednessì— ì˜í•´ ìš°í•­ë„ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\(\\mathbb{E}[Y(T=1)-Y(T=0)|T(1)=1,T(0)=0] =\\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{P(T(1)=1,T(0)=0)}\\)\nì—¬ê¸°ì—ì„œ \\(P(T(1)=1,T(0)=0)\\)ë¥¼ ìì„¸íˆ ì‚´í´ë´…ì‹œë‹¤. ì „ì²´ ì§‘ë‹¨ì—ì„œ \\((T=1|Z=0)\\)ì¸ ì§‘ë‹¨ê³¼ \\((T=0|Z=1)\\)ì¸ ì§‘ë‹¨ì„ ì œì™¸í•œ ê²Œ \\((T(1)=1, T(0)=0)\\)ì¸ ì§‘ë‹¨ì´ê² ì£ ? ë”°ë¼ì„œ ìš°í•­ì€ ì•„ë˜ì™€ ê°™ì´ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\(\\begin{aligned} &=\\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{1-P(T=1|Z=0)-P(T=0|Z=1)} \\\\ &=\\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{1-P(T=1|Z=0)-(1-P(T=1|Z=1))} \\\\ &=\\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{P(T=1|Z=1)-P(T=1|Z=0)} \\end{aligned}\\)\nê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ, \\(T\\)ê°€ binaryì´ë¯€ë¡œ \\(T=1\\)ì— ëŒ€í•œ í™•ë¥ ì€ ê¸°ëŒ€ê°’ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\(\\begin{aligned} &=\\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{P(T=1|Z=1)-P(T=1|Z=0)} \\\\ &= \\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{\\mathbb{E}[T|Z=1]-\\mathbb{E}[T|Z=0]} \\quad (Wald\\;estimand) \\end{aligned}\\)\n\nğŸ’¡ ê´€ì ì„ ì•½ê°„ ë°”ê¿” ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\nQ. Wald estimand (\\(Z\\rightarrow Y\\) ì¸ê³¼ / \\(Z\\rightarrow T\\) ì¸ê³¼) ëŠ” ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì¼ê¹Œìš”?\nâ†’ \\(Z\\)ê°€ \\(T\\)ì— ì–´ë–¤ ì‹ìœ¼ë¡œ ì˜í–¥ì„ ì£¼ëŠëƒì— ë”°ë¼, ì§‘ë‹¨ì„ 4ê°œ sub-populationìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Wald estimandëŠ” ì´ ì¤‘ complier ì§‘ë‹¨ì˜ ATE.\nâ†’ ì¢€ ë” ê°•í•œ ê°€ì • (\\(T\\)ì™€ \\(Y\\)ê°€ linearityí•œ ê´€ê³„) ì´ ë§Œì¡±í•  ë•ŒëŠ”, ì „ì²´ ì§‘ë‹¨ì˜ ATEë¡œ ìƒê°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\nLATEì—ë„ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤\n\nmonotonicityê°€ í•­ìƒ ì¶©ì¡±ë˜ëŠ” ê±´ ì•„ë‹™ë‹ˆë‹¤.\nìƒí™©ì— ë”°ë¼, ì „ì²´ ì§‘ë‹¨ì˜ ATEê°€ í•„ìš”í•˜ì§€ Local ATEê°€ ê¶ê¸ˆí•˜ì§€ ì•Šì€ ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#iv-in-more-general-settings",
    "href": "posts/Introduction_to_causal_inference_Instrumental_Variables/Instrumental_Variables.html#iv-in-more-general-settings",
    "title": "10. Instrumental Variables",
    "section": "7. IV in More General Settings",
    "text": "7. IV in More General Settings\nì•ì„œì„œ \\(Y\\)ê°€ \\(T\\)ì— ëŒ€í•œ linear equationìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ê²½ìš°ë¥¼ ë‹¤ë¤˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì¢€ ë” í™•ì¥í•´ì„œ \\(Y\\)ê°€ \\(T\\)ì— ëŒ€í•´ ì¢€ ë” ë³µì¡í•œ í•¨ìˆ˜ë¡œ í‘œí˜„ë˜ëŠ” ê²½ìš°ë„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[ Y:=f(T,W)+U \\]\nìœ„ì™€ ê°™ì´ ë‚˜íƒ€ë‚´ë©°, ë”¥ëŸ¬ë‹ ë“±ì„ ì´ìš©í•´ \\(f\\)ë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_DID/DID.html",
    "href": "posts/Introduction_to_causal_inference_DID/DID.html",
    "title": "11. Difference-in-Difference(DID)",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì˜ ê¹€ì„±ìˆ˜, ë‚¨ê¶ë¯¼ìƒì…ë‹ˆë‹¤.Â \nIntroduction to Causal Inference ê°•ì˜ì˜ ì—´ ë²ˆì§¸ ì±•í„°ì´ë©°, í•´ë‹¹ ì±•í„°ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\nContents\n\nMotivation and Preliminaries\nDifference-in-Differneces Overview\nAssumptions and Proof\nProbelms with Difference-in-Differences\n\nâ—¦Â ê°•ì˜ ì˜ìƒ ë§í¬ :Â Chapter 10 - Difference in Difference\nÂ  Â  ì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!\n\n1. Motivation\nQ. ì¸ê³¼ê´€ê³„ë¥¼ ë°íˆë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?\nA. RCT, frontdoor adjustment, backdoor adjustment, do-calculus ë“±ì„ ì´ìš©í•´ confounding factorë¥¼ í†µì œí•˜ê³  ì—°ê´€ê´€ê³„ë¥¼ ë³´ì„¸ìš” (~Chapter 7)\nQ. unobserved confounding factorê°€ ìˆìœ¼ë©´ ì–´ë–¡í•˜ì£ ?\nA. ëª‡ ê°€ì§€ ê°€ì •ì„ í†µí•´ êµ¬ê°„ì„ ì¤„ì¼ ìˆ˜ë„ ìˆê³  (Chapter 8) ë„êµ¬ë³€ìˆ˜ë¥¼ ì´ìš©í•´ unobserved confounderì˜ ì˜í–¥ì„ ì—†ì•¨ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Chapter 9)\nQ. ê´œì°®ì€ ë„êµ¬ë³€ìˆ˜ë¥¼ ì°¾ëŠ” ê²Œ ë„ˆë¬´ í˜ë“¤ì–´ìš”.\nA. ë„êµ¬ë³€ìˆ˜ ì™¸ì—ë„, unobserved confounderë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ë²•ë“¤ì´ ì´ê²ƒì €ê²ƒ ìˆìŠµë‹ˆë‹¤.\nâ†’ ê·¸ ì¤‘ í•˜ë‚˜ê°€ ì˜¤ëŠ˜ ë‹¤ë£° ì´ì¤‘ì°¨ë¶„ë²•(Difference-in-Differences)ì…ë‹ˆë‹¤!\n\nDIDëŠ” í‰í–‰ ì¶”ì„¸ë¥¼ ê°€ì •í•©ë‹ˆë‹¤ (parallel trends assumption)\nì‰½ê²Œ ë§í•´, ì²˜ì¹˜ì§‘ë‹¨ê³¼ í†µì œì§‘ë‹¨ì´ ë‹¤ë¥¼ ìˆ˜ëŠ” ìˆì§€ë§Œ (= ë³€ìˆ˜ì˜ ê°’ì€ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ) ê°™ì€ ì¶”ì„¸ë¡œ ì›€ì§ì¸ë‹¤ëŠ” ê°€ì • (= unobserved confounderì˜ ì˜í–¥ì€ ë‘ ì§‘ë‹¨ì— ë™ë“±í•˜ë‹¤) â†’ ì²˜ì¹˜ì§‘ë‹¨ê³¼ í†µì œì§‘ë‹¨ì´ ì›€ì§ì´ëŠ” ì¶”ì„¸ê°€ ë‹¤ë¥´ë‹¤ë©´, ì´ëŠ” ì²˜ì¹˜ ë•Œë¬¸ì´ë‹¤ â†’ ì°¨ë¶„(difference)ì„ í†µí•´ confounderì˜ ì˜í–¥ì„ ìƒì‡„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n\n\n\n1-1 DID í™œìš© ì˜ˆì‹œ\n\në¬´ê¸° í”„ë¡œëª¨ì…˜ Aì™€ ë°©ì–´êµ¬ í”„ë¡œëª¨ì…˜ Bì˜ ìˆ˜ìµì„ ë¹„êµí•´ë´…ì‹œë‹¤.\nìš°ë¦¬ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ë°°ì› ìœ¼ë‹ˆ, â€œë‹¨ìˆœíˆ ì˜¬í•´ì—ëŠ” ë¬´ê¸° í”„ë¡œëª¨ì…˜ Aê°€ 110ì–µì˜ ìˆ˜ìµì„ ëƒˆê³ , Bê°€ 100ì–µì˜ ìˆ˜ìµì„ ëƒˆìœ¼ë‹ˆ, ë¬´ê¸° í”„ë¡œëª¨ì…˜ì´ 10ì–µ ì •ë„ ë” ì„±ê³¼ê°€ ì¢‹ì•˜ë‹¤â€ ë¡œ ë‹¨ìˆœíˆ ë¶„ì„í•˜ê³  ëë‚´ë©´ ì•ˆë©ë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´ ì™œ ì´ëŸ¬í•œ ì°¨ì´ê°€ ë°œìƒí–ˆì„ê¹Œ?\nì»¤ë®¤ë‹ˆí‹°ë¥¼ ë¶„ì„í•´ë³´ë‹ˆ, 2021.12ì›”ë¶€í„° ë¬´ê¸° ìœ„ì£¼ ì—…ë°ì´íŠ¸ê°€ ë˜ì—ˆë‹¤ëŠ” ì»¤ë®¤ë‹ˆí‹°ì˜ í‰ê°€ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ ê³¼ì—° ë¬´ê¸° ìœ„ì£¼ ì—…ë°ì´íŠ¸ê°€ í”„ë¡œëª¨ì…˜ì˜ ìˆ˜ìµì— ì˜í–¥ì„ ë¯¸ì³¤ì„ê¹Œ?\nQ. â€œë¬´ê¸° ìœ„ì£¼ ì—…ë°ì´íŠ¸â€ê°€ í”„ë¡œëª¨ì…˜ ìˆ˜ìµì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì³¤ë‚˜?\nì—¬ê¸°ì„œ â€œë¬´ê¸° ìœ„ì£¼ ì—…ë°ì´íŠ¸â€ê°€ í”„ë¡œëª¨ì…˜ì— ë¯¸ì¹œ Casual effectë¥¼ ë°œë¼ë‚´ê¸° ìœ„í•´ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ê²ƒì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\nê°™ì€ ì‹œì ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ê°œì²´ë¥¼ ê´€ì°°í•œ ìë£Œì—ì„œ ì˜¤ëŠ” ì°¨ì´\n\n\nê° ê°œì²´ë§ˆë‹¤ íŠ¹ì„±ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ì¦‰,ë¬´ê¸°ì™€ ë°©ì–´êµ¬ëŠ” ì „í˜€ ë‹¤ë¥¸ ê°œì²´\n\n\në‹¤ë¥¸ ì‹œì ì— ë”°ë¥¸ ë³€ìˆ˜ì˜ ë³€í™”\n\n\nì‘ë…„ê³¼ ì˜¬í•´ë¼ëŠ” ì‹œì ì˜ ë³€í™”\n\n\n\n\nÂ \në¬´ê¸° í”„ë¡œëª¨ì…˜ A\në°©ì–´êµ¬ í”„ë¡œëª¨ì…˜ B\n\n\n\n\n2021.05\n150ì–µ(A1)\n200ì–µ(B1)\n\n\n2022.05\n110ì–µ(A2)\n100ì–µ(B2)\n\n\n\n\në‹¤ë¥¸ ì‹œì ì— ë”°ë¥¸ ë³€ìˆ˜ì˜ ë³€í™” ì°¨ë¶„\n\nA1Â -Â A2Â : ë¬´ê¸° í”„ë¡œëª¨ì…˜ì˜ ì‹œê°„ì— ë”°ë¥¸ ë³€í™”\nB1Â -Â B2Â : ë°©ì–´êµ¬ í”„ë¡œëª¨ì…˜ì˜ ì‹œê°„ì— ë”°ë¥¸ ë³€í™”\n\në‹¤ë¥¸ ê°œì²´ë¥¼ ê´€ì°°í•œ ìë£Œì—ì„œ ì˜¤ëŠ” ì°¨ì´ ì°¨ë¶„\n\nA1Â -Â B1Â : ë¬´ê¸° ì—…ë°ì´íŠ¸ ì‹œì‘ ì „ ë¬´ê¸° í”„ë¡œëª¨ì…˜ê³¼ ë°©ì–´êµ¬ í”„ë¡œëª¨ì…˜ ê°„ì˜ ì°¨ì´\nA2Â -Â B2Â : ë¬´ê¸° ì—…ë°ì´íŠ¸ ì‹œì‘ í›„ ë¬´ê¸° í”„ë¡œëª¨ì…˜ê³¼ ë°©ì–´êµ¬ í”„ë¡œëª¨ì…˜ ê°„ì˜ ì°¨ì´\n\nì´ì¤‘ ì°¨ë¶„ Î´DD(Difference-in-Differences)\n(1) ì‹œê°„ì  íŠ¹ì„±ì„ ì œê±°Â (A2-A1) - (B2-B1)(2) ë‹¤ë¥¸ ê°œì²´ì—ì„œ ì˜¤ëŠ” ì°¨ì´ë¥¼ ì œê±° (A2-B2) - (A1-B1)\n=&gt; Term = (2) Term(1)Â \n\nì´ë ‡ê²Œ Differenceì— í•œ ë²ˆ ë‹¤ì‹œ Differenceë¥¼ í•˜ê¸° ë•Œë¬¸ì— Difference-in-Differences(DID)ë¼ ë¶ˆë¦¬ë©°, ì´ë ‡ê²Œ DIDëŠ” ë‘ ì§‘ë‹¨ ê°„ì˜ íŠ¹ì§• ì°¨ì´ë¥¼ ì œê±°í•˜ê³ , ì‹œì ì— ë”°ë¥¸ ê²°ê³¼ ë³€ìˆ˜ì˜ ë³€í™”ê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥´ê²Œ ì¼ì–´ë‚˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì°¸ê³  :NC Soft ë‹¨ë¹„ ë¸”ë¡œê·¸\nê·¸ëŸ¼ DIDë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì•Œì•„ì•¼í•˜ëŠ” ì„ í–‰ ê°œë…ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n\n\n\n1-2 Preliminaries\nì• ì±•í„°ì—ì„œ ë°°ìš´ í‰ê· ì²˜ë¦¬ íš¨ê³¼(ATE)ë¥¼ ë– ì˜¬ë ¤ ë³´ì„¸ìš”.\nATE:\n\\[\n\\begin{aligned}\\\n\\mathbb{E}[Y(1)-Y(0)]Â &=\\mathbb{E}[Y(1)]-\\mathbb{E}[Y(0)]Â \\\\\\\n&=\\mathbb{E}[Y(1)Â \\midÂ T=1]-\\mathbb{E}[Y(0)Â \\midÂ T=0]Â \\\\\\\n&=\\mathbb{E}[YÂ \\midÂ T=1]-\\mathbb{E}[YÂ \\midÂ T=0]\n\\end{aligned}\n\\]\nìœ„ì˜ ATEê°€ ì„±ë¦½í•˜ë ¤ë©´ ë§Œì¡±í•´ì•¼ í•˜ëŠ” ê°€ì •ì€ Unconfoundednessì˜€ìŠµë‹ˆë‹¤.\n\n\\((Y(0), Y(1)) \\perp T\\)\nUnconfoundedness\n\nATT:\nDifference in Difference ì—ì„œ í™œìš©í•˜ëŠ” ìƒˆë¡œìš´ Casual estimationì€ ATTë¼ê³  í•˜ë©° Treatmentê°€ (T=1)ì¼ ë•Œì˜ ATEì…ë‹ˆë‹¤.\nì•„ë˜ì˜ ìˆ˜ì‹ê³¼ ê·¸ë¦¼ì„ ë³´ì‹œì£ .\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y(1)-Y(0)Â \\midÂ T=1]Â &=\\mathbb{E}[Y(1)Â \\midÂ T=1]-\\mathbb{E}[Y(0)Â \\midÂ T=1]Â \\\\\\\n&=\\mathbb{E}[YÂ \\midÂ T=1](\\textÂ {Â consistencyÂ })-\\mathbb{E}[Y(0)Â \\midÂ T=1]Â \\\\\\\n&=\\mathbb{E}[YÂ \\midÂ T=1]-\\mathbb{E}[Y(0)Â \\midÂ T=0]Â \\quad(Y(0)Â \\perpÂ T)Â \\\\\\\n&=\\mathbb{E}[YÂ \\midÂ T=1]-\\mathbb{E}[YÂ \\midÂ T=0](\\textÂ {Â consistencyÂ })\\\n\\end{aligned}\n\\]\n\n\n\níŒŒë€ìƒ‰ ì : Control Group, ë¹¨ê°„ìƒ‰ ì : Treamtment Group\n\n\nìœ„ ì‹ì„ ë§Œì¡±ì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ATEì™€ëŠ” ë‹¤ë¥´ê²Œ Unconfoundednessë³´ë‹¤ ì•½í•œ ê°€ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n\\(Y(0) \\perp T\\)\nWeak UnconfoundednessÂ \n\n\n\n2 OverView\nControl Groupì€ Treamtment(T=1)ì— ì˜í–¥ì„ ë°›ì§€ ì•Šìœ¼ë‹ˆ, ì´ ë‘˜ì˜ ì°¨ì´ë¥¼ êµ¬í•˜ë©´ Unconfoundedness ê°€ì • í•˜ì—ì„œ ì•„ë˜ì˜ ì‹ê³¼ ê°™ì´ ë©ë‹ˆë‹¤.\n\\(\\mathbb{E}[Y(1)-Y(0)\\mid T=1]\\) (Without Time)\nê·¸ëŸ¼ ì´ì œ Y-axis(Time)ì„ ì¶”ê°€í•˜ì—¬ ì‹œê°„ì— ëŒ€í•œ ì¶•ì„ ë‚˜íƒ€ë‚´ë³´ê³ , ATTë„ ì‹œê°„ Termì„ ë„£ì–´ ì¼ë°˜í™” ì‹œì¼œë´…ì‹œë‹¤.\n\n\n\níŒŒë€ìƒ‰ ì : Control Group, ë¹¨ê°„ìƒ‰ ì : Treamtment Group\n\n\nì—¬ê¸°ì„œ Treatmentë¥¼ ë°›ëŠ” ê²ƒì€ ì˜¤ì§ Treatment Administered(ì–´ë– í•œ ì²˜ì¹˜ë¥¼ ì‹¤í–‰) ì´í›„ì˜ ì˜¤ë¥¸ìª½ ìƒë‹¨ì— ìˆëŠ” ìˆ˜ì‹ì…ë‹ˆë‹¤. ì¦‰, Treatment Groupì´ë©°, ì´ë•Œ Control Groupì€ ì–´ë– í•œ Treatmentë„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.\níƒ€ì„ t=0ì€ ì²˜ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸° ì „, t=1ì€ ì²˜ì¹˜ë¥¼ ì‹œí–‰í•œ í›„ë¡œ ë‘ê³  ATTì— Time Term(t)ë¥¼ ë„£ì–´ ë‹¤ìŒê³¼ ê°™ì´ ì¼ë°˜í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\n\\begin{aligned}\n\\text{ ATT estimand with time: } \\mathbb{E}\\left[Y_{1}(1)-Y_{1}(0) \\mid T=1\\right]\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[Y_{1}(1) \\mid T=1\\right]-{E}\\left[Y_{1}(0) \\mid T=1\\right](Counter Factual)\n\\end{aligned}\n\\]\ní•˜ì§€ë§Œ ì—¬ê¸°ì„œ \\({E} \\left[Y_{1}(0) \\mid T=1\\right]\\) ëŠ” Counterfactual ì´ê¸° ë•Œë¬¸ì—, ê´€ì¸¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë•Œ Control Groupì˜ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ Counterfactualë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\n\nìœ„ ê·¸ë¦¼ì—ì„œ ë³´ì´ëŠ” ì‹¤ì„ ê³¼ ê°™ì´ ì• ì´ˆë¶€í„° Treatmentë¥¼ ë°›ì§€ ì•Šì€ Control Groupì„ Treatment Groupì—ì„œ Counterfactual\n\\({E}\\left[Y_{1}(0)Â \\midÂ T=1\\right]\\) ì„ ëŒ€ì‹ í•©ë‹ˆë‹¤.\në”°ë¼ì„œ Difference-in-Differencesì˜ ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[Y_{1}(1)-Y_{1}(0) \\mid T=1\\right]=\n\\left(\\mathbb{E}\\left[Y_{1} \\mid T=1\\right]-\\mathbb{E}\\left[Y_{0} \\mid T=1\\right]\\right)-\\left(\\mathbb{E}\\left[Y_{1} \\mid T=0\\right]-\\mathbb{E}\\left[Y_{0} \\mid T=0\\right]\\right)\n\\end{aligned}\n\\]\nì—¬ê¸°ì„œ ì²« ë²ˆì§¸ Term \\(\\left(\\mathbb{E}\\left[Y_{1} \\mid T=1\\right]-\\mathbb{E}\\left[Y_{0} \\mid T=1\\right]\\right)\\) ì€ Treatment Groupì—ì„œ ë§Œë“¤ì–´ì§„ ì‚¼ê°í˜•ì—ì„œ ë†’ì´ì™€ ê°™ê³  ë‘ë²ˆì§¸ Term \\(\\left(\\mathbb{E}\\left[Y_{1} \\mid T=0\\right]-\\mathbb{E}\\left[Y_{0} \\mid T=0\\right]\\right)\\) Control Groupì—ì„œ ë§Œë“¤ì–´ì§„ ì‚¼ê°í˜•ì˜ ë†’ì´ì™€ ê°™ìŠµë‹ˆë‹¤.\n\nê·¸ë¦¬ê³  ì²«ë²ˆì§¸ Termê³¼ ë‘ë²ˆì§¸ Termì„ ì°¨ë¶„(Difference)í•˜ëŠ” ê²ƒì€ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ Purple lineì„ êµ¬í•˜ëŠ” ê²ƒê³¼ ê°™ìœ¼ë©°, ì´ëŠ” ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” Treatmentì— ì˜í•œ Causal estimandì…ë‹ˆë‹¤.\n\nDIDëŠ” Time-Invariant(ëŒ€ìƒ ë˜ëŠ” ì‹œê°„ ì°¨ì´ì—ì„œ ì˜¤ëŠ” ì´ì§ˆì„±) Confoundersì— ëŒ€í•´ Robustness í•œë°, ê·¸ ì´ìœ ëŠ” ë‹¤ë¥¸ ì‹œì ì— ë”°ë¥¸ ë³€ìˆ˜ì˜ ë³€í™” ì°¨ë¶„, ë‹¤ë¥¸ ê°œì²´ë¥¼ ê´€ì°°í•œ ìë£Œì—ì„œ ì˜¤ëŠ” ì°¨ì´ë¥¼ ì°¨ë¶„í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\n\n3. Assumptions\n\nâ†’(1) ìœ„ ê·¸ë¦¼ì—ì„œ \\(\\mathbb{E}\\left[Y_{1}(0) \\mid T=1\\right]\\) ëŠ” Counterfactual ì´ê¸° ë•Œë¬¸ì—, ê´€ì¸¡í•  ìˆ˜ ì—†ìŒ\n(ìš°ë¦¬ê°€ ê´€ì¸¡í•  ìˆ˜ ìˆëŠ” ê²ƒì€ \\({E}\\left[Y_{1}(1) \\mid T=1\\right]\\))\nâ†’(2) ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•ŠëŠ” Control Groupì˜ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ Counterfactualë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ ë§Œë“¦\n\nâ†’(3) ìœ„ ê·¸ë¦¼ì—ì„œ Control Groupì˜ ê¸°ìš¸ê¸°ë¥¼ í†µí•´ ê·¸ì–´ì§„ ì ì„ ì„ í†µí•´ Treatment Groupì—ì„œ Counterfactual \\({E}\\left[Y_{1}(0) \\mid T=1\\right]\\)ì„ ëŒ€ì‹ í•¨\nâ†’(4) Control Groupì˜ ê¸°ìš¸ê¸°ê°€ Treatment Groupì˜ Counterfactual, ì¦‰ \\({E}\\left[Y_{1}(0) \\mid T=1\\right]\\)ì„ ëŒ€ì‹ í•˜ê¸° ìœ„í•œ Assumptions ì´ í•„ìš”\n\nConsistency Assumption\n\nExtended to Time Causal estimand â†’ Statistical estimand\n\n*Counterfacutal Quantities \\({E}\\left[Y_{\\tau}(1) \\mid T=0\\right]\\)Â ì™€ \\({E}\\left[Y_{\\tau}(0) \\mid T=1\\right]\\)ëŠ” ê´€ì¸¡ ë¶ˆê°€í•˜ê¸° ë•Œë¬¸ì— Consistency Assumptionì„ í†µí•´ Statistical estimand ë¡œ ì¶”ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nParalled Trends Assumption\n\nCounterfacual \\({E}\\left[Y_{1}(1) \\mid T=0\\right]\\)ì´ Control Groupê³¼ ë¹„ìŠ·í•˜ê²Œ ë˜‘ê°™ì´ ì›€ì§ì¸ë‹¤ëŠ” ê°€ì •\n\n\n\nParalled Trends Assumption\n\n\n\nìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ê²ƒì€ ìœ„ ê·¸ë¦¼ì—ì„œ ì € Gray pointê°€ Treatment Groupì˜ Counterfactual \\(\\mathbb{E}\\left[Y_{1}(1) \\mid T=0\\right]\\) ì¸ê°€ë¥¼ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤.\nì¦‰, ì € ì²« ë²ˆì§¸ Blue lineì´ ë‘ ë²ˆì§¸ Blue lineê³¼ ê°™ì€ì§€ë¥¼ í™•ì¸í•´ì•¼ í•˜ëŠ” ê²ƒì´ë©°, ì´ê²ƒì€ ë‘ ì„ ì˜ ê¸°ìš¸ê¸°ê°€ ê°™ì€ê°€ë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë¬¸ì œâ‡’ ë”°ë¼ì„œ Parallel Trends Assumptionì´ë¼ê³  í•©ë‹ˆë‹¤.\nìœ„ ê°€ì •ì—ì„œ \\({E}\\left[Y_{0}(0) \\mid T=1\\right]\\)ëŠ” Consistencyë¥¼ í†µí•´ \\({E}\\left[Y_0 \\mid T=1\\right]\\) ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŒ. ë”°ë¼ì„œ í•˜ë‚˜ì˜ ê°€ì •ì´ ë” í•„ìš”í•©ë‹ˆë‹¤.\n\nNo Pretreatment effect Assumption\n\nì–´ë– í•œ ì²˜ì¹˜ì˜ ì‹œì ì´ ë˜ê¸° ì „ì—ëŠ” Treatment Groupì— ì–´ë– í•œ Treatmentë„ ì—†ë‹¤ëŠ” ê²ƒì„ ê°€ì •\n\n\n\n3. Proof\nìœ„ì—ì„œ ë°°ìš´ ê°€ì •ì„ í™œìš©í•˜ì—¬, ì¦ëª…í•´ë´…ì‹œë‹¤.\nìš°ë¦¬ê°€ ë³´ì—¬ì£¼ê³ ì í•˜ëŠ” DID ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\\[  \n\\begin{aligned}  \n&\\mathbb{E}\\left[Y_1(1)-Y_1(0)Â \\midÂ T=1\\right]=\\left(\\mathbb{E}\\left[Y_1Â \\midÂ T=1\\right]-\\mathbb{E}\\left[Y_0Â \\midÂ T=1\\right]\\right)-Â \\\\  \n&\\left(\\mathbb{E}\\left[Y_1Â \\midÂ T=0\\right]-\\mathbb{E}\\left[Y_0Â \\midÂ T=0\\right]\\right)  \n\\end{aligned}  \n\\]\n(1) Consistency ì¶©ì¡±í•˜ì§€ ì•ŠëŠ” Counterfacual Termì„ ì¢Œë³€ì— ë‘ê³ , Paralled Trend assumptionì„ í†µí•´ ìˆ˜ì‹ ì „ê°œ\n\\[  \n\\begin{aligned}(Assumptions)\\mathbb{E}\\left[Y_{1}(0) \\mid T=1\\right] &=\\mathbb{E}\\left[Y_{0}(0) \\mid T=1\\right]+\\mathbb{E}\\left[Y_{1}(0) \\mid T=0\\right]-\\mathbb{E}\\left[Y_{0}(0) \\mid T=0\\right] \\\\&=\\mathbb{E}\\left[Y_{0}(0) \\mid T=1\\right]+\\mathbb{E}\\left[Y_{1} \\mid T=0\\right]-\\mathbb{E}\\left[Y_{0} \\mid T=0\\right] \\\\&=\\mathbb{E}\\left[Y_{0}(1) \\mid T=1\\right]+\\mathbb{E}\\left[Y_{1} \\mid T=0\\right]-\\mathbb{E}\\left[Y_{0} \\mid T=0\\right] \\\\&=\\mathbb{E}\\left[Y_{0} \\mid T=1\\right]+\\mathbb{E}\\left[Y_{1} \\mid T=0\\right]-\\mathbb{E}\\left[Y_{0} \\mid T=0\\right]\\end{aligned}  \n\\]\n(2) ìœ„ì—ì„œ ë§Œë“¤ì–´ì§„ ìˆ˜ì‹ì„ ì•„ë˜ì˜ ì‹ì— ëŒ€ì…\n\\[  \n\\begin{aligned}\\mathbb{E}\\left[Y_{1}(1)-Y_{1}(0)Â \\midÂ T=1\\right]Â &=\\mathbb{E}\\left[Y_{1}(1)Â \\midÂ T=1\\right]-\\mathbb{E}\\left[Y_{1}(0)Â \\midÂ T=1\\right]\\\\&=\\mathbb{E}\\left[Y_{1}Â \\midÂ T=1\\right]-\\mathbb{E}\\left[Y_{1}(0)Â \\midÂ T=1\\right]\\end{aligned}Â Â Â Â   \n\\]\n(3) ê²°ë¡ \n\\[  \n\\begin{aligned}\\mathbb{E}\\left[Y_{1}(1)-Y_{1}(0)Â \\midÂ T=1\\right]Â &=\\mathbb{E}\\left[Y_{1}Â \\midÂ T=1\\right]-\\left(\\mathbb{E}\\left[Y_{0}Â \\midÂ T=1\\right]+\\mathbb{E}\\left[Y_{1}Â \\midÂ T=0\\right]-\\mathbb{E}\\left[Y_{0}Â \\midÂ T=0\\right]\\right)Â \\\\&=\\left(\\mathbb{E}\\left[Y_{1}Â \\midÂ T=1\\right]-\\mathbb{E}\\left[Y_{0}Â \\midÂ T=1\\right]\\right)-\\left(\\mathbb{E}\\left[Y_{1}Â \\midÂ T=0\\right]-\\mathbb{E}\\left[Y_{0}Â \\midÂ T=0\\right]\\right)\\end{aligned}  \n\\]\nì¦‰, ìš°ë¦¬ê°€ ì¦ëª…í•˜ê³ ì í•œ ì•„ë˜ ì‹ì„ ê°€ì •ì„ í†µí•´ ì„±ë¦½í•¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[  \n\\mathbb{E}\\left[Y_{1}(1)-Y_{1}(0) \\mid T=1\\right]=  \n\\left(\\mathbb{E}\\left[Y_{1}Â \\midÂ T=1\\right]-\\mathbb{E}\\left[Y_{0}Â \\midÂ T=1\\right]\\right)-\\left(\\mathbb{E}\\left[Y_{1}Â \\midÂ T=0\\right]-\\mathbb{E}\\left[Y_{0}Â \\midÂ T=0\\right]\\right)  \n\\]\n\n\n4. Problems with Difference-in-Differnces\n\nViolations of Paralle Trends\n\nDIDëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Parallel trands assumptionì„ ë”°ë¥´ì§€ë§Œ ì‹¤ì œì—ì„  ì´ ê°€ì •ì„ ë”°ë¥´ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.Â \n\\(Violation: \\quad \\mathbb{E}\\left[Y_{1}(0)-Y_{0}(0) \\mid T=1\\right] \\neq \\mathbb{E}\\left[Y_{1}(0)-Y_{0}(0) \\mid T=0\\right]\\)\n\nDID procedures rely on different parallel trends assumptions (PTAs), and recover different causal parameters (Michelle Marcus andÂ Pedro H. C. Santâ€™Anna)\n\n\nWe document a â€œrobustnessâ€ versus â€œefficiencyâ€ trade-off in terms of the strength of the underlying Parelle Trends assumtions (Michelle Marcus andÂ Pedro H. C. Santâ€™Anna)\n\nê·¸ëŸ´ë•ŒëŠ”, Wë¼ëŠ” ë³€ìˆ˜ë¡œ Conditioningí•˜ì—¬ Backdoor pathë¥¼ ë§‰ì•„ Parallel Trends ê°€ì •ì´ ì„±ë¦½ë˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nControl for relevant confounders\n\\(\\mathbb{E}\\left[Y_{1}(0)-Y_{0}(0) \\mid T=1, W\\right]=\\mathbb{E}\\left[Y_{1}(0)-Y_{0}(0) \\mid T=0, W\\right]\\)\ní•˜ì§€ë§Œ ìœ„ ì‹ì—ì„œ íšŒê·€ë¶„ì„ì—ì„œ ë§í•˜ëŠ” interaction termì´ Treatment ì™€ Time ì‚¬ì´ì— ìˆë‹¤ë©´, Wë¼ëŠ” ë³€ìˆ˜ë¡œ Conditioning í•œë‹¤ê³  í•´ì„œ Parallel Trendsë¥¼ ì„±ë¦½í•  ìˆ˜ ìˆë„ë¡ í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì£¼ìœ„í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.\n\\(Y:=\\ldots+T \\tau \\quad \\Longrightarrow \\quad \\text { Parallel trends violation }\\)\n\nParalle Trends is Scale-Specific\n\nthe parallel trends assumptionì´ ë§Œì¡±í•œë‹¤ê³  í•´ì„œ, Yì— transoformation ë˜ëŠ” Scaleì„ ì·¨í•œ ê°’ì´ parallel trends assumptionë¥¼ ë§Œì¡±í•œë‹¤ê³  í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì£¼ì˜í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.\n\\(\\mathbb{E}\\left[Y_{1}(0) \\mid T=1\\right]-\\mathbb{E}\\left[Y_{0}(0) \\mid T=1\\right]=\\mathbb{E}\\left[Y_{1}(0) \\mid T=0\\right]-\\mathbb{E}\\left[Y_{0}(0) \\mid T=0\\right]\\)\nì´ìƒìœ¼ë¡œ Chapter 10. Difference-in-Differenceë¥¼ ë§ˆì¹˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\nê°ì‚¬í•©ë‹ˆë‹¤.\n\n\n\n\nCitationBibTeX citation:@online{kim & minsang namgoong2023,\n  author = {Kim \\& Minsang Namgoong, Brady},\n  title = {11\\textbackslash. {Difference-in-Difference(DID)}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKim & Minsang Namgoong, Brady. 2023. â€œ11\\.\nDifference-in-Difference(DID).â€ November 14, 2023."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Graphical_Models/Graphical_Models.html",
    "href": "posts/Introduction_to_causal_inference_Graphical_Models/Graphical_Models.html",
    "title": "04. Graphical Models",
    "section": "",
    "text": "Contents\n\nGraphical modelsë€ ë¬´ì—‡ì¸ê°€ìš”?\nBayesian Networks\nCausal Graphs\nBasic building blocks of graphs\nD-separation\n\nâ—¦ ê°•ì˜ ì˜ìƒ ë§í¬ : Chapter 3 - Graphical Models\nì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!\n\n\n\n(1) Graphical modelsë€ ë¬´ì—‡ì¸ê°€ìš”?\n\nProbabilistic graphical models,Â which provides a mechanism forÂ exploiting structure in complex distributions to describe them compactly, and in a way that allows them to be constructed and utilized effectively.\nâ€œProbabilistic Graphical Models : Principles and Techniquesâ€ (2009, Daphne Koller & Nir Friedman)\n\n\nì •ì˜ : ë°ì´í„°(í™•ë¥ ë³€ìˆ˜)ê°„ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•´ì„œÂ ë³µì¡í•œ ë¶„í¬ë¥¼Â compactí•˜ê²ŒÂ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\nëª©í‘œ :Â \n\n\nâ—¦ í™•ë¥  ë³€ìˆ˜ê°„ì˜ êµ¬ì¡° í‘œí˜„ : ì¸ê³¼ê´€ê³„ (ë°©í–¥ì„±) or ìƒê´€ê´€ê³„\nâ—¦ ë³µì¡í•œ ë¶„í¬ë¥¼ í‘œí˜„ : í™•ë¥  ë³€ìˆ˜ë“¤ê°„ì˜ ê²°í•©í™•ë¥ ë¶„í¬ (Joint Distribution)\nâ—¦Â Compactí•˜ê²Œ í‘œí˜„ : ë¶„í¬ë¥¼ êµ¬ì„±í•˜ëŠ” Parameterì˜ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ í‘œí˜„Â \n\n\nGraph ì •ì˜ : Graphical modelì„ í†µí•´ ë³€ìˆ˜ë“¤ê°„ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•œ ë°©ë²•ì´ Graph ì…ë‹ˆë‹¤. (Chapter 3ì—ì„œëŠ”ìš”!)\nìš©ì–´ ì •ë¦¬ (Graph Terminology)\n\n\nâ—¦ Graph : ìˆ˜í•™ì ìœ¼ë¡œ Nodeì™€ Edgeì˜ ì§‘í•©ì…ë‹ˆë‹¤. &lt; \\(G = (V, E)\\) &gt;\nâ—¦ Node (Vertex) : Graphì—ì„œ ì£¼ë¡œ ë³€ìˆ˜ë“¤ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.Â â—¦Â Edge (Link)Â : Graphì—ì„œ ë³€ìˆ˜ë“¤ê°„ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nâ—¦ ê·¸ë˜í”„ ë°©í–¥ì„± ì—¬ë¶€ì— ë”°ë¼ 2ê°€ì§€ í˜•íƒœê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\nÂ  Â  1) Undirected Graph : ì•„ë˜ Nodesì™€ Edges ê·¸ë¦¼ ì²˜ëŸ¼ ë°©í–¥ì„±ì´ ì—†ëŠ” ê·¸ë˜í”„\nÂ  Â  Â  Â  Â - ì˜ˆì‹œ : Markov Random Fields, Boltzmann Machine\nÂ  Â  2) Directed Graph : ì•„ë˜ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì²˜ëŸ¼, í™”ì‚´í‘œê°€ ì¡´ì¬í•˜ëŠ” ê·¸ë˜í”„Â \nÂ  Â  Â  Â  Â - ì˜ˆì‹œ : Bayesian Networks, HMM (Hidden Markov Models), Latent Variable Models\nâ—¦ì˜í–¥ì„ ì£¼ëŠ” Nodeì´ë©´ Parent (Ancestor)ì´ê³ , ë°›ëŠ” NodeëŠ” Child (Descendant)ë¼ê³  ì •ì˜í•©ë‹ˆë‹¤.\n\nâ—¦ DAG (Directed Acyclic Graph) : Directed graphì—ì„œ cycleì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ê°€ ìˆì§€ë§Œ,\nÂ  Â  DAGëŠ” cycleì´ ì—†ëŠ” ë°©í–¥ì„± ê·¸ë˜í”„ì— í•´ë‹¹ í•©ë‹ˆë‹¤.\nÂ  Â  Â â†’ Cycleì´ ìˆëŠ” ê²½ìš°, Causal Inferenceì—ì„œ ë‹¤ë£¨ê¸° ê¹Œë‹¤ë¡œì›Œ, í•´ë‹¹ ê°•ì˜ì—ì„œëŠ” DAGë§Œ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤.\n\n\n\n\n(2) Bayesian Networks\n\nìœ„ì—ì„œ ì„¤ëª…ë“œë¦° ê²ƒ ì²˜ëŸ¼, ë°©í–¥ì„± ê·¸ë˜í”„ì— ì†í•˜ëŠ” Bayesian Networksì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\nì •ì˜ : ë³€ìˆ˜ ì§‘í•©ì˜ Dependency êµ¬ì¡°ì™€ ê²°í•©í™•ë¥ ë¶„í¬ë¥¼ ì¸ìˆ˜ë¶„í•´ ë°©ì‹ (Factorisation)ì„ í†µí•´, íš¨ê³¼ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” Probabilistic graphical models ì…ë‹ˆë‹¤.\ní™œìš© : DAGê°€ Bayesian Networksì— ì†í•˜ë¯€ë¡œ, Causal Modelì„ í™œìš©í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤..!\nJoint Distribution í‘œí˜„ ë°©ë²• :Â \n\n\nâ—¦ ë³€ìˆ˜ê°„ì˜ ê´€ê³„ (association)ë¥¼ ëª¨ë¥¸ë‹¤ë©´, í™•ë¥  ì—°ì‡„ë²•ì¹™ì„ ì‚¬ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì¡°ê±´ë¶€ ë¶„í¬ë“¤ì˜\nÂ  Â  (coditional distribution) ê³±ìœ¼ë¡œ ê²°í•©ë¶„í¬ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆì–´ìš”.\nÂ  Â - \\(p(x_1, \\dots, x_n) = p(x_1)\\prod_{i=2}^n p(x_i | x_{i-1}, \\dots, x_1)\\)\nâ—¦Â  ë§Œì•½ \\(x_i, i=1,\\dots, n\\) ê°€ ì´í•­(binary) ë³€ìˆ˜ë¼ë©´, \\(p(x_i | x_{i-1}, \\dots, x_1)\\)ì€ \\(2^{i-1}\\) ê°œì˜ ëª¨ìˆ˜ê°€ í•„ìš”í•´ìš”.\nÂ  Â  Â â†’ ê²°ë¡ ì ìœ¼ë¡œ ì¡°ê±´ë¶€ í™•ë¥ ì—ì„œ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ë³€ìˆ˜ê°€ ë§ì•„ì§€ë©´ ëª¨ìˆ˜ê°€ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ê²Œ ë©ë‹ˆë‹¤.\nÂ \nâ—¦Â  í•„ìš”ì—†ëŠ” ë³€ìˆ˜ëŠ” ê³ ë ¤í•˜ì§€ ë§ê³  ì˜í–¥ì„ ì£¼ëŠ” ë³€ìˆ˜ë§Œì„ ê³¨ë¼ì„œ ì¡°ê±´ìœ¼ë¡œ ì¤€ë‹¤ë©´, ê³ ë ¤í•´ì•¼í•˜ëŠ” ê²½ìš°ê°€ ì ì–´ì§€ë¯€ë¡œ\nÂ  Â  Â í•„ìš”í•œ ëª¨ìˆ˜ê°€ ì ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤..!Â \nÂ  Â - ì•„ë˜ ì˜ˆì‹œì—ì„œ \\((x_1, x_2, x_3)\\)ê°€ ì£¼ì–´ì§€ëŠ” ê²½ìš°, \\(x_4\\) = 1(or \\(x_4=0\\))ì¸ í™•ë¥ , ì¦‰ \\(p(x_4|x_3, x_2, x_1)\\)ë§Œ\nÂ  Â  Â ì •í•˜ë©´ ë‚˜ë¨¸ì§€ í™•ë¥ ì€ \\(1-p(x_4|x_3, x_2, x_1)\\)ë¡œ ìë™ìœ¼ë¡œ ì •í•´ì§€ê²Œ ë©ë‹ˆë‹¤.\nÂ  Â  Â â†’Â ë”°ë¼ì„œ, ê° \\((x_1, x_2, x_3)\\) ê²½ìš°ì— 1ê°œì˜ ëª¨ìˆ˜ë§Œì„ í•„ìš”ë¡œ í•˜ê²Œ ë©ë‹ˆë‹¤.\n\nâ—¦Â  ë‹¤ìŒ ë‚´ìš©ì—ì„œëŠ” í™•ë¥  ë¶„í¬ì˜ ì¡°ê±´ë¶€ ë…ë¦½ì— ëŒ€í•œ ê°€ì •ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤~!\n\n\nLocal Markov Assumption\n\n\nâ—¦ ì •ì˜ : DAGì˜ Parentê°€ ì£¼ì–´ì§€ë©´, ë…¸ë“œ XëŠ” ë‚˜ë¨¸ì§€ descendantsê°€ ì•„ë‹Œ ë…¸ë“œë“¤ê³¼ ë…ë¦½Â Â \nâ—¦ ëª©ì  : Conditional probabilityë¥¼ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œ ì…ë‹ˆë‹¤..!\nÂ  Â  (ë§ˆì¹˜, Markov chainì—ì„œ í˜„ì¬ \\(t\\)ì‹œì ì˜ í™•ë¥ ë¶„í¬ê°€ ì´ì „ \\((t-1)\\) ì‹œì ì—ë§Œ ì˜ì¡´í•˜ëŠ” ê²ƒì„ ìƒê°í•˜ë©´,\nÂ  Â  ì´í•´ê°€ ì¡°ê¸ˆ ì‰¬ìš¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤! - \\(P(X_t|X_{t-1}, \\dots, X_1) = P(X_t|X_{t-1})\\) )\nâ—¦ ê¸°ëŒ€íš¨ê³¼ : ê²°í•©ë¶„í¬ì—ì„œ, ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ë³€ìˆ˜ê°€ ì¤„ì–´ ê³ ë ¤í•´ì•¼í•˜ëŠ” ëª¨ìˆ˜ê°€ ì¤„ì–´ë“¤ê²Œ ë©ë‹ˆë‹¤!\nÂ  Â  â†’ \\(P(x_1, \\dots, x_4) = P(x_1) P(x_2|x_1) P(x_3 | x_2, x_1) P(x_4|x_3)\\)\n\nâ—¦ ì˜ˆì‹œ : ì•„ë˜ ê·¸ë¦¼ì—ì„œÂ \\(X_4\\)ëŠ” \\(X_3\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§„ë‹¤ë©´Â ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ê³¼ëŠ” ì¡°ê±´ë¶€ ë…ë¦½ì´ì—ìš”.\nÂ  Â  â†’ \\(P(X_4|X_3,X_2,X_1) = P(X_4|X_3)\\)\nâ—¦ ì§ê´€ : â€œì§€ëŠ¥ â†’ ì„±ì  â†’ ì¥í•™ê¸ˆ ìˆ˜ì—¬â€ ì´ë¼ëŠ” DAGë¥¼ ìƒê°í•´ë´…ì‹œë‹¤.\nÂ  Â  - ë§Œì•½ ì–´ë–¤ í•™ìƒì˜ ì„±ì ì„ ì•ˆë‹¤ë©´, ê·¸ í•™ìƒì˜ ì§€ëŠ¥ì„ ëª°ë¼ë„ ì¥í•™ê¸ˆì„ ìˆ˜ì—¬ì—¬ë¶€ë¥¼ ì•Œ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤..!\nÂ  Â  Â  \\(P(ì¥í•™ê¸ˆ|ì§€ëŠ¥,ì„±ì ) = P(ì¥í•™ê¸ˆ|ì„±ì )\\)\n\n\nBayesian Network Factoriation (Chain rule for Bayesian networks)\n\n\nâ—¦ ì •ì˜ : í™•ë¥ ë¶„í¬ Pì™€ DAGì¸ Gê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Gì— ë”°ë¥¸ Pì˜ Factorisationì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\nÂ  Â  \\(p(x_1, \\dots, x_n) =\\prod_{i=1}^n p(x_i | Pa_i)\\)\nâ—¦ Local Markov assumption âŸº Bayesian network factorization\nÂ  Â  â†’ ìœ„ì—ì„œ ë°°ìš´ Local Markov assumptionê³¼ Bayesian network factorizationì€ ê°™ì•„ìš”!\nÂ  Â  Â  Â (í•´ë‹¹ ë¶€ë¶„ì— ëŒ€í•œ ì¦ëª…ì€ Probabilistic Graphical Models ì±…ì˜ Chapter 3 ì°¸ê³  ë¶€íƒë“œë¦½ë‹ˆë‹¤)\nâ—¦ Local Markov assumptionì˜ í•œê³„ : ë…ë¦½ì„±(ë…ë¦½, ì¡°ê±´ë¶€ ë…ë¦½,â€¦)ì— ëŒ€í•´ì„œë§Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\nÂ  Â  â†’ ì¸ì ‘í•œ Nodeì— ëŒ€í•´ì„œ, ì¢…ì†ì„± (Dependence)ì— ëŒ€í•œ ë³´ì¥ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì¡°ê¸ˆ ë” ê°•í•œ ê°€ì •ì´ í•„ìš”í•´ìš”!\nÂ  Â  â†’ ê·¸ëŸ¬ë©´ Minimality assumptionì— ëŒ€í•´ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.\n\n\nMinimality assumption\n\n\nâ—¦ ì •ì˜ : í•´ë‹¹ ê°€ì •ì€ 2ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.Â \nÂ  Â  - Local Markov assumptionÂ \nÂ  Â  - Adjacent nodes in the DAG are dependent (DAGì—ì„œ ì¸ì ‘í•œ ë…¸ë“œë“¤ì€ ì˜ì¡´ì ì´ë‹¤)\nâ—¦ ì°¨ì´ì  : Local Markov assumptionê³¼ ë¹„êµÂ \nÂ  Â  - ì—°ê²°ëœ ë…¸ë“œ Xì™€ Yê°€ ìˆë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤.Â Â  Â  - Local Markov assumptionë§Œ ìˆëŠ” ê²½ìš°, ë…¸ë“œ \\(X\\)ì™€ \\(Y\\)ê°€ ìˆëŠ” ê²½ìš° \\(P(x,y) = P(x)P(y|x)\\) ë¿ë§Œ ì•„ë‹ˆë¼,Â \nÂ  Â  Â  \\(P(x,y) = P(x)P(y)\\) í˜•íƒœë„ë¡œ ì¸ìˆ˜ë¶„í•´ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ë…¸ë“œ \\(X\\)ì™€ \\(Y\\)ê°€ ë…ë¦½)Â \nÂ  Â  â†’Â Â ê·¸ëŸ¬ë‚˜, Minimality assumptionì—ì„œëŠ” ì¶”ê°€ì ì¸ ë…ë¦½ì„± ê°€ì •ì„ í—ˆìš©í•˜ì§€ ì•Šì•„ìš”.Â Â \nÂ  Â  Â  Â  Â (ê·¸ë˜ì„œ í•´ë‹¹ ê°€ì •ì—ì„œëŠ” \\(P(x,y) = P(x)P(y)\\)ì— ëŒ€í•´ ì´ì•¼ê¸°í•  ìˆ˜ ì—†ì–´ìš”)\n\n\n\n\n(3) Causal Graphs\n\nìœ„ì—ì„œ ë‹¤ë£¬ ë¶€ë¶„ì€ DAGì˜ ì—°ê´€ì„±ì— (Association) ëŒ€í•œ ë¶€ë¶„ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤.\nê·¸ëŸ¬ë‚˜, ì €í¬ê°€ ë‹¤ë¤„ì•¼í•  ì¸ê³¼ì„±ì— (Causation) ëŒ€í•´ì„œëŠ” ì¶”ê°€ì ì¸ ê°€ì •ì´ í•„ìš”í•´ìš” (Causal assumption)\nCausal GraphsÂ  : Bayesian Networks + ì¸ê³¼ì„± ê°€ì •(Causal Edges Assumption)Â \n\n\n\nWhat is a cause?Â \nÂ  Â  â—¦Â ë§Œì•½ ë³€ìˆ˜ Yê°€ ë³€ìˆ˜ Xì˜ ë³€í™”ì— â€‹â€‹ë”°ë¼ ë³€í•  ìˆ˜ ìˆë‹¤ë©´, XëŠ” Yì˜ ì›ì¸ì´ë¼ê³  í•©ë‹ˆë‹¤.Â \n(Strict) Causal Edges Assumption\nÂ  Â  â—¦Â Directed graphì—ì„œ, ëª¨ë“  ë¶€ëª¨(parent) ë…¸ë“œëŠ” ëª¨ë“  ìì‹(children) ë…¸ë“œì˜ ì§ì ‘ì ì¸ ì›ì¸ì…ë‹ˆë‹¤.\nÂ  Â  â†’ Minimality assumptionì˜ 2ë²ˆì§¸ ê°€ì •ì´(Adjacent nodes in the DAG are dependent)\nÂ  Â  Â  Â  ìì—°ìŠ¤ëŸ½ê²Œ strict causal edges assumptionìœ¼ë¡œ ì—°ê²°ë˜ë©°, ë¶€ëª¨ëŠ” ìì‹ì˜ causeë¼ê³  íŠ¹ì •í•˜ëŠ” ê°€ì •ì…ë‹ˆë‹¤.\n\n3)Â  Q : ê·¸ëŸ¬ë©´ Strictí•˜ì§€ ì•Šì€ ê°€ì •ë„ ìˆëŠ” ê±´ê°€ìš”? (non-strict assumption)\nÂ  Â  Â A : ë„¤! ê·¸ëŸ°ë°, ìš°ë¦¬ê°€ ê³µë¶€í•  ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” strict ê°€ì •ì„ ë§Œì¡±í•˜ëŠ” DAGì— ëŒ€í•´ì„œë§Œ ë‹¤ë£° ì˜ˆì •ì´ì—ìš”\n\n\nAssumptions Flowchart : DAGì˜ Causal dependenciesë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ 2ê°€ì§€ ê°€ì •ì„ ë°°ì› ìŠµë‹ˆë‹¤!\n\n\n1) Markov AssumptionÂ \nÂ  Â  : DAGì˜ ë¶€ëª¨ ë…¸ë“œê°€ ì£¼ì–´ì§€ë©´, ë…¸ë“œ XëŠ” ë‚˜ë¨¸ì§€ descendantsê°€ ì•„ë‹Œ ë…¸ë“œë“¤ê³¼ ë…ë¦½Â Â \n2) Causal Edges AssumptionÂ \nÂ  Â  : Directed graphì—ì„œ, ëª¨ë“  ë¶€ëª¨ ë…¸ë“œëŠ” ëª¨ë“  ìì‹ ë…¸ë“œì˜ ì§ì ‘ì ì¸ ì›ì¸\nÂ  Â  Â â†’ í•´ë‹¹ ê°€ì •ì€ Minimality Assumptionì„ ë‚´í¬í•˜ê³  ìˆì–´, ìœ„ì— Markov Assumptionìœ¼ë¡œ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤.\n\n\n\n\n\n(4) Graphical building blocks\n\nDAGì—ì„œ ê·¸ë˜í”„ë¥¼ ì´ë£¨ëŠ” êµ¬ì„±ìš”ì†Œì™€ íë¦„ì— ëŒ€í•´ì„œ ë°°ì›Œë³´ë ¤ê³  í•´ìš”.\nê·¸ë˜í”„ì˜ ìµœì†Œ êµ¬ì„± ìš”ì†Œ (D-separation ìš”ì†Œ)ëŠ” Chain, Fork, Immorality ì´ë ‡ê²Œ 3ê°€ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n\n\n\nChains & Forks : 3ê°œì˜ ë…¸ë“œë¡œ êµ¬ì„±ëœ DAG ì¤‘ Chain, ForkëŠ” ë™ì¼í•œ Dependency ì„±ì§ˆì„ ë³´ì…ë‹ˆë‹¤.\n\n\nâ—¦Â  ì„¤ëª… :Â \nÂ  Â  - \\(X_2\\)ê°€ ì£¼ì–´ì§€ì§€ ì•Šì€ ê²½ìš° : Â \\(X_1, X_3\\)ëŠ” ì§ì ‘ ì—°ê²°ë˜ì–´ìˆì§€ëŠ” ì•Šì§€ë§Œ, ì—°ê´€ì„±ì€ ì¡´ì¬í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â â†’ \\(X_1\\)ì—ì„œ \\(X_3\\)ë¡œ ê°€ëŠ” í†µë¡œê°€ ì°¨ë‹¨ë˜ì–´ ìˆì§€ ì•Šì•„, ê·¸ëŒ€ë¡œ ì •ë³´ê°€ \\(X_3\\)ê¹Œì§€ íë¥´ê²Œ ë©ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  Â  (Unblocked Path)\nÂ  Â  - \\(X_2\\)ê°€ ì£¼ì–´ì§„ ê²½ìš° : Â \\(X_1, X_3\\)ëŠ” ì¡°ê±´ë¶€ ë…ë¦½ì…ë‹ˆë‹¤. (ì—°ê´€ì„±ì€ ì‚¬ë¼ì§€ê²Œ ë©ë‹ˆë‹¤)\nÂ  Â  Â  Â â†’ \\(X_1\\)ì—ì„œ \\(X_3\\)ë¡œ ê°€ëŠ” í†µë¡œê°€ ì°¨ë‹¨ë˜ì–´ ìˆì–´,Â  \\(X_1\\)ì— ìˆëŠ” ì •ë³´ê°€ ë”ì´ìƒ íë¥´ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  Â  (Blocked Path)\nâ—¦Â  ëª©í‘œ : Causal associationì´ì™¸ì˜ Non-causal association ì˜í–¥ì„ ì œê±° (Path ì°¨ë‹¨)\nÂ  Â  - Chains : Mediator (ë§¤ê°œë³€ìˆ˜)ë¥¼ í†µì œÂ \nÂ  Â  - Forks : Confounder (êµë€ë³€ìˆ˜)ë¥¼ í†µì œ\n\nâ—¦Â  ì‚¬ë¡€ :Â \nÂ  Â  - Chains : ìœ„ì—ì„œ ì´ì•¼ê¸°í•œ ì‚¬ë¡€ì¸ â€œì§€ëŠ¥ â†’Â ì„±ì Â â†’ ì¥í•™ê¸ˆ ìˆ˜ì—¬â€ ì´ë¼ëŠ” DAGë¥¼ ìƒê°í•´ë´…ì‹œë‹¤.\nÂ  Â  Â  Â â†’ ë§Œì•½ ì„±ì ì„ ì•Œê³  ìˆë‹¤ë©´, ì§€ëŠ¥ê³¼ ì¥í•™ê¸ˆ ìˆ˜ì—¬ì— ëŒ€í•œ ì—°ê´€ì„±ì€ ë”ì´ìƒ ì¡´ì¬í•˜ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤!\nÂ  Â  Â  Â  Â  Â  &lt; ì„±ì ì„ ì•Œê³  ìˆê³ , ê·¸ì— ë”°ë¼ì„œ ì¥í•™ê¸ˆì„ ìˆ˜ì—¬ ë°›ê²Œëœ ê²ƒì´ê¸° ë•Œë¬¸ì´ì£  &gt;Â \nÂ  Â  - Forks : \\(X_1, X_2, X_3\\)ê°€ ì—°ê´€ë˜ì–´ ìˆëŠ” Fork í˜•íƒœë¥¼ ì„ í˜• ëª¨í˜•ìœ¼ë¡œ ì´í•´ í•´ë´…ì‹œë‹¤!Â Â \nÂ  Â  Â  Â â†’ \\(X_1 = X_2 + \\epsilon_1\\). \\(X_3 = X_2 + \\epsilon_2\\)Â Â \nÂ  Â  Â  Â â†’ \\(X_2 \\perp \\epsilon_1, \\epsilon_2\\) / \\(\\epsilon_1 \\perp \\epsilon_2\\)\nÂ  Â  Â  Â â†’ \\(X_2 = x\\) ë¡œ ì£¼ì–´ì§„ë‹¤ë©´, \\(x + \\epsilon_1 \\perp x + \\epsilon_2\\)\nÂ  Â  Â  Â  Â  Â  ì¦‰, \\(X_1\\)ê³¼ \\(X_3\\)ëŠ” ì¡°ê±´ë¶€ ë…ë¦½ì…ë‹ˆë‹¤.!\n\n\nColiders(Immoralities) and their Descendants :Â \n\n\nâ—¦Â  Immoralities vs Chains & Forks :Â Â \nÂ  Â  - ImmoralitiesëŠ” ì•ì—ì„œ ì„¤ëª…í•œ Chainsê³¼ Forksì™€ ë‹¤ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\nÂ  Â  - \\(X_1\\)ê³¼ \\(X_3\\)ì— ê³µí†µìœ¼ë¡œ ì˜í–¥ë°›ëŠ” ë³€ìˆ˜ì¸ \\(X_2\\)ê°€ ì£¼ì–´ì§„ë‹¤ë©´, \\(X_1\\)ê³¼ \\(X_3\\)ì— ì—°ê´€ì„±ì´ ìƒê¸°ê²Œ ë©ë‹ˆë‹¤.\nâ—¦Â  ëª©í‘œ : Causal associationì´ì™¸ì˜ Non-causal associationì˜í–¥ì„ ì œê±° (Pathë¥¼ ì°¨ë‹¨í•˜ì§€ ì•ŠìŒ)\nÂ  Â  - Immoralities : Colliderë¥¼ í†µì œí•˜ê²Œ ë˜ë©´, Associationì´ í˜•ì„±ë˜ë¯€ë¡œ í†µì œí•˜ì§€ XÂ \n\nâ—¦Â  ë§Œì•½ Colliderì˜ ìì† (Descendents)ì´ ì£¼ì–´ì¡Œë‹¤ë©´, ì–´ë–»ê²Œ ë ê¹Œìš”?Â Â  Â \n\\(X_4\\)ê°€ ì£¼ì–´ì§„ ê²½ìš°, \\(X_1\\)ê³¼ \\(X_3\\)ëŠ” ë” ì´ìƒ ë…ë¦½ì´ ì•„ë‹ˆê²Œ ë©ë‹ˆë‹¤\n\nâ—¦Â  ì‚¬ë¡€ 1 : ì˜ìƒê¸´ ì‚¬ëŒë“¤ì€ ë¬´ë¡€í•œê°€ìš”?Â  Â \n- \\(X_2\\) (ì—°ì• ì—¬ë¶€)ë¥¼ í†µì œí•˜ì§€ ì•Šì•˜ì„ ë•Œ : ì™¸ëª¨ (\\(X_1\\))ì™€ ì¹œì ˆí•¨ (\\(X_3\\))ì€ ë…ë¦½ì— ê°€ê¹ìŠµë‹ˆë‹¤.Â  Â \n- \\(X_2\\) (ì—°ì• ì—¬ë¶€)ë¥¼ í†µì œí•˜ì§€ í–ˆì„ ë•Œ : ì—°ì• ì—¬ë¶€ë¥¼ ì•ˆë‹¤ë©´ (\\(X_2\\)), Â ì—°ì• ë¥¼ í•˜ì§€ ì•ŠëŠ” ì‚¬ëŒë“¤ì€ ì™¸ëª¨ì™€ ì¹œì ˆí•¨ê³¼ ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ê²ƒì„ í™•ì¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤â€¦!\n\n\nâ—¦Â  ì‚¬ë¡€ 2 : ì•„ë˜ì™€ ê°™ì€ Data Generating Processë¥¼ ì‚´í´ë´…ì‹œë‹¤.\nÂ  Â  - \\(X_1\\) ~ \\(N(0,1)\\), \\(X_2\\) ~ \\(N(0,1)\\), \\(X_2 = X_1 + X_3\\)\nÂ  Â  - \\(X_2\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šì€ ê²½ìš°, ê³µë¶„ì‚°ì€ 0 (ë…ë¦½)\nÂ  Â  - \\(X_2\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§„ ê²½ìš°, ê³µë¶„ì‚°ì€ -1 (ìŒì˜ ìƒê´€ê´€ê³„)\n\n\n\n\n\n(5) D-separation\n\nì¼ë°˜ì ì¸ ì¸ê³¼ ëª¨í˜•ì€ ì•ì—ì„œ ë°°ìš´ Building Blocks (Chains, Forks, Immoralities) ì²˜ëŸ¼ ë‹¨ìˆœí•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆì§€ ì•Šì•„ìš”.\në”°ë¼ì„œ ë³µì¡í•œ ì¸ê³¼ëª¨í˜•ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ê·œì¹™ì— ëŒ€í•´ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\nD-separation :Â \n\n\nâ—¦Â  ì •ì˜ : ë‘ ë…¸ë“œì˜ ì§‘í•© \\(X\\), \\(Y\\) ì‚¬ì´ì˜ ëª¨ë“  ê²½ë¡œ(Path)ê°€ ë…¸ë“œ ì§‘í•© \\(Z\\)ì— ì˜í•´ ì°¨ë‹¨ë˜ëŠ” ê²½ìš°,\nÂ  Â  Â \\(X\\)ì™€ \\(Y\\)ëŠ” \\(Z\\)ì— ì˜í•´ d-separation ëœë‹¤ê³  ë§í•©ë‹ˆë‹¤.\nâ—¦Â  ì˜ë¯¸ :Â Â  Â \n- ê·¸ë˜í”„ ìƒì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” d-separationì€ í™•ë¥ ë¶„í¬ ìƒ ì¡°ê±´ë¶€ ë…ë¦½ì„ ì˜ë¯¸í•´ìš”Â  Â \n- d-separationì€ Local Markov assumptionë³´ë‹¤ ë” ê´‘ë²”ìœ„í•˜ë¯€ë¡œ Global Markov assumption(dependencies) ë¼ê³ ë„ í•´ìš”. ì´ ê²½ìš°ì—ëŠ” Localê³¼ Globalì„ êµ¬ë¶„í•˜ì§€ ì•Šê³  Markov assumptionì´ë¼ê³  í•©ë‹ˆë‹¤â€¦!\nÂ  Â  Â â†’ ë³µì¡í•œ ê·¸ë˜í”„ êµ¬ì¡°ì—ì„œ d-separated ë˜ëŠ” ê²½ìš°ë¥¼ ì•Œì•„ì•¼í•˜ê³  d-separatedì—ì„œ ë‚˜ì˜¤ëŠ”\nÂ  Â  Â  Â  Â  blocked pathë¥¼ í†µí•´ confounding association íš¨ê³¼ë¥¼ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤!\nÂ  Â  Â â†’ Graphical modelsì—ì„œ ì¡°ê±´ë¶€ ë…ë¦½ì˜ ê°€ì •/ì„±ì§ˆ(local Markov assumption, d-separated)ì€\nÂ  Â  Â  Â  Â í™•ë¥ ë¶„í¬ì˜ ë¶„í•´ì™€ ì—°ê²°ë©ë‹ˆë‹¤.\n\nâ—¦Â  ì˜ˆì‹œ :Â \nÂ  Â  - \\(T\\)ì™€ \\(Y\\)ëŠ” \\(\\{M_1, W_2, X_2\\}\\)ê°€ ì£¼ì–´ì§„ ê²½ìš°, d-separated ë ê¹Œìš”?Â  Â  - \\(T\\)ì™€ \\(Y\\)ëŠ” \\(\\{M_1, W_2, X_1, X_2\\}\\)ê°€ ì£¼ì–´ì§„ ê²½ìš°, d-separated ë ê¹Œìš”?\nÂ  Â  Â  Â â†’ ì •ë‹µì€ ëŒ“ê¸€ë¡œ ë‹¬ì•„ì£¼ì„¸ìš”\n\n\nTo be continued) ë°°ìš´ Causal Graphsë¥¼ í™œìš©í•´ ì¼ë°˜í™”ëœ ë°©ë²•ì¸ Strcutural Causal Modelsì— ëŒ€í•´Â ë°°ìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.\n\n\nReferenceÂ \n\nâ—¦ Lecture Notes : Bayesian Networks ê°•ì˜ ìë£Œ (ì¹´ë„¤ê¸° ë©œë¡  Uni)Â [Link]\nâ—¦ Books : Probabilistic graphical models principles and techniques [Link]\nâ—¦ Blogs : Collider ê´€ë ¨ ì ìš© NCSoft ì ìš© ì‚¬ë¡€Â [Link]\n\n\n\n\n\nCitationBibTeX citation:@online{hong2023,\n  author = {hong, seongchul},\n  title = {04\\textbackslash. {Graphical} {Models}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nhong, seongchul. 2023. â€œ04\\. Graphical Models.â€ November\n14, 2023."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Potential_Outcomes/Potential_Outcomes.html",
    "href": "posts/Introduction_to_causal_inference_Potential_Outcomes/Potential_Outcomes.html",
    "title": "03. Potential Outcomes",
    "section": "",
    "text": "Contents\n\nPotential Outcomesì´ë€ ë¬´ì—‡ì¸ê°€ìš”? (aka. Neyman-Rubin Causal model)\nì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œ\nì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ ì´í•´í•˜ëŠ”ë° í•„ìš”í•œ ê°€ì •\n\nê°•ì˜ ì˜ìƒ ë§í¬ : Chapter 2 - Potential Outcomes\nì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!\n\n\n\n(1) Potential Outcomesì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\n\nì •ì˜ : ê°ê°ì˜ Treatment Options í•˜ì—ì„œ, ë³¼ ìˆ˜ ìˆëŠ” ëª¨ë“  Outcomes ì…ë‹ˆë‹¤.\n(ê°™ì€ ì‹¤í—˜ ëŒ€ìƒì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì ì¬ì ì¸ ê²°ê³¼ë¥¼ ê³ ë ¤)\nPotential Outcomesì™€ Observed OutcomesëŠ” ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€ìš”?\n\nObserved Outcomes Y : ì‹¤í—˜ ëŒ€ìƒì—ê²Œ Treatmentë¥¼ ì£¼ì—ˆì„ ë•Œ, ë°œìƒí•œ ê²°ê³¼\nPotential Outcomes Y(t) : ëŒ€ìƒì—ê²Œ Treatmentë¥¼ ì£¼ì—ˆì„ ë•Œ, ë°œìƒí•  ìˆ˜ ìˆëŠ” ê²°ê³¼\nâ†’ Observed Outcomes Y â‰  Potential Outcomes Y(t)\n\nëª¨ë“  Potential OutcomesëŠ” ì ì¬ì ìœ¼ë¡œëŠ” ê´€ì¸¡ ê°€ëŠ¥í•˜ë‚˜, ëª¨ë‘ ê´€ì¸¡ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆì—ìš”!\nPotential Outcomesì— ëŒ€í•œ ì§ê´€\n\nâ—¦Â  Intuition : íƒ€ì„ë¨¸ì‹ ì„ í†µí•´ ì‹œê°„ì„ ë˜ëŒë¦´ ìˆ˜ ìˆì–´ì„œ, ê·¸ ë•Œ ë‹¤ë¥¸ actionì„ ì·¨í–ˆë”ë¼ë©´ ì–´ë–»ê²Œ ë˜ì—ˆì„ê¹Œìš”?\nâ—¦Â  Example : íƒ€ì´ë ˆë†€ ë³µìš©ê³¼ ë‘í†µÂ \nÂ  Â  - Factual : ì½”ë¡œë‚˜ ì˜ì‹¬ ì¦ìƒìœ¼ë¡œ íƒ€ì´ë ˆë†€ì„ ë¨¹ì—ˆë”ë‹ˆ, ë‘í†µì´ ì‚¬ë¼ì¡Œë‹¤\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â &lt;\\(do(T=0)\\)&gt;Â Â  Â  Â  Â  Â  Â  Â  Â  Â &lt;\\(Y\\_i(1) = 1\\)&gt;\nÂ  Â  - Counterfactual : íƒ€ì´ë ˆë†€ì„ ì•ˆë¨¹ì—ˆë”ë‹ˆ, ë‘í†µì´ ì‚¬ë¼ì§€ì§€ ì•Šì•˜ë‹¤\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â &lt;\\(do(T=1)\\)&gt;Â Â  Â  Â  Â  Â  Â  Â  Â  Â &lt; \\(Y\\_i(1) = 0\\)&gt;\nÂ  Â  â†’ Potential Outcomes : ë‘í†µì´ ì‚¬ë¼ì§„ ê²½ìš° & ë‘í†µì´ ì‚¬ë¼ì§€ì§€ ì•Šì€ ê²½ìš°\nÂ  Â  â†’ Observed Outcomes : ë‘í†µì´ ì‚¬ë¼ì§„ ê²½ìš°Â Â \nÂ  Â  Â Â \nÂ  Â  - íƒ€ì„ë¨¸ì‹ ì´ ìˆì–´ì„œ íƒ€ì´ë ˆë†€ ë¨¹ê¸° ì „ìœ¼ë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆì–´ì„œ, íƒ€ì´ë ˆë†€ì„ ë¨¹ì§€ ì•Šì€ ê²½ìš°ë¥¼ ê´€ì¸¡í•  ìˆ˜ ìˆë‹¤ë©´?\nÂ  Â  Â  íƒ€ì´ë ˆë†€ì€ ë‚˜ì˜ ë‘í†µ í•´ì†Œì— Causal Effectë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆì„ ê±°ì—ìš”!\nÂ  Â  Â  í•˜ì§€ë§Œ, í˜„ì‹¤ì€ íƒ€ì„ë¨¸ì‹ ì´ ì—†â€¦â€¦ì£ â€¦â€¦.. (ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œì— í•´ë‹¹ë©ë‹ˆë‹¤)\nâ—¦ Causationì€ ì²˜ì¹˜ (Treatment) ì´í›„, Potential Outcomesì— ëŒ€í•œ ì°¨ì´ë¡œ ì •ì˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â \n\nì²˜ì¹˜ì— ëŒ€í•œ ì¸ê³¼ íš¨ê³¼ = (Treatment ë°›ì€ ê²½ìš°ì— ëŒ€í•œ Observed Outcomes) - (Treatment ë°›ì§€ ì•Šì€ ê²½ìš°ì— ëŒ€í•œ Potential Outcomes)\níƒ€ì´ë ˆë†€ì— ëŒ€í•œ ì¸ê³¼íš¨ê³¼ = (íƒ€ì´ë ˆë†€ì„ ë¨¹ì€ í›„, ë‘í†µ ì—¬ë¶€ì— ëŒ€í•œ ê´€ì¸¡ ê²°ê³¼) - (íƒ€ì´ë ˆë†€ì„ ë¨¹ì§€ ì•Šê³ , ë‘í†µì— ëŒ€í•œ ì ì¬ì  ê²°ê³¼)\n\n\n\n\n(2) Fundamental Problem of Causal Inference\n\nPotential Outcomesì—ì„œ ë³¸ ê²ƒ ì²˜ëŸ¼, ê° ì‹¤í—˜ ëŒ€ìƒì—ì„œ Potential Outcomesì„ ë™ì‹œì— ê´€ì°°í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•´ìš”â€¦!\nì¦‰, ìš°ë¦¬ì—ê²Œ â€™ë§Œì•½â€™ì´ë¼ëŠ” ë°ì´í„° (Counterfactuals)ì€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n1)Â Potential Outcomesì— ëŒ€í•´ ë™ì‹œ ê´€ì¸¡ì´ ë¶ˆê°€ëŠ¥Â (ìš°ë¦° íƒ€ì„ë¨¸ì‹  ì—†ì–´ìš”)\nÂ Â Â - ë™ì¼í•œ ì‹¤í—˜ ëŒ€ìƒì— Treatmentë¥¼ ë‹¤ë¥´ê²Œ ì£¼ê³ , ê²°ê³¼ë¥¼ ë‘ ë²ˆ ê´€ì¸¡í•´ë„ ë ê¹Œìš”? No\nÂ  Â â†’ ë‘ ë²ˆì§¸ ê²°ê³¼ëŠ” ì²« ë²ˆì§¸ ê´€ì¸¡ ê²°ê³¼ì— ì˜í–¥ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â \n2)Â Causal Effect ê³„ì‚°ì„ ìœ„í•´, Counterfactuals (Missing values)ì„ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ì§€ì— ëŒ€í•œ ë¶€ë¶„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤!Â \nÂ  Â - ìš°ë¦¬ê°€ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ : Control Group (Treatmentë¥¼ ë°›ì§€ ì•Šì€ ê·¸ë£¹)\nÂ  Â - Causal Effect ì¶”ì •ì„ ìœ„í•´ í•„ìš”í•œ ë¶€ë¶„ : Counterfactuals (Treatment Groupì—ì„œ Treatmentê°€ ì—†ì„ ë•Œ ê²°ê³¼)\nÂ  Â â†’ Control Groupì´ Counterfactualsê³¼ ìµœëŒ€í•œ ê°€ê¹ê²Œ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤.\n\n\nÂ  Â  Â  Â  Â  (í›„ë°˜ë¶€ì˜ Ignorability/Unconfoundedness ê°€ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”!)\nÂ \n3)Â Selection BiasÂ Â : ì‹¤í—˜ ëŒ€ìƒì„ ëœë¤í•˜ê²Œ í• ë‹¹ í•˜ì§€ ì•ŠëŠ” ì´ìƒ, ì‹œìŠ¤í…œì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.\nÂ  Â - Control Groupê³¼ Counterfactuals ê°„ì˜ ì°¨ì´ = Selection Bias\nÂ Â Â - ì˜ˆì‹œ : ê³ ê°ì—ê²Œ ë…¸ì¶œëœ ë°°ë„ˆ ê´‘ê³ ë¥¼ ê³ ê°ì´Â ë³¼ì§€ ì•ˆë³¼ì§€ ì„ íƒí•˜ëŠ” ê±´ ì„ íƒ í¸í–¥ ë¬¸ì œë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆì–´ìš”.\n\nÂ  Â â†’Â ê·¸ë£¹ ê°„ ë¹„êµ ê°€ëŠ¥í•˜ì§€ ì•Šì€ ìƒíƒœë¼ë©´, ê´‘ê³ ë¡œ ì¸í•´ í´ë¦­ì„ (Causal Effect) í–ˆë‹¤ê³  ë§í•  ìˆ˜ ì—†ê²Œë©ë‹ˆë‹¤.\n\nì°¸ê³ ë¡œ, ê²°ê³¼ê°€ ê´€ì¸¡ë˜ê¸° ì „ê¹Œì§€ëŠ” Counterfactualsì¸ì§€ Factualsì¸ì§€ êµ¬ë¶„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ê´€ì¸¡ ì „ê¹Œì§€ í•´ë‹¹ ë¶€ë¶„ì€ Potential Oucomes ì…ë‹ˆë‹¤!\n\nğŸ’¡ Causal Inference vs Machine Learning\n- Causal Inference : Potential outcomesê¹Œì§€ ê³ ë ¤\n- Machine Learning : Potential Outcomes ê³ ë ¤ê°€ í•„ìš”í•˜ì§€ ì•Šê³  Observed outcomesë§Œ ê³ ë ¤\n\n\n(3) ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ ì´í•´í•˜ëŠ”ë° í•„ìš”í•œ ê°€ì •Â \n\nITE ê³„ì‚°ì˜ ì–´ë ¤ì›€ : (2)ë²ˆì˜ ì¸ê³¼ ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œ(Missing values)ì—ì„œ ë³´ì•˜ë˜ ê²ƒ ì²˜ëŸ¼,\nê°œê°œì¸ì— ëŒ€í•œ íš¨ê³¼ (ITE)ì— ëŒ€í•´ì„œ Treatment íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸°ê°€ ì–´ë ¤ìš´ ë¬¸ì œê°€ ìƒê²¨ìš”.\nATE ê³„ì‚° :\nQ : ë°˜ë©´, ATEëŠ” êµ¬í•  ìˆ˜ ìˆì„ê¹Œìš”?Â \nÂ  Â  Â  A : Yes, ê°œì¸ì´ ì•„ë‹Œ ì§‘ë‹¨ì— ëŒ€í•œ í‰ê·  íš¨ê³¼ëŠ” êµ¬í•  ìˆ˜ ìˆì–´ìš”.Â \nÂ  Â  Â  Â  Â  Â ì§‘ë‹¨ì€ ì¼ë°˜ì ìœ¼ë¡œ Control Group (ëŒ€ì¡°êµ°) vs Treatment Group (ì‹¤í—˜êµ°)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¸¡ì •í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  Â ì—„ë°€í•˜ê²Œ ë§í•˜ë©´ Statistical Estimandë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! (ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” 1/3 ì´ë„¤ìš”)\n\n\n\nQ : ê·¸ëŸ°ë°, ì‹¤ì œë¡œ ATEê°€ ê³„ì‚°ì´ ê°€ëŠ¥í•œ ê±¸ê¹Œìš”?\nÂ  Â  Â A : No, ê·¸ ì´ìœ ëŠ” ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œì¸ Counterfactuals (Missing values) ë•Œë¬¸ì´ì—ìš”.\nÂ  Â  Â  Â  Â  ìœ„ì˜ ê·¸ë¦¼ì—ì„œëŠ” Missing values (Selection Bias)ë¥¼ ë¬´ì‹œí•˜ê³  ê³„ì‚°í•œ Statistical Estimandì˜ ê²°ê³¼ì…ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  í•˜ì§€ë§Œ, ì €í¬ê°€ í•„ìš”í•œ ê±´ Causal Estimandì…ë‹ˆë‹¤.\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **\"Association is not Causation\"**\nÂ  Â â†’ Selection Biasë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•´ì„œëŠ” Control/Treatment ê·¸ë£¹ê°„ì— ë¹„êµê°€ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤!\n\n\n\nPotential Outcomes Frameworkì˜ Missing values ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê°€ì •ì„ ë°°ì›Œë´…ì‹œë‹¤!\n\nâ—¦Â Identification AssumptionÂ  Â \nÂ  â†’Â Â ATE (Average Treatment Effect)ê°€ Associational Differenceì™€ ê°™ì•„ì§€ê¸° ìœ„í•œ ê°€ì •Â  Â  Â \nÂ  Â  Â a. (Conditional) Exchangeability = Unconfoundness\nÂ  Â  Â b. Positivity = Overlap\nÂ  Â  Â c.Â No Interference\nÂ  Â  Â d.Â ConsistencyÂ \n\n* SUTVAÂ (StableÂ Unit-TreatmentÂ ValueÂ Assumption)Â  Â \n\ní•´ë‹¹ ê°€ì •ì€ No Interferenceì™€ Consistencyë¥¼ ê²°í•©í•œ ë¶€ë¶„ì´ì—ìš”.\n\n\n\na1. Exchangeability (Ignorability)\n\nì •ì˜ : \\(Treatment \\\\perp (Y(1), Y(0))\\)\nâ†’ Treatmentì™€ ë°œìƒí•œ ê²°ê³¼(Outcome)ì€ ë…ë¦½ (Treatmentì™€ ê´€ê³„ì—†ì´ ë°œìƒí•˜ëŠ” ê²°ê³¼ëŠ” ê°™ìŠµë‹ˆë‹¤!\n\nâ—¦Â \\(E[Y(1)|T=0] = E[Y(1)|T=1] = E[Y(1)]\\)\nâ—¦Â \\(E[Y(0)|T=0] = E[Y(0)|T=1] = E[Y(0)]\\)\n\ní•´ë‹¹ ê°€ì •ì„ í¬ê²Œ 2ê°€ì§€ ê´€ì ì—ì„œ ë°”ë¼ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nâ—¦Â Ignorability : ê´€ì¸¡ë˜ì§€ ì•Šì€ Missing valuesë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„ìš”.\nâ—¦Â Exchangeability : Treatment ê·¸ë£¹ê°„ì€ ì„œë¡œ êµí™˜(ë¹„êµ) ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nê°€ì •ì˜ ê¸°ëŒ€íš¨ê³¼\n\nâ—¦ Confounderë¥¼(X, ê³¼ê¸ˆìˆ˜ì¤€) ëœë¤í•˜ê²Œ í• ë‹¹í•˜ëŠ” íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Random Assignment)\nÂ  Â = \\(X\\)ê°€ \\(T\\)(í”„ë¡œëª¨ì…˜, Treatment)ì— í• ë‹¹ë˜ëŠ” ë°©ì‹ì€ Coin Flipê³¼ ê°™ì•„ìš”\nâ†’ ê·¸ë ‡ê²Œ ë˜ë©´, Treatmentë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ìš”ì¸ë“¤ì— ëŒ€í•´, í‰ê· ì ìœ¼ë¡œ ë™ì§ˆí•˜ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤!\nâ†’ ìˆœìˆ˜í•˜ê²Œ Treatment (í”„ë¡œëª¨ì…˜)ì— ëŒ€í•œ Causal Effect (ê²°ì œ íš¨ê³¼)ë¥¼ ì¶”ì • ê°€ëŠ¥í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.Â \n\n\në¬¸ì œì  : ë‹¤ì–‘í•œ Confoundersê°€ ì¡´ì¬í•˜ëŠ” í˜„ì‹¤ ìƒí™©ì—ì„œ, ë‘ ê·¸ë£¹ì´ Exchangeableí•˜ë‹¤ê³ Â \nê°€ì •í•˜ëŠ” ê²ƒì€ ë‹¤ì†Œ ë¹„í˜„ì‹¤ì  ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â \n\n\n\na2. Unconfoundedness (Conditional Exchangeability)\n\në“±ì¥ë°°ê²½ : ìœ„ì˜ Exchangeability ê°€ì •ì˜ ë¬¸ì œì ì—ì„œ ë§ì”€ë“œë ¸ë˜ ê²ƒ ì²˜ëŸ¼, Observational Study í™˜ê²½ì—ì„œëŠ”\ní˜„ì‹¤ì ì´ì§€ ì•Šì€ ê°€ì •ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â \nì •ì˜ : \\(Treatment|X \\perp (Y(1), Y(0))\\)\n\nÂ ì˜ˆì‹œ :\n\nâ—¦ ìƒí™© : \\(X\\)(ê³¼ê¸ˆ ìˆ˜ì¤€)ìœ¼ë¡œ ì¸í•´, \\(T\\)(í”„ë¡œëª¨ì…˜)ì˜ ìˆœìˆ˜í•œ íš¨ê³¼ë¥¼ ì•Œê¸° ì–´ë ¤ìš´ ìƒí™©ì…ë‹ˆë‹¤.\nâ—¦ ê°€ì • : Â Subgroup (ê³ ê³¼ê¸ˆ, ì¤‘ê³¼ê¸ˆ, ì €ê³¼ê¸ˆ)ì´ ì£¼ì–´ì¡Œì„ ë•Œ, Subgroupê°„ ë¹„êµê°€ ê°€ëŠ¥\nâ—¦ ì ìš© : \\(X\\) (ê³¼ê¸ˆ ìˆ˜ì¤€)ì— ëŒ€í•œ Subgroupì´ ì£¼ì–´ì¡Œì„ ë•Œ, í”„ë¡œëª¨ì…˜ ê·¸ë£¹ì€ êµí™˜ ê°€ëŠ¥\nÂ  Â â†’ ì´ë¡œ ì¸í•´,Â  Y (ê²°ì œ)ì— ëŒ€í•œ, Treatment (í”„ë¡œëª¨ì…˜)ì˜ íš¨ê³¼ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤\n\nê³„ì‚° ë°©ë²• : \\(X\\)ì— ëŒ€í•œ Marginalisation ë¶€ë¶„ë§Œ ì¶”ê°€ë˜ê³ , ë‚˜ë¨¸ì§€ëŠ” Exchangeabilityì™€ ë™ì¼í•©ë‹ˆë‹¤!\nConditional Exchangeability ê°€ì •ì„ ì´ìš©í•´ì„œ ATEë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\në¬¸ì œì  : Unobserved Confounders(\\(W\\))\n\nâ—¦ RCT (Randomized Controlled Trials) í™˜ê²½ì´ ì•„ë‹ˆë©´, ê°€ì •ì´ ìœ„ë°°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nâ—¦ ë˜í•œ ê´€ì¸¡ë˜ì§€ ì•ŠëŠ” êµë€ ë³€ìˆ˜ê°€ ë§ì€ ìƒí™©ì—ì„œ, UnconfoundednessëŠ” í…ŒìŠ¤íŠ¸ë¥¼ í•  ìˆ˜ ì—†ëŠ” ê°€ì •ì´ì—ìš”.\nÂ  Â  (ê·¸ë˜ì„œ, ìœ„ ê°€ì •ì€ ìœ„ë°°ë˜ê¸° ì‰½ìŠµë‹ˆë‹¤ã…œã…œ)\n\n\n\n\nb. Positivity (Common Support)\n\nì •ì˜ : \\(0 &lt; P(T=1 |X=x) &lt; 1\\)\nâ†’ ê³µë³€ëŸ‰ \\(X\\)ì´ ì£¼ì–´ì¡Œì„ ë•Œ, Treatmentê°€ ê³¨ê³ ë£¨ í• ë‹¹ë˜ì–´ì•¼ í•´ìš”.\nÂ  Â  Â ì¦‰, Treatmentë¥¼ ë°›ì€ ê·¸ë£¹ê³¼ ë°›ì§€ ì•Šì€ ê·¸ë£¹ì´ íŠ¹ì„±ì´ ìœ ì‚¬í•´ì•¼ í•©ë‹ˆë‹¤!\nPositivityë¥¼ ë³´ëŠ” ë‹¤ì–‘í•œ ê´€ì \n\n\nì¡°ê±´ë¶€ í™•ë¥  ê³„ì‚° : í•´ë‹¹ Positivity ê°€ì •ì´ ì—†ë‹¤ë©´, Causal Effectë¥¼ ì¶”ì •í•  ìˆ˜ ì—†ê²Œ ë©ë‹ˆë‹¤.\nÂ  Â  ì•„ë˜ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ë¶„ëª¨ ë¶€ë¶„ì´ \\(P(T=1|X=x)\\) ë˜ëŠ” \\(P(T=0|X=x)\\) 0ì´ ë˜ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤â€¦!\n\nOverlap : ì–´ë– í•œ Covariate \\(X\\)ì˜ ë¶„í¬ê°€ ì´ìƒì ì¼ê¹Œìš”?Â \nÂ  Â  Â Treatmentê°€ ê°ê° ì£¼ì–´ì¡Œì„ ë•Œ, Covariateì— ëŒ€í•œ ë¶„í¬ê°€ ë¹„ìŠ·í•´ì•¼ í•©ë‹ˆë‹¤!\n\n\nPostivityì™€ Unconfoundedness TradeoffÂ \nÂ  Â : Machine Learningì—ì„œì˜ ì°¨ì›ì˜ ì €ì£¼ ì²˜ëŸ¼, Conditioní•˜ëŠ” Covariateì˜ ì°¨ì›ì´ ì»¤ì§€ë©´ ì»¤ì§ˆ ìˆ˜ë¡ Overlapì´ ë˜ëŠ” ë¶€ë¶„ì´ ì ì  ì¤„ì–´ë“¤ê²Œ ë©ë‹ˆë‹¤.Â \nâ†’ ì¦‰, ë” ë§ì€ Covariatesì— Conditionì„ ì¤„ ìˆ˜ë¡, Unconfoundedness ê°€ì •ì€ ë§Œì¡±í•˜ê¸° ì‰¬ì›Œì§€ì§€ë§Œ, ë°˜ëŒ€ë¡œ ì°¨ì›ì´ ì»¤ì§€ê²Œ ë˜ì–´ Overlap (Positivity)ê°€ì •ì€ ë§Œì¡±í•˜ì§€ ëª»í•  í™•ë¥ ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.\n\n\n\n\nc.Â No Interference\n\nì •ì˜ : \\(Y\\_i(t\\_1,...,t\\_{i-1},t\\_i,t\\_{i+1}...,t\\_n) = Y\\_i(t\\_i)\\)\nâ†’Â ê°œê°œì¸ì˜ Outcomeì€ ë‹¤ë¥¸ ì‚¬ëŒì˜ Treatmentì— ì˜í–¥ì„ ë°›ì§€ ì•Šì•„ì•¼í•©ë‹ˆë‹¤.\nì˜ˆì‹œ :\n\nâ—¦ Treatment : ê°•ì•„ì§€ ì…ì–‘í•œ ê²½ìš° - \\(do(T=1)\\) / ì…ì–‘í•˜ì§€ ì•Šì€ ê²½ìš° - \\(do(T=0)\\)\nâ—¦ Outcome : \\(Y\\_i(1)\\) - í–‰ë³µí•¨, \\(Y\\_i(0)\\) - í–‰ë³µí•˜ì§€ ì•ŠìŒ\nâ—¦ ì‹¤í—˜ ëŒ€ìƒ ê°œì¸ì˜ Treatmentì— ëŒ€í•œ Outcome(í–‰ë³µ)ì€ ì£¼ë³€ ëŒ€ìƒìœ¼ë¡œë¶€í„° ì˜í–¥ì„ ë°›ì§€ ì•Šì•„ì•¼ í•´ìš”.\n\në¬¸ì œì  :\n\nâ—¦Â Nodeê°„ Connectionì´ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ì—ì„œëŠ” ê°€ì •ì´ ìœ„ë°°ë˜ê¸° ì‰½ìŠµë‹ˆë‹¤.\nÂ  Â (ì„œë¡œê°€ ì—°ê²°ì´ ë˜ì–´ìˆê¸° ë•Œë¬¸ì´ì£ !)\nâ—¦ ë‹¤ë¥¸ ì‚¬ëŒì˜ ì˜í–¥ì„ ë°›ì§€ ì•Šì•„ì•¼ í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë°›ëŠ” ê²½ìš°ê°€ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ìš”â€¦â€¦.\n\n\n\nd.Â Consistency\n\nì •ì˜ : \\(T=t \\\\Rightarrow Y=Y(t)\\)\nâ†’ ë™ì¼í•œ Treatmentì˜ ê²½ìš°, ê·¸ì— ë”°ë¥¸ ê²°ê³¼ë„ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤\n\nÂ  Â  Â  Â  Â  Â  â€œThere areÂ no multitple versionsÂ of Treatmentâ€\n\nì˜ˆì‹œ :Â \n\nâ—¦ Teatment : ê°•ì•„ì§€ ì…ì–‘í•œ ê²½ìš° - \\(do(T=1)\\) / ì…ì–‘í•˜ì§€ ì•Šì€ ê²½ìš° - \\(do(T=0)\\)\nâ—¦ Outcome : \\(Y\\_i(1)\\) - í–‰ë³µí•¨, \\(Y\\_i(0)\\) - í–‰ë³µí•˜ì§€ ì•ŠìŒ\nâ—¦ Consistency ê°€ì •ì— ë”°ë¥´ë©´, ê°•ì•„ì§€ë¥¼ ì…ì–‘í•œ ê²½ìš° \\(do(T=1)\\), 2ê°œì˜ Outcome (\\(Y\\_i(1)\\), \\(Y\\_i(0)\\)) ì¤‘ì—ì„œÂ \nÂ  Â  í•˜ë‚˜ì˜ ê²°ê³¼ì— ëŒ€í•´ì„œë§Œ ê´€ì¸¡ì´ ë˜ì–´ì•¼ í•´ìš”.\nÂ  Â  Â â†’ ì•„ë˜ì™€ ê°™ì´ ë™ì¼í•œ ì‹¤í—˜ ëŒ€ìƒì—ê²Œ Treatmentë¥¼ ì£¼ì—ˆì„ ë•Œ, ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ë©´ ê°€ì •ì— ìœ„ë°°ê°€ ëœ ê²ƒì…ë‹ˆë‹¤.\n\n\në¬¸ì œì  : ë‹¹ì—°í•´ë³´ì´ëŠ” ê°€ì •ì´ì§€ë§Œ, ì‹¤ì œ ì‹¤í—˜ì—ì„œëŠ” ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤!\n\n\n\ne. Trying it all together (Identifiability of the ATE)\n\nìœ„ì—ì„œ ë°°ìš´ 4ê°€ì§€ ê°€ì •ì„ ëª¨ë‘ ì¢…í•©í•´ì„œ, Causal Effectë¥¼ Identify í•  ìˆ˜ ìˆì–´ìš”.Â \n\n\nTo be continued) ì•ìœ¼ë¡œ ì¸ê³¼ì¶”ë¡ ì˜ ë‹¤ë¥¸ Frameworkì¸ Strcutural Causal Modelsì— ëŒ€í•´ ë°°ìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.\n\n\nReference\nâ—¦ Lecture Notes : 2021 Summer Session on Causal Inference (ë°•ì§€ìš© êµìˆ˜ë‹˜)Â [Link]\nâ—¦ Books : ë°ì´í„° ë¶„ì„ì˜ í˜ (ì´í†  ê³ ì´ì¹˜ë¡œ ì €) [Link]\n\n\n\n\nCitationBibTeX citation:@online{shin2023,\n  author = {shin, Jinsoo},\n  title = {03\\textbackslash. {Potential} {Outcomes}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nshin, Jinsoo. 2023. â€œ03\\. Potential Outcomes.â€ November 14,\n2023."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Causal_Models/Causal_Models.html",
    "href": "posts/Introduction_to_causal_inference_Causal_Models/Causal_Models.html",
    "title": "05. Causal Models",
    "section": "",
    "text": "Contents\n\nDo-operator and Interventional Distributions\nModularity Assumption\nBackdoor adjustment\nStructural causal models\n\nâ—¦Â ê°•ì˜ ì˜ìƒ ë§í¬ :Â Chapter 4 - Causal Models\nÂ  Â  ì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!\nâ—¦ ì´ë²ˆ ê°•ì˜ëŠ” ì•„ë˜ì˜ ë‚´ìš©ì„ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤.\nÂ  Â  Â Casual Inferenceë¥¼ í•  ë•Œ, ì´ë¡ ì ìœ¼ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” Causal Estimandë¥¼ ì—¬ëŸ¬ê°€ì§€ ê°€ì •ì„ í†µí•´ ê³„ì‚°í•  ìˆ˜ ìˆëŠ”\nÂ  Â  Â Statistical Estimandë¡œ ì¶”ì •í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë•Œ í•„ìš”í•œ ê°œë…ì¸ Causal Modelì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.\n\n\n\n\n(1) Do-operatorë€ ë¬´ì—‡ì¸ê°€ìš”?\n\nì •ì˜ : ì£¼ì–´ì§„ í˜„ìƒì„ ê·¸ëŒ€ë¡œ ê´€ì°°í•˜ëŠ”ê²Œ ì•„ë‹Œ, ë” ë‚˜ì•„ê°€ â€œê°œì…í•œë‹¤â€ë¼ëŠ” ê²ƒì„ í‘œí˜„í•˜ëŠ” ìˆ˜í•™ ì—°ì‚°ìì…ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â (Intervention)\nì—­í•  : Do-operatorë¥¼ í†µí•´, Treamentì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ëª¨ë“  ìš”ì¸ì˜ íš¨ê³¼ë¥¼ ë¬´ì‹œí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â (Treatmentì˜ ë¶€ëª¨ ë…¸ë“œ)\nConditioning (ì¡°ê±´) vs Intervention (ê°œì…)Â \n\n\nâ—¦ Conditioning on \\(T=t\\) : ì „ì²´ ëª¨ì§‘ë‹¨ or ê´€ì¸¡í•œ ë°ì´í„°ì—ì„œ Treatment \\(t\\)ë¥¼ ë°›ì€ ëª¨ì§‘ë‹¨ì˜ ë¶€ë¶„ ì§‘í•©ì— í•´ë‹¹í•©ë‹ˆë‹¤.\nâ—¦ Intervention on \\(T=t\\) : ì²˜ì¹˜í•œ ë¶€ë¶„ ì§‘í•©ì´ ì•„ë‹Œ, ì „ì²´ ëª¨ì§‘ë‹¨ì— ëŒ€í•´ì„œ \\(T=t\\) ë¡œ ì„¤ì •í•œ ê²ƒì„ ë§í•©ë‹ˆë‹¤.\nÂ  Â  Â - í†µìƒì ìœ¼ë¡œ Interventionì€ \\(do(T=t), do(t)\\)ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.\nÂ  Â  Â - ì´ëŠ” ì£¼ì–´ì§„ í˜„ìƒì„ ê·¸ëŒ€ë¡œ ê´€ì°°í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, Doing(ê°œì…)í•œë‹¤ë¼ëŠ” ì˜ë¯¸ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nâ—¦ ì•„ë˜ì™€ ê°™ì´ Conditioningê³¼ Interventionì€ ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ë°©ì‹ì´ë¯€ë¡œ, ë‹¤ë¥¸ ë°ì´í„° ë¶„í¬ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤.\n\n\n\n\nObservational DistributionÂ vs Interventional Distribution\n\n\nâ—¦ Observational distributionÂ \nÂ  Â  Â - í‘œí˜„ : \\(P(Y),P(Y,T,X)\\)Â  Â  Â - íŠ¹ì§• : ê°œì…(doing)ì´ ì—†ì´,Â ìƒì„±ëœ ë¶„í¬ë¥¼ Observational distributionì´ë¼ í•©ë‹ˆë‹¤.\nâ—¦ InterventionalÂ distributionÂ \nÂ  Â  Â -Â í‘œí˜„ : \\(P(Y|do(T=t)),P(Y|do(T=t),X=x))\\)\nÂ  Â  Â - íŠ¹ì§• : Treatmentì— ëŒ€í•œ ê°œì… (do-operator)ì´ ì¡´ì¬í•©ë‹ˆë‹¤.Â \nÂ  Â  Â  Â  â†’ Â ì²˜ì¹˜(\\(T\\))ë¥¼ í†µí•´ Randomized trialì‹¤í—˜ê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ í†µí•´ ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ë¥¼\nÂ  Â  Â  Â  Â  Â  Â  Causal Estimandë¼ê³  í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â  â†’Â ë°˜ë©´, do-operatorë¥¼ í¬í•¨í•˜ì§€ ì•Šì€ ì¶”ì •ì¹˜ (Estimand)ëŠ” Statistical Estimandë¼ê³  í•©ë‹ˆë‹¤.Â  Â  Â \nÂ  Â  Â - ì˜ˆì‹œ : \\(P(Y|do(T=t),Z = z)\\)ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¼ê¹Œìš”?\nÂ  Â  Â  Â  â†’ ëª¨ì§‘ë‹¨ì— ëŒ€í•´ì„œ \\(T=t\\)ì— ëŒ€í•œ ê°œì…ì„ ë°›ì€ Z=zì¸ ë¶€ë¶„ì§‘í•©ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nIdentifiability :\n\nâ—¦ ì •ì˜ : ê°œì…ì„ í†µí•´ ì–»ì€ Causal Estimandë¥¼ Identification ê°€ì •ì„ í†µí•´ Statistical Estimandë¡œ ë°”ê¾¸ëŠ” ê³¼ì •\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â (ì§ì ‘ ê³„ì‚°í•  ìˆ˜ ì—†ìŒ, Counterfactuals)Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â (ì§ì ‘ ê³„ì‚°ê°€ëŠ¥) Â  Â  Â  Â  Â Â \nÂ  Â  â†’ ì´ ë•Œ, Confounders \\(X\\)ë¥¼ ì œì–´í•˜ëŠ”ë° í•„ìš”í•œ Causal Modelsì˜ ìš”ì†Œë¥¼ ì´ë²ˆ Chapterì—ì„œ ë°°ìš¸ ì˜ˆì •ì…ë‹ˆë‹¤!\n\n\n(ì°¸ê³ ) Do-operator\nì—¬ê¸°ì„œ Do-operatorëŠ” ì‰½ê²Œ ë§í•´ì„œ â€œê°œì…í•œë‹¤â€ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ë‹¤ì‹œë§í•´ì„œ Do-operatorì˜ ì—­í• ì€, Treamentì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ëª¨ë“  ë³€ìˆ˜ì˜ íš¨ê³¼ë¥¼ ë¬´ì‹œí•˜ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n\nì—¬ê¸°ì„œ \\(P(Y|X)\\)ëŠ” ì¸ê³¼íš¨ê³¼ê°€ ì•„ë‹ˆì—ìš”. ì™œëƒí•˜ë©´ Confounderë¡œ ì¸í•œ, Backdoor pathê°€ ì—´ë ¤ ìˆê¸° ë•Œë¬¸ì´ì£ .\nDo-operatorë¥¼ ì ìš©í•˜ë©´, Xì— ì˜í–¥ì„ ì£¼ëŠ” Câ†’ X ì‚¬ì´ì˜ ì—°ê²°ê³ ë¦¬ë¥¼ ê·¸ë˜í”„ ìƒì—ì„œ ì´ë¡ ì ìœ¼ë¡œ ì—†ì• ëŠ” ê²ƒì´ë©°,ì´ ìƒíƒœì—ì„œ ê³„ì‚°í•œ \\(P(Y|do(X))\\) ê°’ì´ ë°”ë¡œ ì¸ê³¼ íš¨ê³¼ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\nì´ëŸ¬í•œ Do-operatorëŠ” ì‹¤ì œ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ê°’ì´ë¼ê¸° ë³´ë‹¤ëŠ” ì´ë¡ ì ì¸ ê°œë…ì´ë©°, ê·¸ë˜ì„œ ê²°êµ­ Do-operatorë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì¡°ê±´ë¶€ í™•ë¥ ë¡œÂ ë§Œë“¤ì–´ì¤˜ì•¼ í•˜ëŠ”ë°, ì´ë•Œ í•„ìš”í•œ ê°€ì •ì„ ì•ì—ì„œ ë°°ìš´ Identification ì…ë‹ˆë‹¤.\nâ€œì •ë¦¬í•˜ìë©´, Interventional probabilityë¥¼ í†µí•´ Casual effectë¥¼ ì •ì˜í•˜ê³ , Identifiableì¸ì§€ë¥¼ í†µí•´ì„œ ì‹¤ì œë¡œ ì¶”ì • ê°€ëŠ¥í•œì§€ì— ëŒ€í•´ íŒë‹¨í•˜ë©°, ì´ë¥¼ í†µí•´ ì¸ê³¼ ê´€ê³„ë¥¼ êµ¬í•´ë³´ìëŠ” ê²ƒì´ì£ â€\n\n\n(2) Modularity Assumption\n\nì•ìœ¼ë¡œ Identificationì„ í•˜ê¸° ìœ„í•´ì„œ ê°€ì •ì´ í•„ìš”í•œë°ìš”, í•´ë‹¹ ê°€ì •ì— ëŒ€í•´ ë°°ì›Œë³´ë„ë¡ í• ê²Œìš”!\nCausal Indentificationë¥¼ í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ ê°€ì • : Interventions are local\nê·¸ëŸ¬ë©´, Interventions are localì´ ë¬´ì—‡ì¼ê¹Œìš”?\n\n\nâ—¦Â Â ì˜ë¯¸ : ì–´ë–¤ ë…¸ë“œì— ê°œì…(Intervention)í•˜ê²Œ ë˜ë©´, ê°œì…ìœ¼ë¡œ ì¸í•œ ë³€í™”ê°€ localí•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\nâ—¦Â Â íš¨ê³¼ : ì¦‰, ê°œì…í•œ ë…¸ë“œ \\(X_i\\)ì— ëŒ€í•´ì„œ ë¶€ëª¨ ë…¸ë“œ \\(pa_i\\)ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ë§Œ ë³€í•˜ê³ , ë‚˜ë¨¸ì§€ ë…¸ë“œì—ì„œ ì£¼ëŠ” ì˜í–¥ì€ ìœ ì§€ê°€ ëœë‹¤ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤.\nâ—¦Â  ì˜ˆì‹œ : ì•„ë˜ ê·¸ë¦¼ì—ì„œ, í•´ë‹¹ ê°€ì •ì— ë”°ë¥´ë©´ ì› ì•ˆì— ìˆëŠ” \\(X_i\\)ì˜ ë¶€ëª¨ ë…¸ë“œ(\\(pa_i\\))ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ë§Œ ë³€í™”í•˜ê³ ,\nÂ  Â  Â ë‚˜ë¨¸ì§€ëŠ” ì˜í–¥ì€ ìœ ì§€ ë©ë‹ˆë‹¤.\n\n\n\nModularity assumption : Interventions are localì„ ì¼ë°˜í™”í•œ ê°€ì •\n\n\nâ—¦ ì •ì˜ :Â \n\nÂ  Â ì–´ë–¤ Graph ìƒ nê°œì˜ ë…¸ë“œê°€ ì¡´ì¬í•˜ê³ , ê·¸ ì¤‘ ê°œì…(intervention)ì„ í•œ ë…¸ë“œì˜ ì¸ë±ìŠ¤ ì§‘í•©ì„ Së¼ê³  í•œë‹¤ë©´,\nÂ  Â  1. ë…¸ë“œ iê°€ ê°œì…ë˜ì§€ ì•Šì€ ê²½ìš° (\\(iâˆ‰S\\)) ë…¸ë“œ i (\\(X_i\\))ì˜ ë¶€ëª¨ ë…¸ë“œ(\\(pa_i\\))ê°€ ë…¸ë“œ \\(i\\)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ê·¸ëŒ€ë¡œ ìœ ì§€\nÂ  Â  2. ë…¸ë“œ iê°€ ê°œì…ë˜ì—ˆë‹¤ë©´ (\\(iâˆˆS\\)), \\(x_i\\)ê°’ìœ¼ë¡œ ê°œì…í•œ ê²½ìš° \\(P(x_i|pa_i)\\) = 1,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \\(x_i\\)ê°’ìœ¼ë¡œ ê°œì…í•˜ì§€ ì•Šì€ ê²½ìš° \\(P(x_i|pa_i)\\) = 0\nÂ  Â  Â * [n] = 1,2,3,4â€¦,n ì´ë©° ê° ìˆ«ìëŠ” ë…¸ë“œì˜ indexë¥¼ ì˜ë¯¸\nÂ  Â  Â  Â  \\(X_i\\): Indexê°€ iì¸ ë…¸ë“œë¥¼ ì˜ë¯¸, \\(x_i\\) : ê°’(scalar)ì„ ì˜ë¯¸\n\nâ—¦ Assumption violation : ê·¸ëŸ¬ë©´, Modularity ê°€ì •ì´ ìœ„ë°°ëœë‹¤ëŠ” ê²ƒì€ ì–´ë–¤ ì˜ë¯¸ì¼ê¹Œìš”?\nÂ  Â  Â - ë…¸ë“œ \\(T\\) ì— ëŒ€í•œ ê°œì…(intervention)ì´ \\(T\\)ì˜ ë¶€ëª¨ ë…¸ë“œì— ëŒ€í•œ ì˜í–¥ë ¥ì— ë³€í™”ë¥¼ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼, Â \nÂ  Â  Â  Â \\(T_2\\)ì˜ ë¶€ëª¨ ë…¸ë“œì— ëŒ€í•´Â ë³€í™”ë¥¼ ì£¼ì—ˆì„ ë•Œ, â€œInterventionì´ localì´ ì•„ë‹ˆë‹¤â€ë¼ê³  í•©ë‹ˆë‹¤.\n\nâ—¦ ì˜ˆì‹œÂ :Â \nÂ  Â  (a) ê°œì…ì´ ì—†ëŠ” Observational distribution\nÂ  Â Â (b) \\(T\\)ì— ê°œì… : \\(T\\)ì˜ ë¶€ëª¨ ë…¸ë“œì—ì„œ ì˜¤ëŠ” ì˜í–¥ë§Œ ì‚¬ë¼ì§€ê³ , ë‚˜ë¨¸ì§€ëŠ” ìœ ì§€ë©ë‹ˆë‹¤.\nÂ  Â Â (c) \\(T_2\\)ì— ê°œì… : \\(T_2\\)ì˜ ë¶€ëª¨ ë…¸ë“œì—ì„œ ì˜¤ëŠ” ì˜í–¥ì´ ì‚¬ë¼ì§€ê³ , ë‚˜ë¨¸ì§€ëŠ” ìœ ì§€ë©ë‹ˆë‹¤.\nÂ  Â  Â â†’ \\(P(Y)\\), \\(P(Y|do(T=t))\\), \\(P(Y|do(T_2=t_2))\\)ëŠ” ì„œë¡œ ì—°ê´€ë˜ì§€ ì•Šì€ ì™„ì „íˆ ë‹¤ë¥¸ ë¶„í¬ê°€ ë©ë‹ˆë‹¤.\nÂ  Â  Â â†’ (b), (c) ì²˜ëŸ¼ edgeê°€ ì œê±°ëœ ê·¸ë˜í”„ë¥¼ Manipulated graphë¼ê³  í•´ìš”!\n\n\n\n\nTruncated factorization\n\n\nâ—¦ ìš°ë¦¬ëŠ” ë°©ê¸ˆ ë°°ìš´ Modularity assumptionì„ í†µí•´ ìƒˆë¡œìš´ ì‹ì„ ì¶”ë¡ í•´ë‚¼ ìˆ˜ ìˆì–´ìš”.Â \nâ—¦ ì •ì˜ : Bayesian network factorizationì—ì„œ modularity assumptionì´ ì ìš©ëœ ì‹ì…ë‹ˆë‹¤.Â \nâ—¦ ê³¼ì • : ê·¸ëŸ¼ ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ ë‚´ìš©ì„ Remind í•´ë³¼ê¹Œìš”?\nÂ  Â  1. Bayesian network factoriazation\nÂ  Â  Â  Â  Â = Chain rule of probability + Markov assumption\n\nÂ  Â  Â  Â  - Chain rule of probability : \\(P(x,y) = P(x)â‹…P(y|x)\\)\nÂ  Â  Â  Â  - Markov assumption : ëª¨ë“  ë…¸ë“œëŠ” ì˜¤ì§ ë¶€ëª¨ ë…¸ë“œë¡œë¶€í„° ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤.\nÂ  Â  2. Modularity assumption ì ìš©\nÂ  Â  Â  Â ì—¬ê¸°ì„œ ê°œì…(intervention)ì„ í•œ ë…¸ë“œì˜ ì¸ë±ìŠ¤ ì§‘í•©ì„ Së¼ê³  í•œë‹¤ë©´, \\(iâˆˆS\\) ì¸ ë…¸ë“œë“¤ì— ëŒ€í•´ì„œëŠ”,\nÂ  Â  Â  Â \\(P(x_i|pa_i)\\) = 1ì´ê¸° ë•Œë¬¸ì—, bayesian network factorization ê³„ì‚° ê³¼ì •ì—ì„œ ìƒëµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\nÂ  Â  Â  â†’ Â ë”°ë¼ì„œ \\(iâˆ‰S\\)ì¸ ë…¸ë“œë“¤ì— ëŒ€í•´ì„œë§Œ \\(P(x_i|pa_i)\\)ë¥¼ ê³„ì‚°í•˜ë©´ ë˜ë©°, bayesian network factorization\nÂ  Â  Â  Â  Â  Â ì‹ì—ì„œ ì•„ë˜ì™€ ê°™ì€ ì‹ì„ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nÂ \n\nâ—¦ ì˜ˆì‹œ : \\(P(y|do(t))\\)ë¥¼ Identify í•´ë´…ì‹œë‹¤.\nâ†’ Treatment \\(T\\)ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë¶€ë¶„ì´, ì œê±° (Trucated) ë©ë‹ˆë‹¤.\n\n\n\n\n(3) Backdoor adjustment\n\nì´ì „ ë‚´ìš©ì—ì„œ ê°œì…(Doing)ì„ í†µí•´ì„œ Treatmentì— ì˜í–¥ì„ ì£¼ëŠ” ì™¸ë¶€ ë³€ìˆ˜(Backdoor)ë¥¼ ì°¨ë‹¨í•˜ë©´ì„œ\nCasual effectë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤ê³  í–ˆìŠµë‹ˆë‹¤.\n\n\n\nQ : ê·¸ëŸ¬ë©´, Observational dataì—ì„œ ì–´ë–»ê²Œ Causal effectë¥¼ êµ¬í•  ìˆ˜ ìˆì„ê¹Œìš”?\nÂ  Â  Â A : ê´€ì¸¡ ë°ì´í„°ì¸ observational dataì—ì„œëŠ” ê°œì…(intervention)ì„ í†µí•´ ê·¸ë˜í”„ë¥¼ ë§ˆìŒëŒ€ë¡œ ë³€ê²½í•˜ê¸°ëŠ” ì–´ë ¤ì›Œìš”.\nÂ  Â  Â  Â  Â  Â  Â í•˜ì§€ë§Œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ Observational dataì˜ Graphì— ì¡°ê±´ì„ ì¶”ê°€í•œë‹¤ë©´, \\(P(Y|do(X))\\)ì™€ ë™ì¼í•œ íš¨ê³¼ë¥¼\nÂ  Â  Â  Â  Â  Â  Â ì¤„ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œìš”?\n\nÂ  Â  * ì•„ë˜ ê·¸ë˜í”„ì—ì„œì˜ Backdoor Paths (Non-causal association)\nÂ  Â  Â  Â â—¦Â \\(T - W - Y\\)\nÂ  Â  Â  Â â—¦ \\(T - C - Y\\)\n\n\nQ : ê·¸ë ‡ë‹¤ë©´ ì–´ë– í•œ ì¡°ê±´ì„ í†µí•´, Observation dataì—ì„œ Doing(ê°œì…)í•œ ê²ƒê³¼ ë™ì¼í•œ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆì„ê¹Œìš”?\nÂ  Â  Â A : Observation data Graphì— ì¶”ê°€í•  ì¡°ê±´ì— ëŒ€í•œ ê¸°ì¤€ì´ í•„ìš”í•©ë‹ˆë‹¤. í•´ë‹¹ ì¡°ê±´ì„ ì •ë¦¬í•œ ê²ƒì´ ë°”ë¡œ,\nÂ  Â  Â  Â  Â  Â Backdoor criterionì…ë‹ˆë‹¤.\n\n\n\nBackdoor Criterion :\n\n\nâ—¦ ì •ì˜ : \\(T â†’ Y\\)ê°„ì˜ Causal associationì„ ì œì™¸í•œ ëª¨ë“  Backdoor pathsë¥¼ ë§‰ì„ ìˆ˜ ìˆëŠ” ë³€ìˆ˜ë“¤ì˜ ì§‘í•© \\(W\\)\nâ—¦ ì¡°ê±´ :\nÂ  Â 1. ì§‘í•© WëŠ” Tì—ì„œ Yë¡œ ê°€ëŠ” ëª¨ë“  Backdoor pathsë¥¼ block\nÂ  Â 2. ì§‘í•© WëŠ” Tì˜ ì–´ëŠ ìì†ë„ í¬í•¨í•˜ì§€ ë§ì•„ì•¼ í•¨\nâ—¦ Sufficient adjustment set : Modularity ê°€ì •ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë³€ìˆ˜ë“¤ì˜ ì§‘í•© \\(W\\)ê°€ Backddor Criterionì„\nÂ  Â  ë§Œì¡±í•œë‹¤ë©´, \\(W\\)ë¥¼ Sufficient adjustment setì´ë¼ê³  í•©ë‹ˆë‹¤.\nâ—¦ ì˜ë¯¸ : \\(W\\)ê°€ Backdoor Criterionì„ ë§Œì¡±í•˜ê²Œ ë˜ë©´, \\(T\\)ì— ëŒ€í•œ \\(Y\\)ì˜ Causal Effectë¥¼ Identify í•  ìˆ˜ ìˆì–´ìš”!\nâ—¦ ì˜ˆì‹œÂ :Â Â  Â \n1. Â ë§Œì•½ confounderê°€ ìˆë‹¤ë©´, confounderë¥¼ í†µì œë¥¼ í•´ì•¼ backdoor pathë¥¼ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ confounderì˜ ì§‘í•©ì´ backdoor criterionì„ ë§Œì¡±í•˜ëŠ” ì§‘í•©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê² ì£ .Â \n2.Â ë°˜ë©´ì— colliderëŠ” í†µì œí•˜ë©´ ì•ˆë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ëŸ¬í•œ colliderëŠ” backdoor criterionì„ ë§Œì¡±í•˜ëŠ” ì§‘í•©ì´ë¼ê³  ë³¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nÂ  Â  Â â†’Â ì¦‰, ìš°ë¦¬ê°€ í•´ì•¼í•˜ëŠ” ê²ƒì€ backdoor criterionì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  ì§‘í•©ì„ í†µì œí•´ì•¼í•´ìš”!\nâ—¦ ì¦ëª… : ê·¸ë ‡ë‹¤ë©´ ì •ë§ Backdoor adjustmentë¥¼ í†µí•´ Doing(ê°œì…)ì˜ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ê¹Œìš”?\nÂ  Â  1. Â \\(P(Y|do(t),W)=P(Y|T,W)\\)Â  &lt; line 1 to line 2 &gt;\nÂ  Â  Â  Â  Â  - Backdoor criterion 1ë²ˆ ì¡°ê±´ì— ì˜í•´ \\(W\\)ëŠ” ëª¨ë“  backdoor pathsë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  â†’ ë”°ë¼ì„œ, \\(T\\)ë¡œ ë“¤ì–´ì˜¤ëŠ” edgeì˜ ì˜í–¥ì´ ì œê±° ë©ë‹ˆë‹¤!\nÂ  Â  Â  Â  Â  - ì¢Œë³€ : \\(do(t)\\)ì˜ modularity assumptionì— ì˜í•´ \\(T\\)ì— ë“¤ì–´ì˜¤ëŠ” edgeì˜ ì˜í–¥ì´ ì œê±°\nÂ  Â  Â  Â  Â  -Â ìš°ë³€ : \\(W\\)ë¥¼ conditioní•¨ìœ¼ë¡œì¨, \\(T\\)ë¡œ ë“¤ì–´ì˜¤ëŠ” edgeì˜ ì˜í–¥ì„ ì œê±°\nÂ  Â  2.Â Â \\(P(W|do(t)) = P(W)\\) &lt; line 2 to line 3 &gt;\nÂ  Â  Â  Â  Â  - \\(T=t\\) ë¼ê³  í†µì œë¥¼ í–ˆìœ¼ë¯€ë¡œ, \\(W\\)â†’\\(T\\) ê´€ê³„ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤. (ë…ë¦½)\nÂ  Â  Â  Â  Â  â†’Â  ë”°ë¼ì„œ, Backdoor adjustmentë¥¼ í†µí•´ observational dataë¥¼ ê°€ì§€ê³  ì•ì—ì„œ Casualë¡œ ì •ì˜í•œÂ  Â  Â  Â  Â \nÂ  Â  Â  Â  Â  Â  Â  Â \\(P(Y|do(t))\\)ë¥¼ ê·œëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\nBackdoor Criterionê³¼ D-separation : D-separationì„ backdoor criterion ê°€ì§€ê³  ì •ì˜í•´ë´…ì‹œë‹¤.\n\n\n1. Backdoor criterion &lt; ì™¼ìª½ ê·¸ë¦¼ &gt;\nÂ  Â  Â -Â ì¡°ê±´ 1. ì§‘í•© \\(W\\)ëŠ” \\(T\\)ì—ì„œ \\(Y\\)ë¡œ ê°€ëŠ” ëª¨ë“  backdoor pathsë¥¼ ë§‰ì•„ì•¼í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â ì—´ë ¤ ìˆëŠ” Backdoor pathëŠ” \\(W_2\\) or \\(W_1\\)â†’ Blocked\nÂ  Â  Â -Â ì¡°ê±´ 2. ì§‘í•© \\(W\\)ëŠ” \\(T\\)ì˜ ì–´ëŠ ìì†ë„ í¬í•¨í•˜ì§€ ë§ì•„ì•¼ í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â \\(T\\)ì˜ ìì†ì¸ \\(X_2\\)ê°€ ë§‰í˜€ ìˆìŠµë‹ˆë‹¤. â†’ Unblocked\n2. Â Backdoor adjustment ì²˜ë¦¬ ëœ ê·¸ë˜í”„ \\(G\\) &lt; ê°€ìš´ë° ê·¸ë¦¼ &gt;\nÂ  Â  Â -Â ê·¸ë¦¬ê³  ë‹¨ í•˜ë‚˜ì˜ Associationë„ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¡°ê±´ë¶€ ë…ë¦½ì„ ì˜ë¯¸í•˜ëŠ” D-separatedëŠ” \\(T\\)â†’\\(M_1\\)ìœ¼ë¡œ ê°€ëŠ”\nÂ  Â  Â  Â edgeë¥¼ ì œê±°í•˜ê²Œ ë©ë‹ˆë‹¤.\n3. D-separation &lt; ì˜¤ë¥¸ìª½ ê·¸ë¦¼ &gt;\nÂ  Â  Â -Â ì´ë ‡ê²Œ í•˜ì—¬ ìƒì„±ëœ ê·¸ë˜í”„ë¥¼ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ \\(G_{\\bar{T}}\\)ë¡œ í‘œí˜„í•˜ê³ , \\(W\\) ì»¨ë””ì…˜ ì•„ë˜ì—ì„œ \\(Y\\)ì™€ \\(T\\)ëŠ”\nÂ  Â  Â  Â d-separated ë˜ì—ˆë‹¤ê³  í‘œí˜„í•©ë‹ˆë‹¤.\n\n\n\n\n(4)Structural causal models (êµ¬ì¡°ì  ì¸ê³¼ ëª¨ë¸)\n\nêµ¬ì¡°ì  ì¸ê³¼ëª¨ë¸ (Structural Casual Model)ì€ ë³€ìˆ˜ë“¤ ì‚¬ì´ì˜ ì¸ê³¼ ê´€ê³„ë¥¼ êµ¬ì¡°í™” ëœ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.\ní‘œí˜„ : ìˆ˜í•™ì—ì„œ ì“°ëŠ” â€˜=â€™ ê³¼ëŠ” ë‹¬ë¦¬, causation ìƒì—ì„œëŠ” ì—­ì´ ì„±ë¦½í•˜ì§€ ì•Šìœ¼ë©°, ì•„ë˜ì™€ ê°™ì´ í‘œê¸°í•©ë‹ˆë‹¤.\nâ—¦Â  Structural equationÂ B := f(A)\nâ—¦ ì—¬ê¸°ì„œ Aì™€ B ì˜ mappingì´ deterministic í•©ë‹ˆë‹¤. ëª…í™•í•œ ê´€ê³„ê°€ ì´ì™¸ì˜ í™•ë¥ ì ì¸ ë¶€ë¶„ (Stochastic)ì„\nÂ  Â  ê³ ë ¤í•˜ê¸° ìœ„í•´ì„  Bì˜ unknown causesë„ ì¸ì§€í•´ì•¼ í•´ìš”. ê·¸ë˜ì„œ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ê³ ë ¤í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\nÂ  Â  â†’Â Â B := f(A,U)\n\n\n\nStructural Casual Models\n\n\nâ—¦Â ì •ì˜ : Structural causal modelsì€ ë‹¤ìŒ 3ê°œì˜ ì§‘í•©ì— ëŒ€í•œ íŠœí”Œ(Tuple)ì…ë‹ˆë‹¤.\nÂ  Â  1.Â Â U : ì™¸ìƒ(exogenous) ë³€ìˆ˜, ëª¨ë¸ ë°–ì—ì„œ ê·¸ ê°’ì´ ê²°ì •ë˜ëŠ” ë³€ìˆ˜ë“¤ì˜ ì§‘í•©\nÂ  Â  Â  Â  Â - ë¶€ëª¨ ë…¸ë“œê°€ ì—†ëŠ” ë³€ìˆ˜ë¡œ ì´ ë…¸ë“œì˜ causesë¥¼ ëª¨ë¸ë§ í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.\nÂ  Â  Â  Â  Â - ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” ë³€ìˆ˜ \\(U_B, U_C, U_D\\)ì— í•´ë‹¹í•©ë‹ˆë‹¤.\nÂ  Â  2.Â Â V : ë‚´ìƒ(endogenous) ë³€ìˆ˜, ëª¨ë¸ ë‚´ì—ì„œ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì— ì˜í•´ ì„¤ëª…ë˜ëŠ” ì§‘í•©\nÂ  Â  Â  Â  Â - ë¶€ëª¨ë…¸ë“œê°€ ì¡´ì¬í•˜ëŠ” ë³€ìˆ˜ë¡œ ëª¨ë¸ë§ í•˜ê³ ìí•˜ëŠ” structural equationì˜ ë³€ìˆ˜\nÂ  Â  Â  Â  Â - ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” ë³€ìˆ˜Â  \\(B, C, D\\)ì— í•´ë‹¹í•©ë‹ˆë‹¤.\nÂ  Â  3.Â Â f : ëª¨ë¸ ë‚´ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì— ë”°ë¼ Vì— ì†í•œ ë³€ìˆ˜ë“¤ì˜ ê°’ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ ì§‘í•©\n\nâ—¦Â Structural Causal Model (SCM)ë¥¼ ì“°ëŠ” ì´ìœ ê°€ ë¬´ì—‡ì¼ê¹Œìš”?\nÂ  Â  1. Potential Outcomes í‘œí˜„\nÂ  Â  Â  Â  Â - SCMì—ì„œ \\(T=t\\)ë¡œ ê³ ì •í–ˆì„ë•Œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ ê°’ì€ potential outcome ì…ë‹ˆë‹¤.\nÂ  Â Â 2. ì¼ë°˜í™”ëœ ë¶„í¬ ê³ ë ¤ ê°€ëŠ¥\nÂ  Â  Â  Â  Â - DAGë¡œ í‘œí˜„ ì‹œ, Causal directionì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ì¸ê³¼ê´€ê³„ë¥¼ í™•ì¸í•˜ê¸°ì— ì í•©í•˜ì§€ ì•Šì•„ìš”.\nÂ  Â  Â  Â  Â  Â í•˜ì§€ë§Œ SCMì€ DAGì˜ êµ¬ì¡°ì  í• ë‹¹ì„ ë”°ë¥´ë©° SEM (Strctural Equation Models)ì˜ functional formì„\nÂ  Â  Â  Â  Â  Â í†µí•´, intervention setì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ë” ë§ì€ ìƒí™©ì—ì„œì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆì–´ìš”.\nÂ  Â Â 3. Causal Models ì²´ê³„í™”Â \nÂ  Â  Â  Â  Â - ê·¸ë˜í”„ ê¸°ë°˜ìœ¼ë¡œ ì¸ê³¼ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ê±´ ê°„ë‹¨í•˜ì§€ë§Œ ì´ëŸ° ê·¸ë˜í”„ê°€ ë³µì¡í•´ì§€ë©´ ì§ê´€ì ì¸ ì´í•´ë§Œìœ¼ë¡  í•œê³„ê°€\nÂ  Â  Â  Â  Â  Â ì¡´ì¬í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì—, ê·¸ë˜í”„ ê¸°ë°˜ì˜ ì¸ê³¼ê´€ê³„ ë¶„ì„ì„ ìˆ˜í•™ì ì¸ ì–¸ì–´ë¥¼ í†µí•´ ë³´ë‹¤ ì²´ê³„í™” í•  ìˆ˜ ìˆì–´ìš”.\nâ—¦Â ì˜ˆì‹œ : ì•„ë˜ ê·¸ë˜í”„ë¥¼ SCM êµ¬ì¡°ë¡œ í‘œí˜„í•´ë´…ì‹œë‹¤.\n\n\\(U=\\) {\\(X\\)}, \\(V=\\) {\\(T,Y\\)}, \\(F=\\) {\\(f_T,f_Y\\)}\n\\(f_T:=\\alpha_1X\\)\n\\(f_Y:=\\beta T+\\alpha_2X\\)\n\n\n\n\n\nÂ SCMsì—ì„œì˜ Intervention : Modularity assumptionì— ì˜í•´ SCM(M)ê³¼ Interventional SCM(\\(M_t\\))ì—ëŠ” \\(M_t\\)ì—ì„œ ê°œì…ì´ ì¼ì–´ë‚˜ëŠ” ë³€ìˆ˜ \\(T\\)ì— ëŒ€í•œ êµ¬ì¡°ë°©ì •ì‹ì´ T:=të¡œ ëŒ€ì²´ë˜ëŠ” ê²ƒ ì™¸, ê°œì…ì´ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ” ë‹¤ë¥¸ ë³€ìˆ˜ì— ëŒ€í•œ êµ¬ì¡° ë°©ì •ì‹ì€ ë™ì¼í•©ë‹ˆë‹¤.\n\n\n\nThe Law of Counterfactuals (and Interventions)\n\n\nâ—¦ ì •ì˜ : Â \\(Y_t(u) = Y_{M_t}(u)\\)\nâ—¦ ì˜ë¯¸ : SCMì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, ì‹¤ì§ˆì ìœ¼ë¡œ Counterfactualsì„ ê³„ì‚° í•  ìˆ˜ ìˆë‹¤ëŠ” Principleì…ë‹ˆë‹¤.\nâ—¦ ì˜ì˜ : Chapter 2ì—ì„œ ì´ì•¼ê¸°í•œ ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.Â \nÂ  Â  í•´ë‹¹ ë‚´ìš©ì€ Chapter 14ì—ì„œ ë” ì„¸ë¶€ì ìœ¼ë¡œ ë‹¤ë£° ì˜ˆì •ì´ì—ìš”.\n\n\nColliderê³¼ Treatmentì˜ ìì‹ë…¸ë“œëŠ” ì™œ Conditionì„ í•˜ì§€ ì•Šì„ê¹Œìš”?\n\n\n\nCausationì„ ë§‰ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nÂ  Â  â—¦ ì™¼ìª½ ê·¸ë¦¼ : ì•„ë˜ ê·¸ë¦¼ì€ \\(T\\)ì™€ \\(Y\\)ê°€ d-seperartedë˜ì–´, ëª¨ë“  causationì´ ë§‰íŒ ìƒí™©ì…ë‹ˆë‹¤.\nÂ  Â  â—¦Â ì˜¤ë¥¸ìª½ ê·¸ë¦¼ : Causationì´ ìˆê¸° ìœ„í•´ì„œëŠ” \\(T\\)ì™€ \\(Y\\) ì‚¬ì´ì— direct pathê°€ ìˆìœ¼ë©´ ë©ë‹ˆë‹¤.\nÂ  Â  Â  Â  â†’ ì´ëŸ¬í•œ blocking causal association ì´ìœ ë¡œ descendants of treatmentë¥¼ condition í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n\n\nìƒˆë¡œìš´ í˜•íƒœì˜ Associationì´ ìƒê¸°ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤ : new post-treatment association\nÂ  Â  â—¦ì˜¤ë¥¸ìª½ ê·¸ë¦¼ : \\(M\\)ì—ì„œ ê´€ì¸¡ë˜ì§€ ì•Šì€ ì™¸ìƒë³€ìˆ˜ \\(U_M\\)ê³¼ \\(T\\), \\(M\\)ì‚¬ì´ì—” colliderê°€ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆì–´ìš”.\nÂ  Â  Â  Â â†’Â colliderì˜ ìì‹ ë…¸ë“œì¸ \\(Z\\)ë¥¼ conditioní•˜ë©´, ìƒˆë¡œìš´ post-treatment associationì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\nÂ \n\nìƒˆë¡œìš´ í˜•íƒœì˜ Associationì´ ìƒê¸°ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤ : new pre-treatment association &lt; M-bias &gt;\nÂ  Â  â—¦ ì•„ë˜ ê·¸ë¦¼ì—ì„œë„ \\(Z2\\) ê°€ colliderì´ë¯€ë¡œ, M-bias í˜•íƒœì—ì„œ conditioning ì‹œí‚¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n\n\n\n\n\nBackdoor Adjustment ì˜ˆì œ\n\nâ—¦ ë°ì´í„° ì„¤ëª…\nÂ  Â - ìƒí™© : ë¯¸êµ­ì¸ì˜ 46%ê°€ ê³ í˜ˆì••ì´ ìˆê³  ê³ í˜ˆì••ì€ ì‚¬ë§ë¥  ì¦ê°€ì™€ ì—°ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nÂ  Â - ê°€ì„¤ : ì´ë•Œ ë‚˜íŠ¸ë¥¨ ì„­ì·¨ê°€ ê³ í˜ˆì••ì— ì˜í–¥ì„ ì¤„ê¹Œìš”?\nÂ  Â - Outcome : í˜ˆì••\nÂ  Â - Treatment : ë‚˜íŠ¸ë¥¨ ì„­ì·¨\nÂ  Â - Covariates \\(W\\) : ë‚˜ì´,Â  Covariates \\(Z\\) : ì†Œë³€ì— ë°°ì¶œë˜ëŠ” ë‹¨ë°±ì§ˆ ì–‘\nÂ  Â  Â â†’ ë‚˜ì´ëŠ” í˜ˆì••ê³¼ ì‹ ì²´ì˜ ë‚˜íŠ¸ë¥¨ ìˆ˜ì¹˜ë¥¼ ì¡°ì ˆí•˜ëŠ” Confounderì´ë©°, ì†Œë³€ì— ë°°ì¶œë˜ëŠ” ë‹¨ë°±ì§ˆ ì–‘ì´ ë§ì€ ê²ƒì€\nÂ  Â  Â  Â  Â  ê³ í˜ˆì••ê³¼ ë‚˜íŠ¸ë¥¨ ì„­ì·¨ëŸ‰ì´ ë§ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¦‰, \\(Z\\)ëŠ” Coliderì´ë©° \\(W\\)ëŠ” Cofounder ì…ë‹ˆë‹¤.\n\nâ—¦ Causal Graph\nÂ  Â -Â ìœ„ì—ì„œ ë°°ìš´ Backdoor criterionì— ì˜í•˜ë©´, Confounderì¸ \\(W\\)ì— ëŒ€í•œ Backdoor pathë§Œ ë§‰ìœ¼ë©´ ë©ë‹ˆë‹¤.Â \nÂ  Â  Â Identificationì„ í†µí•´, ê³„ì‚°í•  ìˆ˜ ìˆëŠ” Statistical estimandë¥¼ ê·¸ë˜í”„ ì´ìš©í•´ ë„ì¶œí•´ ë´…ì‹œë‹¤.\nÂ  Â -Â Chapter 3ì—ì„œ ë°°ìš´ ë°©ì‹ìœ¼ë¡œ Statistical estimandë¥¼ ì‘ì„±í•´ë³´ë©´, \\(E_w,zE[Y|t,W]\\) ì´ì§€ë§Œ,coliderì¸ \\(z\\)ëŠ”\nÂ  Â  Â ë§‰ì„ í•„ìš”ê°€ ì—†ì–´ìš”. ê·¸ë˜ì„œ Causal graphë¥¼ í†µí•´ ë„ì¶œëœ Statistical estimandëŠ” \\(E_wE[Y|t,W]\\)ì…ë‹ˆë‹¤.\n\n\nâ—¦ IdentificationÂ \nÂ  Â -Â ìœ„ì™€ ê°™ì€ ë°©ì‹ì„ í†µí•´, Backdoor pathë¥¼ ì–´ë–¤ ë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ì¡°ì ˆí•´ì•¼ í•˜ëŠ”ì§€ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nÂ  Â -Â ìš°ë¦¬ê°€ êµ¬í•´ì•¼í•˜ëŠ” ê°’(Causal estimand) : \\(E[Y|do(t)]\\)\nÂ  Â -Â ìš°ë¦¬ê°€ êµ¬í•  ìˆ˜ ìˆëŠ” ê°’(Statistical estimand from causal graph) : \\(E_wE[Y|t,W]\\)\nÂ  Â -Â ìš°ë¦¬ê°€ ë°°ìš´ ê³¼ì •ì„ ì ìš©í•´ë³´ë©´ ìµœì´ˆì˜ ì¢…ì†ë³€ìˆ˜ëŠ” â€˜sodiumâ€™, â€˜ageâ€™, â€™proteinuriaâ€™ê³¼ ê°™ì•˜ê² ì£ ?\nÂ  Â  Â  ì´ ë•Œ, Colliderì¸ proteinuria (ë‹¨ë°±ë‡¨)ë¥¼ ì œê±°í•´ì„œ â€™sodiumâ€™ê³¼ â€™ageâ€™ë¥¼ ì¢…ì†ë³€ìˆ˜ë¡œ í™œìš©í•´ë´…ì‹œë‹¤.\n\nâ—¦ EstimationÂ  Â -Â ë°ì´í„°ë¥¼ í†µí•´ ATE estimationì„ í™•ì¸í•˜ë©´, ê° Condition ë§ˆë‹¤ ì°¨ì´ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\nÂ  Â - 1) ë³€ìˆ˜ í†µì œ í•˜ì§€ ì•Šì•˜ì„ ë•Œ ì˜¤ë¥˜ : 407%\nÂ  Â  Â 2) \\(T\\),\\(Y\\) ë³€ìˆ˜ì™€ ê´€ê³„ëœ ëª¨ë“  ì˜í–¥ì„ í†µì œí–ˆì„ ë•Œ ì˜¤ë¥˜ : 19%\nÂ  Â  Â 3) Backdoor pathë¥¼ ì°¨ë‹¨í–ˆì„ ë•Œ ì˜¤ë¥˜ : 0.02%\n\n\n\nìœ„ ê³¼ì •ì„ Python codeë¥¼ í™œìš©í•´, ê°„ë‹¨í•œ ì‹¤í—˜ì„ ì§„í–‰í•´ë³¼ê²Œìš”.\n\n\nì´ì „ ì˜ˆì œì—ì„œì˜ ë³€ìˆ˜ê°„ ì˜í–¥ë ¥ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ë³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•˜ì˜€ìŠµë‹ˆë‹¤.\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as sm\n\nnp.random.seed(12345)\nnum = 10000\n\nW = np.random.normal(size = num)\n\nT = 6 * W + np.random.normal(size = num)\nY = 5 * T +  3 * W + np.random.normal(size = num)\nZ = 2 * T + 7 * Y + np.random.normal(size = num)\n\ndata = pd.DataFrame({'T':T,'Y':Y,'Z':Z,'W':W})\nìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” ê´€ê³„ëŠ” Tê°€ Yì— ë¯¸ì¹˜ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ êµ¬í•˜ê³  ì´ë•Œ ê·¸ë¦¼ê³¼ ê°™ì´ 5ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\në¨¼ì € ì–´ë– í•œ ì¢…ì†ë³€ìˆ˜ë¥¼ í†µì œí•˜ì§€ ì•Šê³  ì„ í˜•íšŒê·€ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê´€ê³„ë¥¼ êµ¬í•´ë³¼ê²Œìš”.\nmodel1 = sm.ols('Y ~ T', data).fit()\nmodel1.summary().tables[1]\n\nYì— ëŒ€í•œ Tì˜ ì˜í–¥ì´ 5.4866ìœ¼ë¡œ 0.4866ì´ë¼ëŠ” ì¡ìŒì´ ìƒê²¼ë„¤ìš”. êµë€ë³€ìˆ˜ë¥¼ í†µì œí•˜ì§€ ì•Šì•„ì„œ ì˜¤ì°¨ê°€ ìƒê²¼ìŠµë‹ˆë‹¤.\nê·¸ëŸ¬ë©´ Yì™€ Tì‚¬ì´ ì˜í–¥ì„ ì£¼ëŠ” ëª¨ë“  ë³€ìˆ˜ë¥¼ í†µì œí•´ë³¼ê¹Œìš”?\nmodel2 = sm.ols('Y ~ T + W +Z', data).fit()\nmodel2.summary().tables[1]\n\nYì— ëŒ€í•œ Tì˜ ì˜í–¥ì´ -0.1770ìœ¼ë¡œ ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” 5ë€ ê°’ê³¼ ë§¤ìš° ë©€ì–´ì¡ŒìŠµë‹ˆë‹¤.\ncolliderë¥¼ í†µì œí•˜ë©´ì„œ Yì™€ Tê°„ì˜ ìƒˆë¡œìš´ ì¢…ì† ê´€ê³„ë¥¼ ë§Œë“¤ì–´ ë‚´ì–´ collider biasë¥¼ ë§Œë“¤ì—ˆêµ°ìš”.\nì´ì²˜ëŸ¼ ëª¨ë“  ë³€ìˆ˜ë¥¼ í†µì œí•˜ì—¬ collider ë˜í•œ í†µì œí•˜ê²Œ ë˜ë©´Â í¸í–¥ì´ ë°œìƒí•  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.\në§ˆì§€ë§‰ìœ¼ë¡œ Yì™€ Tì‚¬ì´ colliderì¸ ZëŠ” ì œê±°í•˜ì§€ ë§ê³  êµë€ë³€ìˆ˜ì¸ Wë§Œ í†µì œí•´ë³´ê² ìŠµë‹ˆë‹¤.\nmodel3 = sm.ols('Y ~ T + W', data).fit()\nmodel3.summary().tables[1]\n\nYì— ëŒ€í•œ Tì˜ ì˜í–¥ì´ 4.9967ìœ¼ë¡œ ê°€ì¥ 5ì— ê°€ê¹Œìš´ ê²°ê³¼ë¥¼ ë„ì¶œí•´ëƒˆìŠµë‹ˆë‹¤.\në³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬ non-causal associationì€ í†µì œí•˜ê³  ì •í™•í•œ causal associationì„ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì€ ì¤‘ìš”í•©ë‹ˆë‹¤.Â Â \nTo be continued) ë‹¤ìŒì€ ì¸ê³¼ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ”ë° ìˆì–´, Gold standardë¼ê³  ë¶ˆë¦¬ëŠ” Randomized Experimentì— ëŒ€í•´ ë°°ìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.\n\n\n\nReferenceÂ \n\nâ—¦ Lecture Notes : 2021 Summer Session on Causal Inference (ë°•ì§€ìš© êµìˆ˜ë‹˜)Â [Link]\nâ—¦ Blog\nÂ  Â - Backdoor Adjustment [Link]\nÂ  Â -Â ì¸ê³¼ì¶”ë¡ . ê·¸ë˜í”„ì™€ í™•ë¥ Â [Link]\n\n\n\n\n\nCitationBibTeX citation:@online{kim & hojae jeong2023,\n  author = {kim \\& hojae jeong, seongsoo},\n  title = {05\\textbackslash. {Causal} {Models}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n& hojae jeong, seongsoo kim. 2023. â€œ05\\. Causal\nModels.â€ November 14, 2023."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_RCT/RCT.html",
    "href": "posts/Introduction_to_causal_inference_RCT/RCT.html",
    "title": "06. Randomised Experiments",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì˜ ê¹€ì†Œí¬ì…ë‹ˆë‹¤.Â \nIntroduction to Causal Inference ê°•ì˜ì˜ ë‹¤ì„¯ ë²ˆì§¸ ì±•í„°ì´ë©°, í•´ë‹¹ ì±•í„°ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\nContents\n\nRandomized Experiments\nFrontdoor adjustment\n\nâ—¦ ê°•ì˜ ì˜ìƒ ë§í¬ : Chapter 5 - Randomised Experiments\nÂ  Â  ì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!\n\n\n\n(1) Randomized Experiments\n\nRandomized experiments are magic\n\n\nObservational study : ê´€ì¸¡ë˜ì§€ ì•Šì€ confoundersì˜ ì¡´ì¬ê°€ëŠ¥ì„± ë•Œë¬¸ì—, unconfoundednessë¥¼ ë³´ì¥ë°›ê±°ë‚˜ backdoor criterionê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nRandomized experiments : unobserved confoundingì˜ ê°€ëŠ¥ì„±ì„ ì°¨ë‹¨í•¨ìœ¼ë¡œì¨ unconfoundednessë¥¼ ë³´ì¥ë°›ê³  backdoor criterionì´ ë§Œì¡±ë˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n\nÂ  Â  Â  Â â‡’ Randomizationì€ Association is causationì„ ì„±ë¦½í•˜ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤!\n\nRandomized Control Trial (RCT)\n\n\n1. 2ì¥ì— ë‚˜ì˜¨ ì˜ˆì‹œÂ \nÂ  Â  â—¦ ë³€ìˆ˜ ì†Œê°œ\nÂ  Â  Â  Â  - \\(T\\) (Treatment) : ì‹ ë°œì„ ì‹ ê³  ì”ë‹¤\nÂ  Â  Â  Â  - \\(Y\\) (Outcome) : ë‹¤ìŒ ë‚  ë‘í†µ ì—¬ë¶€\nÂ  Â  Â  Â  - \\(X\\) (Confounder) : ì „ë‚  ë°¤ ìˆ ì— ì·¨í–ˆëŠ”ì§€ ì—¬ë¶€Â \nÂ  Â  â—¦ Observational study : ì‹ ë°œì— ì‹ ê³  ìëŠ” ê·¸ë£¹(\\(T=1\\)) vs ë²—ê³  ìëŠ” ê·¸ë£¹(\\(T=0\\))ì˜ ê·¸ë£¹ì„ ìˆëŠ” ê·¸ëŒ€ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\nÂ  Â  Â  Â  - ì–´ë–¤ ë¬¸ì œê°€ ìƒê¸¸ê¹Œìš”? Treatment / Control groupê°„ ë¹„êµ ê°€ëŠ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\nÂ  Â  Â  Â  â‡’ ì¦‰, ì‹ ë°œ ì‹ ê³  ì” ê·¸ë£¹(\\(T=1\\)) ì¤‘, ì „ë‚  ìˆ ì— ì·¨í•œ ì‚¬ëŒì˜ ë¹„ìœ¨(confounding)ì´ ì‹ ë°œ ë²—ê³  ì” ê·¸ë£¹ì— ë¹„í•´\nÂ  Â  Â  Â  Â  Â  í›¨ì”¬ ë†’ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆì–´ìš”.\n\nÂ  Â  â—¦ Randomized experiment : ì‹ ë°œì„ ì‹ ê³  ì˜ì§€ ë²—ê³  ì˜ì§€ ì—¬ë¶€ë¥¼(treatmentë¥¼) ë™ì „ ë˜ì§€ê¸°ë¡œ ê²°ì •í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â Â â‡’ ë™ì „ ì•ë’·ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì´ ê°™ìœ¼ë¯€ë¡œ, ì‹ ë°œ ì‹ ì€ ê·¸ë£¹ê³¼ ë²—ì€ ê·¸ë£¹ ë‚´ ì „ë‚  ìˆ ì— ì·¨í•œ ì‚¬ëŒì˜ ë¹„ìœ¨ì´Â \nÂ  Â  Â  Â  Â  Â  ê±°ì˜ ê°™ì•„ì§€ê²Œ ë©ë‹ˆë‹¤.\n\n\n\nRandomized Experimentsì— ëŒ€í•œ 3ê°€ì§€ ê´€ì Â \n\n\n1. Comparability and Covariate balance\nÂ  â—¦ ì²˜ì¹˜ ì—¬ë¶€ì— ëŒ€í•œ ëœë¤í™” : ì²˜ì¹˜ ì§‘ë‹¨ê³¼ í†µì œ ì§‘ë‹¨ì˜ ë‹¤ë¥¸ ëª¨ë“  ì¡°ê±´ì„ ê°™ê²Œ ë§Œë“¤ê³ (confounders ë¶„í¬ë¥¼ í¬í•¨),Â \nÂ  Â  Â  Â  ë”± í•˜ë‚˜ ì²˜ì¹˜ ì—¬ë¶€ë§Œ ë‹¤ë¥´ê²Œ í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â  â†’ ë”°ë¼ì„œ ì²˜ì¹˜ ì§‘ë‹¨ê³¼ í†µì œ ì§‘ë‹¨ì˜ ê²°ê³¼ì— ì°¨ì´ê°€ ìƒê¸¸ ê²½ìš° ì´ë¥¼ ì²˜ì¹˜ ì—¬ë¶€ ë•Œë¬¸ì´ë¼ê³  í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\nÂ  Â  â—¦ ì²˜ì¹˜ ì—¬ë¶€ë¥¼ ëœë¤í™” í•˜ëŠ” ê²ƒì€ unobserved covariatesê¹Œì§€ë„ covariate balanceë¥¼ ê°–ê²Œ í•˜ëŠ” íš¨ê³¼ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\nÂ  Â  Â  Â  ê·¸ë ‡ê¸° ë•Œë¬¸ì— \\(T\\)ê°€ \\(X\\)ì— ì˜í•´ ê²°ì •ë˜ì§€ ì•Šì•„ìš”. (\\(T{\\perp \\!\\!\\! \\perp} X\\))\nÂ  Â  Â  Â Â \\[P(X|T=1)\\stackrel{d}{=}P(X) \\quad and \\quad P(X|T=0)\\stackrel{d}{=}P(X)\\]\nÂ  Â  â—¦ Covariate balance : ì²˜ì¹˜ ì§‘ë‹¨ê³¼ í†µì œ ì§‘ë‹¨ì—ì„œ covariates \\(X\\)ì˜ ë¶„í¬ê°€ ê°™ìŒ\nÂ  Â  Â  Â  \\[P(X | T=1) \\stackrel{d}{=} P(X|T=0)\\]\nÂ  Â  Â  Â  Â â†’ â€œCovariate balance ì´ë©´, association is causationì…ë‹ˆë‹¤â€\nÂ  Â  â—¦Covariate balanceì— ëŒ€í•œ ë¶€ë¶„ì„ \\(P(y|do(t)) = P(y|t)\\)ì„ í†µí•´ ì¦ëª…í•  ìˆ˜ ìˆëŠ”ë°ìš”. ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\nÂ  Â  â—¦ Randomization â‡’ Covariate balance â‡’ â€œAssociation is causationâ€\n2. Exchangeability\nÂ  â—¦ Exchangeability ì˜ë¯¸ : Treatment ì—¬ë¶€ì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì€ ì„±ì§ˆì´ ë‹¬ë¼ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\nÂ  Â  Â  Â  - ê·¸ë£¹ì˜ êµ¬ì„±\nÂ  Â  Â  Â  - í‰ê· ì  ê²°ê³¼\nÂ  Â  â—¦Â Exchangeabilityì˜ í˜•ì‹ì  ì •ì˜ì™€ â€œAssociation is causationâ€ì˜ ë„ì¶œê³¼ì •ì€ ì•„ë˜ì™€ ê°™ì•„ìš” &lt; textbook p.51 &gt;\nÂ  Â  Â  \\[E[Y(0)|T=0] = E[Y(0)|T=1]\\] \\[E[Y(1)|T=1] = E[Y(1)|T=0]\\]\nÂ  Â  â—¦ Treatment/Control ê·¸ë£¹ì´ êµí™˜ ê°€ëŠ¥ : ì•ë©´ì´ ë‚˜ì˜¨ ì‚¬ëŒì„ ì²˜ì¹˜ ì§‘ë‹¨ì— ë„£ê¸°ë¡œ í•˜ë“  ë’·ë©´ì´ ë‚˜ì˜¨ ì‚¬ëŒì„\nÂ  Â  Â  Â  ì²˜ì¹˜ ì§‘ë‹¨ì— ë„£ê¸°ë¡œ í•˜ë“  ê° ê·¸ë£¹ì˜ Yì— ëŒ€í•œ ê¸°ëŒ“ê°’ì€ ê°™ìŠµë‹ˆë‹¤.\n\n3. No backdoor paths\nÂ  Â  â—¦ Të¥¼ randomizeí•˜ë©´ TëŠ” ë”ì´ìƒ ì¸ê³¼ì  parentsë¥¼ ê°€ì§€ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤.Â \nÂ  Â  Â  Â  - \\(X\\) â†’ \\(T\\)ë¡œ ê°€ëŠ” edgeê°€ ì‚¬ë¼ì ¸ì„œ backdoor pathê°€ ëŠê¹€\nÂ  Â  Â  Â  - Unobserved variablesë„ ë§ˆì°¬ê°€ì§€ë¡œ pathê°€ ëŠê¹€\n\n\n\n\n(2)Â Frontdoor Adjustment\n\nBackdoor adjustmentë¥¼ ë‹¤ì‹œ ë¦¬ë§ˆì¸ë“œ í•´ë´…ì‹œë‹¤!\n\nÂ  Â  Â  Â  â†’ Â ì™¼ìª½ì²˜ëŸ¼ ë¹¨ê°„ìƒ‰ ì ì„ (backdoor path)ë¥¼ í†µí•œ non-causal associationì´ ì¡´ì¬í•  ë•Œ,\nÂ  Â  Â  Â  Â  Â  Â  \\(W\\_2\\)ì™€ \\(C\\)ì— ëŒ€í•´ ê°ê° conditioning í•¨ìœ¼ë¡œì¨ ê·¸ pathë¥¼ ì°¨ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nFrontdoor Adjustment ë„ì… ë°°ê²½ : Unobserved Confounders \\(W\\)\n\n\nÂ  Â  â—¦ Q : \\(W\\)ëŠ” unobserved confounderì´ë¯€ë¡œ conditioningì„ í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ìƒê¹ë‹ˆë‹¤. ì´ ê²½ìš° backdoor pathë¥¼Â  Â  Â  Â  Â  Â  Â  Â  Â  ë§‰ì„ ìˆ˜ ì—†ê²Œ ë˜ëŠ”ë°ìš”, ì—¬ì „íˆ \\(T â†’ Y\\)ì— ëŒ€í•œ casual effectë¥¼ identify í•  ìˆ˜ ìˆì„ê¹Œìš”?\nÂ \nÂ  Â  â—¦ A : â€œFrontdoor pathâ€ë¥¼ í†µí•´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n\nÂ  Â  â—¦Â  ì§ê´€ :\nÂ  Â  Â  Â - ì•„ë˜ ê·¸ë¦¼ì—ì„œ Backdoor pathëŠ” \\(T â†’ W â†’ Y\\)ì˜ Confounding association ì…ë‹ˆë‹¤.\nÂ  Â  Â  Â - ìš°ë¦¬ê°€ ê´€ì‹¬ì´ ìˆëŠ” Causal associationì´ \\(M\\)ë¥¼ í†µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\nÂ  Â  Â  Â  â†’ \\(M\\)ì— ì§‘ì¤‘í•˜ë©´, Causal associationì„ ë¶„ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â \n\n\n\nFrontdoor Adjustment 3ê°€ì§€ StepÂ \n\n\n0. Frontdoor Adjustment ê³¼ì •\nÂ  Â  â—¦Â  \\(T â†’ M\\)ì— ëŒ€í•œ ì¸ê³¼ íš¨ê³¼ Identify\nÂ  Â  â—¦Â  \\(M â†’ Y\\)ì— ëŒ€í•œ ì¸ê³¼ íš¨ê³¼ Identify\nÂ  Â  â—¦Â  ì•ì˜ 2ê°œì˜ ìŠ¤í…ì„ í•©ì³ì„œ, \\(Tâ†’ Y\\)ì— ëŒ€í•œ ì¸ê³¼ íš¨ê³¼ë¥¼ Identify\n\n1. Frontdoor Adjustment : Step 1Â  &lt; \\(T â†’ M\\)ì— ëŒ€í•œ ì¸ê³¼ íš¨ê³¼ Identify &gt;Â \nÂ  Â  â—¦Â  \\(T â†’ M\\)ìœ¼ë¡œ ê°€ëŠ” pathì—ëŠ” \\(Y\\)ê°€ \\(Tâ†’Wâ†’Yâ†’M path\\)ì—ì„œ colliderë¡œ backdoor pathë¥¼ ë§‰ê³  ìˆì–´ìš”.\nÂ  Â  â—¦Â  ë”°ë¼ì„œ \\(T â†’ M\\)ìœ¼ë¡œ ê°€ëŠ” associationì€ causal associationë§Œ ì¡´ì¬í•˜ê²Œ ë©ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â â‡’ Empty setì„ adjustment setìœ¼ë¡œ ì‚¬ìš©í•´, ì•„ë˜ì™€ ê°™ì´ bacdkoor adjustmentë¡œ identify ê°€ëŠ¥í•©ë‹ˆë‹¤.\nÂ  Â  Â  Â  Â  Â  Â \\[P(m|do(t)) = P(m|t)\\]\n2. Frontdoor Adjustment : Step 2Â  &lt; \\(M â†’ Y\\)ì— ëŒ€í•œ ì¸ê³¼ íš¨ê³¼ Identify &gt;Â \nÂ  Â  â—¦Â  \\(Mâ†’ Y\\)ì— \\(Mâ†’Tâ†’Wâ†’Y\\)ì˜ backdoorpathê°€ ì¡´ì¬í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” \\(T\\)ì— ëŒ€í•´ conditioningí•´ì„œ\nÂ  Â  Â  Â  Â backdoor pathë¥¼ ë§‰ì„ ìˆ˜ ìˆì–´ìš”.\nÂ  Â  â—¦Â  Frontdoor adjustmentëŠ” backdoor adjustmentë¥¼ ì‘ìš©í•œ ë²„ì „ì´ë¼ê³ ë„ ë³¼ ìˆ˜ ìˆì–´ìš”.\nÂ  Â  Â  Â  \\[P(y|do(m)) = \\sum_tP(y|m,t)P(t)\\]\nÂ  Â  Â  Â  â‡’ \\(T\\)ë¥¼ sufficient adjustment setìœ¼ë¡œ ì‚¬ìš©í•´ì„œ backdoor adjustment ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n3. Frontdoor Adjustment : Step 3Â  &lt; ì•ì˜ 2ê°œì˜ ìŠ¤í…ì„ í•©ì³ì„œ, \\(Tâ†’ Y\\)ì— ëŒ€í•œ ì¸ê³¼ íš¨ê³¼ë¥¼ Identify &gt;Â \nÂ  Â  â—¦Â  \\(T â†’ Y\\) ì˜ ì¸ê³¼ íš¨ê³¼ ê·œëª…ì„ ìœ„í•´ step1, 2ë¥¼ chaining í•˜ê²Œ ë˜ë©´ ì•„ë˜ì™€ ì‹ê³¼ ê°™ìŠµë‹ˆë‹¤.\nÂ  Â  Â  Â  Â \\[P(y|do(t)) = \\sum_mP(m|do(t))P(y|do(m))\\]\n\n\n\nFrontdoor Adjustment Criterion\n\n\nâ—¦ ë§Œì•½ (\\(T\\), \\(M\\), \\(Y\\))ê°€ frontdoor criterionë¥¼ ë§Œì¡±ì‹œí‚¤ê³ , positivityë¥¼ ê°€ì •í•œ ê²½ìš°, step1ê³¼ step2ì˜ ê³µì‹ì„\nÂ  Â  Â step3ì™€ í•©ì¹œì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\nÂ  Â  Â  Â  Â \nÂ  Â  Â  Â \\[ P(y | do(t)) = \\sum_mP(m|t)\\sum_{t'}P(y|m, t')P(t') \\]\nâ—¦ ë³€ìˆ˜ ì„¸íŠ¸ \\(M\\)ì€ \\(T\\)ì™€ \\(Y\\)ì— ê´€í•´, ë‹¤ìŒ ì¡°ê±´ë“¤ì´ ì°¸ì´ë©´ frontdoor criterionì„ ë§Œì¡±ì‹œí‚¨ë‹¤ê³  í•  ìˆ˜ ìˆì–´ìš”.\nÂ  Â  Â - \\(M\\)ì´ \\(T â†’ Y\\)ì˜ íš¨ê³¼ë¥¼ ì™„ì „í•˜ê²Œ ë§¤ê°œ\nÂ  Â  Â  Â  Â  (complete mediation, ì˜ˆì‹œ : ëª¨ë“  \\(T\\)ì—ì„œ \\(Y\\)ë¡œ ê°€ëŠ” causal pathê°€ \\(M\\)ì„ ì§€ë‚˜ëŠ” ê²½ìš°)\nÂ  Â  Â -Â \\(T\\)ì—ì„œ \\(M\\)ìœ¼ë¡œ ê°€ëŠ” unblocked backdoor pathê°€ ì—†ìŒ\nÂ  Â  Â - \\(M\\)ì—ì„œ \\(Y\\)ë¡œ ê°€ëŠ” ëª¨ë“  backdoor pathê°€ \\(T\\)ì— ì˜í•´ì„œ blocked\n\n\nTo be continued)Â  ë‹¤ìŒì€ Nonparametric Identification ì— ëŒ€í•´ ë°°ìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.\n\n\n\n\nCitationBibTeX citation:@online{kim2023,\n  author = {kim, sohee},\n  title = {06\\textbackslash. {Randomised} {Experiments}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nkim, sohee. 2023. â€œ06\\. Randomised Experiments.â€ November\n14, 2023."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Nonparametric_Identification/Nonparametric_Identification.html",
    "href": "posts/Introduction_to_causal_inference_Nonparametric_Identification/Nonparametric_Identification.html",
    "title": "07. Nonparametric Identification",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì˜ ë‚¨ê¶ë¯¼ìƒì…ë‹ˆë‹¤.\nIntroduction to Causal Inference ê°•ì˜ì˜ ì—¬ì„¯ ë²ˆì§¸ ì±•í„°ì´ë©°, í•´ë‹¹ ì±•í„°ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Nonparametric_Identification/Nonparametric_Identification.html#do-calculus",
    "href": "posts/Introduction_to_causal_inference_Nonparametric_Identification/Nonparametric_Identification.html#do-calculus",
    "title": "07. Nonparametric Identification",
    "section": "\\(do\\)-calculus",
    "text": "\\(do\\)-calculus\nì ê¹ ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ì„ ë˜ì§šì–´ ë³¼ê¹Œìš”? ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ treatment \\(T\\)ì™€ outcome \\(Y\\)ì˜ ì¸ê³¼ê´€ê³„, ì¦‰ \\(P(y\\:|\\:do(t))\\)ë¥¼ ë°í˜€ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì„ identificationì´ë¼ í•˜ì£ .\n\nì•„ì£¼ì•„ì£¼ ê°„ë‹¨í•œ ê²½ìš°, ë‹¤ë¥¸ ë…¸ë“œ ì—†ì´ \\(T \\rightarrow Y\\)ë¼ë©´ statistical quantityê°€ ê³§ causal quantityì…ë‹ˆë‹¤. \\(P(y\\:|\\:do(t))\\:=\\:P(y\\:|\\:t)\\)ì…ë‹ˆë‹¤.\nê·¸ëŸ°ë° ì—¬ëŸ¬ ë…¸ë“œë“¤ ì‚¬ì´ì˜ ê´€ê³„ê°€ ë³µì¡í•˜ê²Œ ì—®ì´ë©´, \\(P(y\\:|\\:do(t))\\)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë” ë³µì¡í•œ ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\ngraph factorizationì„ í†µí•´ ë…¸ë“œë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì­‰ ëŠ˜ì–´ë†“ê³ \nìš°ë¦¬ê°€ ê´€ì‹¬ì—†ëŠ” ê²ƒë“¤(\\(T\\)ë¥¼Â ì œì™¸í•œ ë…¸ë“œë“¤)ì€ marginalizeí•˜ê³ \nê·¸ë¦¬ê³  \\(do(t)\\) í•­ì„ ì—†ì• ëŠ” ë°©í–¥ìœ¼ë¡œ(observationìœ¼ë¡œ ë°”ê¿”ë‚˜ê°€ëŠ” ë°©í–¥ìœ¼ë¡œ) ì‹ì„ ì •ë¦¬í•œë‹¤.\nì´ ê³¼ì •ì´ ì§€ë‚œ ì‹œê°„ì— ë°°ì› ë˜ adjusting for confoundersì…ë‹ˆë‹¤.\n\n\nì•ì„œ ë‹¤ë£¬ backdoor criterionê³¼ frontdoor criterionì€ ì´ëŸ° ê³¼ì •ì´ ê°„ë‹¨í•œ ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤. DAGë§Œ ì“± ë³´ê³ ë„ identifyí•  ìˆ˜ ìˆëŠ” íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ì£ . ê·¸ëŸ°ë° íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ë§ê³ , ì¼ë°˜ì ì¸ ì ‘ê·¼ë²•ì€ ì—†ì„ê¹Œìš”?\në‹¹ì—°íˆ ìˆìŠµë‹ˆë‹¤! ì´ ê³¼ì •ì— ì“°ì¼ ìˆ˜ ìˆëŠ” 3ê°€ì§€ì˜ inference ruleì´ ìˆëŠ”ë°, ì´ ì¼ë ¨ì˜ ê·œì¹™ë“¤ì´ ë°”ë¡œ \\(do\\)-calculusì…ë‹ˆë‹¤.\n\n\n\nëˆ„êµ°ê°€ê°€ ë˜ì§„ ì§ˆë¬¸ì— ì£¼ë””ì•„ í„ êµìˆ˜ë‹˜ì´ ì§ì ‘ ë‹µë³€í•´ì£¼ì…¨ë„¤ìš”!\n\n\nÂ ì‰½ê²Œ ë§í•˜ìë©´, \\(do\\)-calculusëŠ” ì¼ì¢…ì˜ ë…¼ë¦¬ì  ë²•ì¹™ì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ìœ í´ë¦¬ë“œ ê¸°í•˜í•™ì˜ 5ê°€ì§€ ê³µì¤€ì„ ì´ìš©í•´ í”¼íƒ€ê³ ë¼ìŠ¤ì˜ ì •ë¦¬ë¥¼ ìœ ë„í•  ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼, \\(do\\)-calculusì˜ 3ê°€ì§€ ê·œì¹™ì„ ì´ìš©í•˜ë©´ ì¸ê³¼ì  ê´€ê³„ë¥¼ ê·œëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. frontdoor & backdoor adjustmentëŠ” ì´ ë²•ì¹™ë“¤ì„ ì ìš©í•´ì„œ ìœ ë„í•œ ê³µì‹ ì¤‘ í•˜ë‚˜ì¸ ê±°ê³ ìš”.\nÂ ë°‘ì— ìì„¸íˆ ì„¤ëª…í•˜ê² ì§€ë§Œ \\(do\\)-calculusì˜ 3ê°€ì§€ ë£°ì€ DAGì—ì„œ â€˜ì´ëŸ° ë…¸ë“œë“¤ì€ ë¬´ì‹œí•´ë„ ë¼!â€™ ë˜ëŠ” â€˜ì´ëŸ° interventionì€ observationìœ¼ë¡œ ì·¨ê¸‰í•´ë„ ë¼!â€™ (ë˜ëŠ” ê·¸ ì—­) ë¼ê³  ë§í•´ì¤ë‹ˆë‹¤. ì‰½ê²Œ ë§í•´ ë³µì¡í•˜ê²Œ ì–½í˜€ìˆëŠ” ìƒê´€ê´€ê³„ í•­ë“¤ì„ ì³ë‚´ë©´ì„œ ì¸ê³¼ê´€ê³„ë¥¼ ê·œëª…í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ê±°ì£ .\n\n\\(do\\)-calculusì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ìë©´â€¦\nRule 1: ~~í•œ ì¡°ê±´ì´ ë§Œì¡±ëœë‹¤ë©´, ~~ì— ëŒ€í•œ observationì„ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤.\nRule 2: ~~í•œ ì¡°ê±´ì´ ë§Œì¡±ëœë‹¤ë©´, ~~ì— ëŒ€í•œ interventionì„ observationìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆë‹¤.\nRule 3: ~~í•œ ì¡°ê±´ì´ ë§Œì¡±ëœë‹¤ë©´, ~~ì— ëŒ€í•œ interventionì„ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤.\në˜ëŠ” 3ê°œì˜ ê·œì¹™ ëª¨ë‘â€¦\nDAGë¥¼ ~~í•˜ê²Œ ì¡°ì‘í–ˆì„ ë•Œ, \\(Y\\)ì™€ \\(Z\\)ê°€ d-separated ê´€ê³„ë¼ë©´ \\(Z\\)ì— ëŒ€í•œ í•­ì„ ë¬´ì‹œ or ì¹˜í™˜í•  ìˆ˜ ìˆë‹¤.\n\n\n\\(do\\)-calculusë¥¼ ìœ„í•œ ëª‡ ê°€ì§€ í‘œê¸°ë²•\n\\(do\\)-calculusë¥¼ ì‚´í´ë³´ê¸° ì•ì„œ, ì´ ì±•í„°ì—ì„œ ìì£¼ ì“°ì´ëŠ” ë…¸ë“œ ì´ë¦„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\(T\\) ë˜ëŠ” \\(X\\): ì¡°ì‘ë³€ì¸. ìš°ë¦¬ê°€ ê·¸ ì˜í–¥ë ¥ì„ ì•Œì•„ë³´ê³ ì í•˜ëŠ” ë…¸ë“œë“¤.\n\\(Y\\): ì¢…ì†ë³€ì¸. ì‰½ê²Œ ë§í•˜ë©´ ê²°ê³¼. ìš°ë¦¬ê°€ ê¶ê·¹ì ìœ¼ë¡œ ì›í•˜ëŠ” ê²ƒì€ \\(P(y\\:|\\:do(t))\\)ë¥¼ ì•Œì•„ë‚´ëŠ” ê²ƒ.\n\në˜, \\(do\\)-calculusì—ì„œëŠ” causal graphë¥¼ ì´ë¦¬ì €ë¦¬ ì¡°ì‘í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì•„ë˜ì™€ ê°™ì€ í‘œê¸°ë²•ì„ ë„ì…í•©ë‹ˆë‹¤.\n\nCausal graph \\(G\\)ì— ëŒ€í•´ì„œ,Â \n\n\\(G\\_{\\overline{X}}\\): ì§‘í•© \\(X\\)ì— í¬í•¨ëœ ë…¸ë“œë“¤ì˜ incoming edge (\\(parent(X)\\rightarrow X\\)ì¸ ì—£ì§€)ë¥¼ ì œê±°í•œ ê·¸ë˜í”„\n\\(G\\_{\\underline{X}}\\): ì§‘í•© \\(X\\)ì— í¬í•¨ëœ ë…¸ë“œë“¤ì˜ outgoing edge (\\(X \\rightarrow child(X)\\)ì¸ ì—£ì§€)ë¥¼ ì œê±°í•œ ê·¸ë˜í”„\n\nì²˜ìŒ ë³´ëŠ” í‘œê¸°ë²•ì´ì§€ë§Œ, ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸°ëŠ” ì–´ë µì§€ ì•Šì„ ê²ë‹ˆë‹¤. ë…¸ë“œ ì‚¬ì´ì˜ ê´€ê³„ê°€ í­í¬ì²˜ëŸ¼ ìœ„ì—ì„œ ì•„ë˜ë¡œ íë¥¸ë‹¤ê³  ìƒê°í•´ë´…ì‹œë‹¤.\n\n\\(G\\_{\\overline{X}}\\):Â ìœ„ë¥¼ ë§‰ëŠ”ë‹¤ â†’ ë¶€ëª¨ë¡œë¶€í„° ì˜¤ëŠ” ì—£ì§€ ì°¨ë‹¨\n\\(G\\_{\\underline{X}}\\):Â ì•„ë˜ë¥¼ ë§‰ëŠ”ë‹¤ â†’ ìì‹í•œí…Œ ë„˜ê²¨ì£¼ëŠ” ì—£ì§€ ì°¨ë‹¨\n\nê·¸ëŸ¼ ë³¸ê²©ì ìœ¼ë¡œ \\(do\\)-calculusì˜ 3ê°€ì§€ ê·œì¹™ì„ ì‚´í´ë³¼ê¹Œìš”?\n\n\nRule 1: Observationì˜ ì‚½ì… / ì œê±°\n\\([P(y\\:|\\:do(t),z,w)=P(y\\:|\\:do(t),w)\\quad if \\quad Y {\\perp\\!\\!\\!\\perp}_{G_{\\overline{T}}} Z\\:|\\:T,W]\\)\nìˆ˜í•™ì ì¸ ì˜ë¯¸ë¥¼ í’€ì–´ë³´ìë©´, ìš°ë¦¬ê°€ \\(P(y\\:|\\:do(t))\\)ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì—ì„œ \\(Z\\)ì— ëŒ€í•œ observation í•­ì´ ë“±ì¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë°Â ì´ ë•Œ, \\(G\\_{\\overline{T}}\\)ì—ì„œ \\((Y {\\perp\\!\\!\\!\\perp} Z\\:|\\:T,W)\\)ê°€ ì„±ë¦½í•˜ë©´ \\(Z\\)ì— ëŒ€í•œ observation í•­ì€ ë¬´ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì¡°ê¸ˆ ì§ê´€ì ìœ¼ë¡œ í’€ì–´ë³´ìë©´, \\(Z\\)ê°€ \\(Y\\)ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” pathê°€ ì—†ë‹¤ë©´, \\(Z\\)ì— ëŒ€í•œ observationì€ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤ëŠ” ì´ì•¼ê¸°ì…ë‹ˆë‹¤.\n\nì˜ˆì‹œ í•˜ë‚˜ ì‚´í´ë³¼ê¹Œìš”? ìœ„ì™€ ê°™ì€ ê·¸ë˜í”„ë¥¼ ìƒê°í•´ë³´ì. (ì—¬ê¸°ì„œëŠ” \\(T\\) ëŒ€ì‹  \\(X\\)ë¼ëŠ” ë¬¸ìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)\n\\(X\\rightarrow Y\\)ì— ëŒ€í•œ ì¸ê³¼ê´€ê³„ë¥¼ ë°íˆê³  ì‹¶ì€ë°, \\(Z\\)ë¥¼ ì‹ ê²½ì¨ì•¼ í• ê¹Œìš”?\n\\(do\\)-calculusì˜ 1 ê·œì¹™ì— ë”°ë¥´ë©´, \\(G\\_{\\overline{X}}\\)ì—ì„œ \\(W\\),\\(X\\)ë¥¼ conditioning í–ˆì„ ë•Œ, \\(Z\\)ì™€ \\(Y\\)ê°€ d-separation ë˜ì–´ìˆëŠ” ê±¸ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œÂ \\(Z\\)ëŠ” ë¬´ì‹œí•´ë„ ë©ë‹ˆë‹¤!\nì§ê´€ì ìœ¼ë¡œ ë´¤ì„ ë•Œë„, \\(Z\\)ê°€ ë°”ë€Œì—ˆë‹¤ê³  \\(Y\\)ì— ì˜í–¥ì„ ì£¼ëŠ” ê²½ë¡œê°€ ì—†ì£ ? ì§€ê¸ˆ ê°™ì€ ê²½ìš°ëŠ” ê·¸ë˜í”„ê°€ ê°„ë‹¨í•´ ë°”ë¡œ ë³´ì´ì§€ë§Œ, ê·¸ë˜í”„ê°€ ë³µì¡í•´ì§€ë©´ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë˜ê² ì£ ?\n\n\nRule 2: Action â†”ï¸ Interventionì˜ êµí™˜\nê·¸ëŸ°ë° Rule 1ì„ ì•„ë¬´ë¦¬ í™œìš©í•´ë„ ì‹ì— do-operatorê°€ ìˆëŠ” ê±¸ ì—†ì•¨ ìˆ˜ëŠ” ì—†ì£ ? ê·¸ê±¸ í•´ì£¼ëŠ” ê²Œ Rule 2ì…ë‹ˆë‹¤!\n\\([P(y\\:|\\:do(t),do(z),w)=P(y\\:|\\:do(t),z,w)\\quadÂ ifÂ \\quadÂ YÂ {\\perp\\!\\!\\!\\perp}_{G_{\\overline{T},\\underline{Z}}}Â Z\\:|\\:T,W]\\)\nìˆ˜í•™ì ìœ¼ë¡œëŠ”, \\(G\\_{\\overline{T},\\underline{Z}}\\)ì—ì„œ \\((Y {\\perp\\!\\!\\!\\perp} Z\\:|\\:T,W)\\)ê°€ ì„±ë¦½í•˜ë©´ \\(Z\\)ì— ëŒ€í•œ interventionì€ observationê³¼ ê°™ìŠµë‹ˆë‹¤. ì¦‰, \\(Z\\)ì— \\(do\\)-operatorë¥¼ ë§ˆìŒëŒ€ë¡œ ì‚½ì…í•˜ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì§ê´€ì ìœ¼ë¡œëŠ”, \\(Z\\)ê°€ \\(Y\\)ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” pathê°€ directed path ë°–ì— ì—†ë‹¤ë©´ \\(Z\\)ì— ëŒ€í•œ interventionì€ observationê³¼ ê°™ê²Œ ìƒê°í•  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n\në§ˆì°¬ê°€ì§€ë¡œ ìœ„ì™€ ê°™ì€ ì˜ˆì‹œë¥¼ ìƒê°í•´ë´…ì‹œë‹¤. ì—¬ê¸°ì„œ \\(do(z)\\)ë¥¼ ê´€ì¸¡ê°’ \\(z\\)ë¡œ ìƒê°í•  ìˆ˜ ìˆì„ê¹Œìš”?\n\\(G\\_{\\overline{X},\\underline{Z}}\\)ë¥¼ ë³´ë©´ \\(W,X\\)ë¥¼ conditioning í–ˆì„ ë•Œ, \\(Z\\)ì™€ \\(Y\\)ëŠ” ì„œë¡œ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤ (d-separation). ë”°ë¼ì„œ \\(do(z)\\)ë¥¼ ê·¸ëƒ¥ \\(z\\)ë¡œ ìƒê°í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.\n\n\nRule 3: Actionì˜ ì‚½ì… / ì œê±°\nê·œì¹™ 3ì€ ê½¤ë‚˜ ë³µì¡í•˜ê²Œ ìƒê²¼ìŠµë‹ˆë‹¤.\n\\([P(y\\:|\\:do(t),do(z),w)=P(y\\:|\\:do(t),w)\\quadÂ ifÂ \\quadÂ YÂ {\\perp\\!\\!\\!\\perp}_{G_{\\overline{T},\\overline{Z(W)}}}Â Z\\:|\\:T,WÂ ]\\)\në¨¼ì € \\(Z(W)\\)ê°€ ë­˜ê¹Œìš”? \\(Z(W)\\)ëŠ” â€™\\(Z\\)ì— ì†í•œ ë…¸ë“œ ì¤‘ì—, \\(W\\)ì˜ ë¶€ëª¨ê°€ ì•„ë‹Œ ë…¸ë“œë“¤ì˜ ì§‘í•©â€™ì…ë‹ˆë‹¤.\n\nìœ„ ì¼€ì´ìŠ¤ì—ì„œ, \\(Z\\)ëŠ” \\(W\\)ì˜ ë¶€ëª¨ê°€ ì•„ë‹™ë‹ˆë‹¤. ë”°ë¼ì„œ \\(Z(W)\\)ëŠ” ê·¸ëƒ¥ \\(Z\\)ì™€ ê°™ìŠµë‹ˆë‹¤. (ì—¬ê¸°ì„œëŠ” \\(Z\\)ê°€ í•˜ë‚˜ì˜ ë…¸ë“œì´ì§€ë§Œ, \\(Z\\)ê°€ ì—¬ëŸ¬ ë…¸ë“œì˜ ì§‘í•©ì„ ì˜ë¯¸í•˜ë„ë¡ í™•ì¥ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ, ëª…ì‹¬í•˜ì„¸ìš”!)\në”°ë¼ì„œ \\(G\\_{\\overline{X},\\overline{Z(W)}}\\)ëŠ” ìœ„ì™€ ê°™ì´ ë³€ê²½ë©ë‹ˆë‹¤. \\(Z\\rightarrow X\\)ëŠ” \\(\\overline{X}\\)ì— ì˜í•´, \\(W\\rightarrow Z\\)ëŠ” \\(\\overline{Z(W)}\\)ì— ì˜í•´ ì œê±°ë˜ì—ˆì£ .\n\në°˜ë©´ ì´ ê²½ìš°, í•˜ë‚˜ ìˆëŠ” \\(Z\\)ê°€ \\(W\\)ì˜ ë¶€ëª¨ì´ë¯€ë¡œ \\(Z(W)\\)ëŠ” ê³µì§‘í•©ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ \\(\\overline{Z(W)}\\)ë¥¼ í•œë‹¤ê³  í•´ë„ \\(X\\rightarrow Z\\)ëŠ” ë°”ë€Œì§€ ì•ŠìŠµë‹ˆë‹¤.\nê·¸ëŸ¬ë©´ ë‘ ì¼€ì´ìŠ¤ì—ì„œ \\(do(z)\\)ëŠ” ë¬´ì‹œí•  ìˆ˜ ìˆì„ê¹Œìš”? ë‘ ì¼€ì´ìŠ¤ ëª¨ë‘, \\(X\\)ì™€ \\(W\\)ë¥¼ conditioningí•˜ë©´ \\(Z\\)ì™€ \\(Y\\)ëŠ” d-separatedì´ë¯€ë¡œ \\(do(z)\\)ëŠ” ë¬´ì‹œ ê°€ëŠ¥í•©ë‹ˆë‹¤!\n\n\nApplication\nì•ì„œì„œ frontdoor & backdoor adjustmentëŠ” \\(do\\)-calculusë¥¼ í†µí•´ ìœ ë„ë  ìˆ˜ ìˆë‹¤ê³  í–ˆìŠµë‹ˆë‹¤. ê·¸ ê³¼ì •ì„ ì§ì ‘ ì‚´í´ë³´ë©´ì„œ \\(do\\)-calculusê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì ìš©ë  ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤.\n\nBackdoor Adjustment\n\n\n\n\nFrontdoor Adjustment\n\n\n\n\n\\(do\\)-calculusì˜ íŠ¹ì§•\n\\(do\\)-calculusëŠ” ì•„ë˜ì™€ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤.\n\nComplete: \\(do\\)-calculusì˜ 3ê°€ì§€ ê·œì¹™ì„ ì´ìš©í•˜ë©´ ëª¨ë“  identifiable causal estimandë¥¼ identifyí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´, ì´ ì„¸ ê°€ì§€ ê·œì¹™ì„ ì´ìš©í•´ë„ identifyí•˜ì§€ ëª»í•˜ëŠ” ì¼€ì´ìŠ¤ëŠ” ê·¸ëƒ¥ identifyí•  ìˆ˜ ì—†ëŠ” ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤.\nNonparametric: \\(do\\)-calculusëŠ” ë°ì´í„° ë¶„í¬ì— ëŒ€í•´ íŠ¹ë³„í•œ ê°€ì •ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŠ¹ì • ë¶„í¬ë¥¼ ê°€ì •í–ˆì„ ë•ŒëŠ” parametric identificationì´ë¼ê³  ë¶ˆë¦¬ë©° nonparametricì— ë¹„í•´ ë” ë§ì€ causal estimandë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì½”ìŠ¤ì—ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤ë„¤ìš”."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Nonparametric_Identification/Nonparametric_Identification.html#determining-identifiability-from-the-graph",
    "href": "posts/Introduction_to_causal_inference_Nonparametric_Identification/Nonparametric_Identification.html#determining-identifiability-from-the-graph",
    "title": "07. Nonparametric Identification",
    "section": "Determining Identifiability from the Graph",
    "text": "Determining Identifiability from the Graph\n\\(do\\)-calculusê°€ ìœ ìš©í•˜ê³  ì¢‹ê¸´ í•œë°, frontdoorì´ë‚˜ backdoor criterionì²˜ëŸ¼ ê·¸ë˜í”„ë§Œ ì–¸ëœ» ë³´ê³ ë„ identifiabilityë¥¼ ì•Œì•„ë‚¼ ìˆ˜ëŠ” ì—†ì£ . causal graphë¥¼ ì–¸ëœ» ë³´ê³ ë„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆëŠ”, íŠ¹ìˆ˜í•œ causal estimandëŠ” ë” ì—†ì„ê¹Œìš”?\nìˆìŠµë‹ˆë‹¤! Unconfounded Children Identifiabilityë¼ê³  ë¶ˆë¦¬ëŠ” ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ ê²½ìš°ì£ .\n\ní•˜ë‚˜ì˜ conditioning setìœ¼ë¡œ \\(T\\)ì˜ ìì† ì¤‘ì— \\(Y\\)ì˜ ì¡°ìƒì¸ ê²ƒë“¤ë¡œ í†µí•˜ëŠ” backdoor pathë¥¼ ëª¨ë‘ ë§‰ì„ ìˆ˜ ìˆë‹¤ë©´, \\(P(y\\:|\\:do(t))\\)ëŠ” identifiableí•˜ë‹¤.\n\nfrontdoor adjustmentì™€ ë¹„ìŠ·í•´ ë³´ì…ë‹ˆë‹¤. ìì„¸í•œ ì˜ˆì‹œë¥¼ ë³¼ê¹Œìš”?\n\nìœ„ì˜ ê·¸ë¦¼ì—ì„œ frontdoorëŠ” ë§Œì¡±í•˜ì§€ ì•Šì§€ë§Œ confounded childrenì€ ë§Œì¡±í•œë‹¤. Confounded children criterionì´ ì¢€ ë” ì¼ë°˜ì ì¸ ê¸°ì¤€ì¸ ì…ˆì´ì£ .\nì´ ì™¸ì—ë„ hedge conditionì´ë¼ëŠ” ê²Œ ìˆê¸´ í•œë°, ì´ ê°•ì˜ì—ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Nonparametric_Identification/Nonparametric_Identification.html#ì°¸ê³ ìë£Œ",
    "href": "posts/Introduction_to_causal_inference_Nonparametric_Identification/Nonparametric_Identification.html#ì°¸ê³ ìë£Œ",
    "title": "07. Nonparametric Identification",
    "section": "ì°¸ê³ ìë£Œ",
    "text": "ì°¸ê³ ìë£Œ\n\ndo-calculus adventures!\nDeriving the front-door criterion with the do-calculus\nchandan singh Causality"
  },
  {
    "objectID": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html",
    "href": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html",
    "title": "Randomised Controlled Trial",
    "section": "",
    "text": "ğŸ‘‰ í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” ì•„ë˜ ì¸ê³¼ì¶”ë¡  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. - ê³ ìˆ˜ë“¤ì˜ ê³„ëŸ‰ê²½ì œí•™ 1ì¥ ë¬´ì‘ìœ„ ì‹œí–‰ - Causal Inference for the brave and true Chapter 2. Randomised Experiments / í•œêµ­ì–´ ë²ˆì—­ ìë£Œ - ì¸ê³¼ì¶”ë¡ ì˜ ë°ì´í„° ê³¼í•™\n\n\n\n\n\në°°ìš´ë‚´ìš©ì„ ì–´ë–»ê²Œ í˜„ì—…ì—ì„œ í™œìš©í• ì§€ & ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŠë‚€ì ì— ì¤‘ì \në‹¨ìˆœ ì±… ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ê±´ ì•ìœ¼ë¡œ ì‹¤ì œ ì—…ë¬´ í™œìš© ì‹œ, ë„ì›€ì´ ì•ˆë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³¸ì¸ë§Œì˜ ë°©ì‹ìœ¼ë¡œ ì¬êµ¬ì¡°í™”\n\n\n\n\nëª©ì  : í•´ë‹¹ ì±•í„°ë¥¼ ë°°ìš°ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?\n\nì‹¤í—˜ ë””ìì¸ ì‹œ, ì´ìƒì ì´ì§€ë§Œ, Treatment ì¸ê³¼íš¨ê³¼ ì¶”ì •ì— ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•\n\n\n\n\në‚´ìš© : ì´ë²ˆ ì±•í„°ì—ì„œ ì–´ë–¤ ë©”ì„¸ì§€ë¥¼ ì „ë‹¬í•˜ë ¤ê³  í•˜ëŠ”ê°€?\n\nëª¨ì§‘ë‹¨ì„ ê°€ì§€ê³  ì‹¤í—˜ì„ í•˜ëŠ” ê²ƒì€ í˜„ì‹¤ì ìœ¼ë¡œ ì–´ë ¤ì›€. ë”°ë¼ì„œ, ëª¨ì§‘ë‹¨ê³¼ ìœ ì‚¬í•œ ê·¸ë£¹ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ Random Assignment í•˜ëŠ” ê²ƒì´ í•µì‹¬\nRandomised Experiments ìƒí™©ì—ì„œ, Association = Causation\n\nê²°êµ­, ìœ ì €ë¥¼ ëœë¤í•˜ê²Œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” í™˜ê²½ì´ë©´, ë³µì¡í•œ ì‹¤í—˜ë””ìì¸ or í†µê³„ ëª¨í˜•ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ Causal Effect ì¶”ì •ì´ ê°€ëŠ¥\n\nRCT ì´í›„ì— ë°°ìš¸ ë‚´ìš© IV, DID, RDD, Matching, Synthetic Controlì´ ì™œ í•„ìš”í•œì§€ ë‚´í¬í•˜ëŠ” ì±•í„°\n\nì°¸ê³ ë¡œ, ìœ„ì˜ ë°©ë²•ë¡ ì€ ì‹¤í—˜ ë””ìì¸ì„ ì˜ í™œìš©í•´ì„œ, ë‘ ê·¸ë£¹ì„ ë¹„êµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë°©ë²•\n\n\n\n\n\nëŠë‚€ì  : ë°°ìš´ ë‚´ìš©ì„ ì–´ë–»ê²Œ í˜„ì—…ì—ì„œ ì ìš©í•´ë³¼ ìˆ˜ ìˆëŠ”ê°€? ì™œ ì ìš©ì´ ì–´ë ¤ìš¸ê¹Œ?\n\nì§ê´€ê³¼ ì‹¤í—˜ : ì–´ë–»ê²Œ ë³´ë©´ ë‹¹ì—°í•œ, ê²€ì¦ ì‘ì—…ì— ì§ê´€ë³´ë‹¤ëŠ” ì‹¤í—˜ì´ í•„ìš”í•˜ì§€ë§Œ, ì•„ë˜ ìƒí™©ë“¤ë¡œ ì¸í•´ ì—¬ì˜ì¹˜ ì•ŠìŒ\nì¡°ì§ ë¬¸í™” : OCE (Online Controlled Experiment) í”Œë«í¼ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì€ í˜¼ì í•  ìˆ˜ ì—†ìœ¼ë©°, ì¡°ì§ ì°¨ì›ì—ì„œ ì‹¤í—˜ì˜ ì¤‘ìš”ì„±ì„ ì´í•´í•˜ê³  ìˆì–´ì•¼ í•¨\n\nNAVER Search A/B Test í”Œë«í¼ [Blog Link] / [Video]\nì˜¤ëŠ˜ì˜ ì§‘ A/B ì‹¤í—˜ í”Œë«í¼ êµ¬ì¶•ê¸° [Link]\n\në¦¬ì†ŒìŠ¤ : ì´í•´ ë¶€ì„œì™€ ì‹¤í—˜ ì„¤ê³„ë¥¼ ê°™ì´í•˜ê³  ë¶„ì„ì„ ì§„í–‰í•˜ëŠ” ê²ƒì€ ì‹œê°„ê³¼ ë¹„ìš© ì†Œëª¨ê°€ í° ì‘ì—…. ë”°ë¼ì„œ, ì‹¤í—˜ ì„¤ê³„ë‹¨ê³„ì—ì„œ ê° ì¡°ì§ì˜ ì„±ê³¼ ì§€í‘œ Alignì´ ë˜ì—ˆëŠ”ì§€ í™•ì¸ í•„ìš”\nìœ ì € ê²½í—˜ : ë™ì¼ ë¹Œë“œì—ì„œ ì–´ë–¤ ìœ ì €ì—ê²Œë§Œ í”„ë¡œëª¨ì…˜ì„ í•˜ëŠ” ê²ƒì€ ë¼ì´ë¸Œ ì„œë¹„ìŠ¤ ì¸¡ë©´ì—ì„œ ì•ˆì¢‹ì„ ìˆ˜ ìˆìŒ\në¶„ì„ê°€ì˜ ê³ ì¶©\n\nê¸°íš/ê°œë°œ ë‹¨ê³„ì—ì„œ ê²€ì¦í•´ì•¼í•  ì§€í‘œê°€ ì œëŒ€ë¡œ í˜‘ì˜/ë…¼ì˜ë˜ì§€ ì•ŠìŒ\nData Generating Process : ë¡œê·¸ëŠ” ì˜ ë‚¨ê³  ìˆì„ê¹Œ?\nì‹¤í—˜ì„ ì§„í–‰í•  ìœ ì €êµ°ì´ ëœë¤í•˜ê²Œ ë‚˜ë‰˜ì–´ì§€ì§€ ì•ŠìŒ (Selection Bias)\nConfounding Factor ì œëŒ€ë¡œ í†µì œí•˜ì§€ ëª»í•˜ëŠ” ì¼€ì´ìŠ¤ê°€ ì¡´ì¬ (ê¸°ê°„ / ê·¸ë£¹íŠ¹ì„±)\n\n\n\n\n\nì–´ë ¤ìš´ì  : ì–´ë–¤ ë¶€ë¶„ì´ í•´ë‹¹ ì±•í„°ë¥¼ ë‹¤ë£° ë•Œ ì–´ë µê³  ìƒì†Œí–ˆë‚˜?\n\në„ë©”ì¸ ì§€ì‹ : ë¯¸êµ­ì˜ ì˜ë£Œë³´í—˜ ì²´ê³„ ë° ì‚¬íšŒì ì¸ ë°°ê²½ì„ ì˜ ëª°ë¼ì„œ, ë‚´ìš©ì„ ì´í•´í•˜ëŠ”ë° ê¹Œë‹¤ë¡œì› ìŒ\nê°€ì„¤ê²€ì •ê³¼ ì¸ê³¼ì¶”ë¡  : ì–´ë–¤ ë¶€ë¶„ì—ì„œ ê´€ë ¨ì´ ìˆëŠ”ì§€ ìƒê°í•˜ëŠ”ë°, ì‹œê°„ì„ ë§ì´ ìŸìŒ\n\n\n\n\n\n\n\n\n\nğŸ’¡Causal Inference is concerned with a very specific kind of prediction problem\n\nPredicting the results of an action, manipulation, or Intervention - â€œMaking Things Happenâ€ (2003, Woodward)\n\n\n\n\n\nì¸ê³¼ì¶”ë¡  (Causal Inference)ì´ë€ ë¬´ì—‡ì¼ê¹Œ?\n\në¬¸ì œì— ëŒ€í•œ ì›ì¸ì„ ì°¾ê³  í•´ë‹¹ ì›ì¸ì— ëŒ€í•œ íš¨ê³¼ë¥¼ ì¶”ë¡ í•˜ëŠ” ê²ƒ\nì¦‰, Treatmentë¥¼ ì£¼ì—ˆì„ ë•Œ, ì´ì— ë”°ë¥¸ Outcomeì´ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ë¥¼ ì¶”ì •\n\nì´ë²¤íŠ¸(Treatment)ë¥¼ ì§„í–‰í•˜ë©´, ìœ ì €ì˜ ì”ì¡´ìœ¨(Outcome)ì´ ë†’ì•„ì§ˆê¹Œ?\n\n\n\n\n\nê·¸ëŸ¬ë©´ MLê³¼ ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€? &lt; Causal Inference vs Machine Learning &gt;\n\nCausal Inference : Potential outcomesê¹Œì§€ ê³ ë ¤\nMachine Learning : Observed outcomesë§Œì„ ê³ ë ¤\n\n\n\n\nìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. Why? Confounding Factors\n\n\n\nimg\n\n\n\nâœ… Standard Approach & Causal Approach\n\nê¸°ë³¸ ì ‘ê·¼ : \\(X\\)ì˜ ë³€í™”ê°€ \\(Y\\)ì˜ ë³€í™”ì™€ ì–´ë–»ê²Œ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ ì •ëŸ‰í™”í•˜ëŠ”ë° ê´€ì‹¬\n\n\\(Y = Î²_0 + Î²_1X+ Îµ\\) â†’ \\(E(Y|X=x+1) - E(Y|X=x)\\)\nRegression (Chapter 2)ì—ì„œ ë‹¤ë£° ê¸°ë³¸ì ì¸ ì ‘ê·¼\n\nì¸ê³¼ì¶”ë¡  ì ‘ê·¼ : \\(X\\) (ì›ì¸)ì˜ ë³€í™”ê°€ \\(Y\\)ì˜ ë³€í™”ë¥¼ ìœ ë°œí•˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ”ë° ê´€ì‹¬\n\ní•´ë‹¹ ì ‘ê·¼ì€ Yê°€ ë³€í•˜ëŠ” ì´ìœ (ì›ì¸)ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µì„ í•  ìˆ˜ ìˆìŒ\në§Œì•½ \\(X\\)ì™€ \\(Y\\)ê°€ ì¸ê³¼ì ìœ¼ë¡œ ê´€ê³„ê°€ ìˆë‹¤ë©´, \\(Y\\)ì˜ ë³€í™”ëŠ” \\(X\\)ì˜ ë³€í™”ë¡œ ì„¤ëª… ê°€ëŠ¥\n\n\n\n\n\n\n\n\n\nâ–¶ï¸ ê±´ê°•ë³´í—˜ì´ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ì¸ê³¼íš¨ê³¼ ì¶”ì • &lt;ê³ ìˆ˜ë“¤ì˜ ê³„ëŸ‰ê²½ì œí•™ Chapter 1. ë¬´ì‘ìœ„ ì‹œí–‰&gt;\n\n\n\n\n\nëª©ì  : ë³´í—˜ ê°€ì…ì´ ì‹¤í—˜ ëŒ€ìƒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ 2ê°€ì§€ ê´€ì ì—ì„œ íŒŒì•… â†’ ì‹¤í—˜ ê·œëª¨ì™€ ë¹„ìš©ì„ ê³ ë ¤ ì‹œ, ì—¬ëŸ¬ ê°€ì§€ì˜ ëª©í‘œ ì§€í‘œ ì„¤ì •ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ í•  ìˆ˜ ìˆìŒ\n\nê°€ì„¤ 1 : ë³´í—˜ ì˜ë£Œ ê°€ê²©ì´ ê°ì†Œí•˜ë©´, ì‹¤ì œë¡œ ì˜ë£Œ ì„œë¹„ìŠ¤ë¥¼ ë” ì‚¬ìš©í•  ê²ƒì´ë‹¤. â†’ Yes\nê°€ì„¤ 2 : ë³´í—˜ ê°€ì…ì„ í†µí•´, ê±´ê°• ì¦ì§„ì˜ ì¸ê³¼ì  íš¨ê³¼ê°€ ìˆì„ ê²ƒì´ë‹¤. â†’ No\n\n\n\n\n\n\n\në°°ê²½ : êµ­ê°€ ì •ì±…ì„ ìœ„í•´, ì˜í–¥ì„ ë°›ëŠ” ëª¨ë“  êµ­ë¯¼ì„ ëŒ€ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œ ë°œìƒ í•  ìˆ˜ ìˆìŒ\n\nêµ­ê°€ì ìœ¼ë¡œ ë„ˆë¬´ë‚˜ í° ë¦¬ì†ŒìŠ¤ê°€ ë“¤ì–´ê°€ë©° í†µì œ í•˜ê¸° ì‰½ì§€ ì•ŠìŒ\në˜í•œ, Random Assignment ê³¼ì •ì—ì„œ ìœ¤ë¦¬ì ì¸ ë¶€ë¶„ì´ ì´ìŠˆê°€ ë  ìˆ˜ ìˆìŒ\n\n\n\n\nëª©ì  : ëª¨ì§‘ë‹¨ (Population)ì„ ì˜ ë°˜ì˜í•˜ëŠ” ì‹¤í—˜ ìœ ì €êµ° (Sampling Group)ì„ ì ì ˆíˆ ìƒ˜í”Œë§\n\nSampling Bias ìµœì†Œí™”\nApple to Apple (Random Assignment)\n\n\n\n\nì‹¤í—˜ ê·¸ë£¹ ì„¤ê³„ : ì˜¬ë°”ë¥¸ ì‹¤í—˜ì„¤ê³„ë¥¼ í†µí•´, ëª©ì ì— ë§ëŠ” ì¸ê³¼íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•¨\n\ní…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ê°€ì„¤ ì„¤ê³„ ë° ëª©í‘œ ì§€í‘œ ì„¤ê³„\n\nPrimary Index &lt;ê¶ê·¹ì ì¸ ëª©í‘œê°€ ë˜ëŠ” ì§€í‘œ&gt; : ì§€ì¶œí•œ ì˜ë£Œë¹„ / ê±´ê°• ì§€í‘œ\nSecondary Index &lt;ì‹¤í—˜ê³¼ ì§ì ‘ì  ì—°ê´€ì´ ë˜ëŠ” ì§€í‘œ&gt; : ë¶€ë‹´ ë³´í—˜ë£Œ\nGuardrail Index &lt;ì‹¤í—˜ ê³¼ì •ì— ë¶€ì •ì  ì˜í–¥ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ì§€í‘œ&gt; : ê±´ê°• ì§€í‘œ í‘¸ì‹œë¥¼ í–ˆëŠ”ë° ì´íƒˆí•˜ëŠ” ìœ ì €ê°€ ë°œìƒí•œ Nexon ì‚¬ë¡€\n\n\n\n\nì‹¤í—˜êµ°/ëŒ€ì¡°êµ° ì„¤ì • : ë³´í—˜ì˜ ë³´ì¥ ìˆ˜ì¤€ (ê°€ì…ì ë¶€ë‹´ ë³´í—˜ë£Œ ìˆ˜ì¤€)ì— ë”°ë¼ 5ê°œ ìƒí’ˆìœ¼ë¡œ êµ¬ë¶„\n\nControl Group : ë¬´ë³´í—˜ ìƒíƒœì— ê°€ê¹Œìš´ ë³´í—˜ ìˆ˜ì¤€ì„ ê°€ì§„ ìƒí’ˆ (ì¬ë‚œì  í”Œëœ)\nTreatment Group : ê·¸ ì™¸ ë³´í—˜ ë³´ì¥ì´ ë˜ëŠ” ìƒí’ˆêµ° (4ê°€ì§€)\n\n\n\n\në°©ë²• : ë³´í—˜ ë¯¸ê°€ì…ìë¥¼ ëŒ€ìƒìœ¼ë¡œ 5ê°œì˜ ë³´í—˜ ìƒí’ˆì— Random Assignment\n\n\n\nê·¸ë£¹ ê²€ì¦ : ê³¼ì—° ëœë¤í•˜ê²Œ ë‚˜ëˆˆ ì‹¤í—˜êµ°ê³¼ ëŒ€ì¡°êµ°ì´ ì„œë¡œ ë¹„êµ ê°€ëŠ¥í•œê°€ (Ceteris Paribus)\n\nì‹¤í—˜ ëŒ€ìƒ (ìœ ì €/êµ­ë¯¼)ì„ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” ë¹„êµ ë³€ìˆ˜ ì„¤ì • (Ex. ë‚˜ì´/ì—°ë ¹/ê³¼ê¸ˆ ìˆ˜ì¤€ ë“±)\ní†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸ â†’ ìƒí™©ì— ë”°ë¼ A/A í…ŒìŠ¤íŠ¸ ì§„í–‰ì„ í•˜ê¸°ë„ í•¨\ní‘œë³¸ì˜ ìˆ˜ê°€ ì¶©ë¶„í•œì§€ (Law of Large Numbers ë‚˜ì˜¨ ë°°ê²½) & ë™ì¼í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì œëŒ€ë¡œ ë‚˜ë‰˜ì—ˆëŠ”ì§€ ì²´í¬ í•„ìš”\n\nâ†’ í•´ë‹¹ ì‹¤í—˜ì—ì„œ ê·¸ë£¹ê°„ ì°¨ì´ëŠ” ë¬´ì‹œí•´ë„ ë˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì—¬ì§\n\n\n\n\n\n\nì‹¤í—˜ ì§„í–‰ ì‹œ ì²´í¬ ì‚¬í•­\n\nì§€í‘œ ëª¨ë‹ˆí„°ë§ : ì‹¤í—˜ì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ, ì‹¤í—˜ ëŒ€ìƒ/ìœ ì €ê°€ ë°›ê²Œë  ê²½í—˜ì— ë¶€ì •ì ì¸ ìš”ì†Œê°€ ìˆëŠ”ì§€ & ì‹¤í—˜ì— ì˜í–¥ì„ ì£¼ëŠ” ì™¸ë¶€ ìš”ì¸ì´ ìˆëŠ”ì§€ ëª¨ë‹ˆí„°ë§\në¡œê·¸ í™•ì¸ : ì‹¤í—˜ ë¶„ì„ì— ì§„í–‰ë  ë¡œê·¸ê°€ ì˜ ìŒ“ì´ê³  ìˆëŠ”ì§€ í™•ì¸\n\n\n\n\nì‹¤í—˜ ë¶„ì„ : ë¦¬í¬íŠ¸ ë° ì‹¤í—˜ Dashboard ì œê³µ\n\ní•´ë‹¹ ì‹¤í—˜ì„ í†µí•´, ìœ ì €ì˜ ê²½í—˜ì„ ì–´ë–¤ ì¸¡ë©´ì—ì„œ ê°œì„ í–ˆëŠ”ì§€ ì‚¬ì „ ì„¤ê³„ ì§€í‘œ ë° ì‹¤í—˜ ê·¸ë£¹ì„ ë°”íƒ•ìœ¼ë¡œ ì„±ê³¼ ë¶„ì„\n\n\n\n\n\n\n\nëª©ì  :\n\nì‹¤í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì¡°ì§ ë‚´ ì˜ì‚¬ê²°ì •ì— í™œìš© â†’ ë³´í—˜ ê°€ì…ì´ ê±´ê°• ì¦ì§„ì— ë„ì›€ì´ ë ê¹Œ? No\nì´ë²ˆ ì‹¤í—˜ì—ì„œ ì–»ì€ Insightì™€ ë³´ì™„ì ì„ í†µí•´, ì´í›„ ì‹¤í—˜ ê³¼ì • ê°œì„ \n(ë°˜ë³µ ì‹¤í—˜ì´ ê°€ëŠ¥í•˜ë‹¤ë©´) ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‹¤í—˜ì„ ê°œì„  ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë…¸ë ¥\n\n\n\n\nê±´ê°• ë³´í—˜ ì‹¤í—˜ Feedback\n\nì‹¤í—˜ ì„¤ê³„ì˜ ë¬¸ì œ : ê±´ê°•ë³´í—˜ì´ë¼ëŠ” êµ­ê°€ì  ì‹¤í—˜ì—ì„œ ì™„ë²½í•œ Random Assignmentê°€ ëœ ê²ƒì´ ë§ì„ê¹Œ?\n\nSampling Bias : ëª¨ì§‘ë‹¨ (ì‹¤ì œë¡œ ë³´í—˜ì— ê°€ì…í•˜ì§€ ì•Šì€ ì‚¬ëŒë“¤)ê³¼ ìƒ˜í”Œ ê·¸ë£¹ (ì¬ë‚œì  í”Œëœì— ê°€ì…ëœ Control Group) ê°„ì˜ ì°¨ì´ê°€ ì¡´ì¬\nUnobserved Confounders : ê´€ì¸¡ëœ ì¸êµ¬í†µê³„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ê³„ë¥¼ í–ˆì„ ë•Œ, ì•½ê°„ì˜ Sampling Bias ì¡´ì¬. ê·¸ë ‡ë‹¤ë©´, ê´€ì¸¡ë˜ì§€ ì•Šì€ ë³€ìˆ˜ì—ì„œëŠ” í•´ë‹¹ ë¶€ë¶„ì´ ë” í¬ê²Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?\nCensoring Issue : ì‹¤í—˜ ì¤‘ë„ ì´íƒˆì / ì‹¤í—˜êµ°ì´ì§€ë§Œ, ì˜í–¥ì„ ë°›ì§€ ì•Šì€ ìœ ì €ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬ë¥¼ í–ˆëŠ”ê°€? ì œì™¸í–ˆë‹¤ë©´ Selection Biasê°€ ì•„ë‹Œê°€?\n\n\n\n\nê²°ê³¼ì˜ ìœ íš¨ì„± : ë³´í—˜ ì¦ì§„ìœ¼ë¡œ ê±´ê°• ê°œì„ ì„ í•  ìˆ˜ ì—†ì—ˆë˜ ê²ƒì´ ë§ì„ê¹Œ?\n\nëª©í‘œ ì§€í‘œ : ê±´ê°• ì§€í‘œëŠ” ê³¼ì—° ê°ê´€ì ìœ¼ë¡œ ì •ëŸ‰í™”ê°€ ê°€ëŠ¥í•œ ë¶€ë¶„ì¸ê°€?\nì‹¤í—˜ ê°œì„  : ë³´í—˜ ê°€ì…ì— ì˜í•œ íš¨ê³¼ê°€ ì—†ë‹¤ë©´ ì´í›„ì˜ ì‹¤í—˜ ê°œì„ ì€? ì˜¤ë¦¬ê±´ì˜ ê±´ê°• ë³´í—˜ ì‹¤í—˜\n\n\n\n\n\n\n\n\n\n\nëª©ì  : ê²°êµ­, ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ ì´í•´í•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•´ë‚˜ê°€ê¸° ìœ„í•¨\n\nTreatment : ì ‘ì† ì´ë²¤íŠ¸ ì°¸ì—¬ ì—¬ë¶€\n\nì‹¤í—˜êµ° (Treatment Group) : ì´ë²¤íŠ¸ ì°¸ì—¬ì— ë°°ì •ëœ ìœ ì €êµ°\nëŒ€ì¡°êµ° (Control Group) : ì´ë²¤íŠ¸ ì°¸ì—¬ì— ë°°ì •ë˜ì§€ ì•Šì€ ìœ ì €êµ°\n\nOutcome : ìœ ì € ì”ì¡´ìœ¨\n\në¹„êµ ê·¸ë£¹ê°„ ìœ ì˜ë¯¸í•œ ì°¨ì´ (ATE, Average Treatment Effect)ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ì§€í‘œ\n\n\n\n\n\në¬¸ì œ : Counterfactuals (Counter to fact, ì¼ì–´ë‚˜ì§€ ì•Šì€ ìƒí™©ì„ ê°€ì •)\n\nì‹¤í—˜ ì§„í–‰ ì‹œ, ìœ ì €ëŠ” ì°¸ì—¬/ë¯¸ì°¸ì—¬ ì¤‘ í•œ ê°œì˜ ìƒíƒœë¡œë§Œ ì¡´ì¬í•  ìˆ˜ ìˆìŒ\në”°ë¼ì„œ, ì‹¤í—˜ ë‹¹ì‹œ ë¯¸ì°¸ì—¬ ìœ ì €ê°€ ê·¸ë ‡ì§€ ì°¸ì—¬í•œ ìƒí™©(ì‹¤ì œë¡œ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ)ì„ ê°€ì •\ní•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” íƒ€ì„ë¨¸ì‹ ì´ ì—†ê¸° ë•Œë¬¸ì— ë™ì¼í•œ ìœ ì €ì— ëŒ€í•´ì„œ 2ê°€ì§€ ì‚¬í•­ì„ ê´€ì¸¡ ë¶ˆê°€\nâ†’ ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œ \n\n\n\nimg\n\n\n\n\n\n\n\n\ní•˜ë‚˜ì˜ ì‹¤í—˜ ëŒ€ìƒì— ëŒ€í•´ Treatmentì— ëŒ€í•œ Potential Outcomes ëª¨ë‘ ê´€ì°° ë¶ˆê°€\n\nSelection Bias ë°œìƒí•˜ëŠ” ì´ìœ ?\n\nì¸ê³¼ì¶”ë¡  ê·¼ë³¸ì ì¸ ë¬¸ì œ : Control Group â‰  Counterfactuals\níŠ¹ì„±ì´ ë‹¤ë¥¸ ë‹¤ë¥¸ ëŒ€ìƒê³¼ ë¹„êµ ì‹œ Selection Biasê°€ ë°œìƒ : &lt;ì¿ ì¦ˆë‹¤ë¥´ì™€ ë§ˆë¦¬ì•„ ì‚¬ë¡€&gt;\n\nê²°êµ­, ì´ ë¬¸ì œê°€ í•´ê²°ì´ ë˜ì–´ì•¼ Treatmentì— ë”°ë¥¸ ì¸ê³¼ì ì¸ íš¨ê³¼ íŒŒì•…ì´ ê°€ëŠ¥\n\n\n\n\nIndividual Treatment Effect (ITE) : ê°œë³„ ìœ ì € i ì— ëŒ€í•´ Treatment ì²˜ì§€ íš¨ê³¼\n\nìœ ì € i ì—ê²ŒëŠ” 2ê°€ì§€ Potential Outcomesì´ ì¡´ì¬\n\n\\(T = 1\\) : ì´ë²¤íŠ¸ ì°¸ì—¬ / \\(T = 0\\) : ì´ë²¤íŠ¸ ë¯¸ì°¸ì—¬\n2ê°€ì§€ Potential Outcomes ì¤‘ì—ì„œ í•˜ë‚˜ë§Œ ì¡´ì¬í•  ìˆ˜ ìˆìŒ\nìœ ì € i ì— ëŒ€í•œ ê°œë³„ ì¸ê³¼íš¨ê³¼ (ITE)\n$ ITE = Y_{0i} - Y_{1i} $\n\n\nAverage Treatment Effect (ATE) : ìœ ì € ê·¸ë£¹ì— ëŒ€í•œ Treatment ì²˜ì§€ íš¨ê³¼\n\nìœ ì € ê°œì¸í™” ê´€ì ì—ì„œëŠ” ITEê°€ ì´ìƒì ì´ê³  ì¤‘ìš”í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì€ ìœ ì € ê·¸ë£¹ë‹¨ìœ„ì˜ ì‹¤í—˜ì´ ì¼ë°˜ì ì´ë©° ê°œê°œì¸ì— ëŒ€í•´ ITEë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ì¡´ì¬\në”°ë¼ì„œ, ìœ ì € ê°œì¸ì˜ ì¸ê³¼íš¨ê³¼ë¥¼ í‰ê· ì„ ë‚´ì–´ ì§‘ë‹¨ ë ˆë²¨ì—ì„œ ì„¤ëª…\në§Œì•½, ì œëŒ€ë¡œ ëœ ì‹¤í—˜ ì„¤ê³„ë¥¼ í•˜ì§€ ì•Šê³  ATEë¥¼ ê³„ì‚°í•œë‹¤ë©´? ì•„ë˜ì™€ ê°™ì€ Selection Biasê°€ ìƒê¹€ &lt;Causal Inference for the brave and true Chapter1. ìˆ˜ì‹ ì°¸ì¡°&gt;\n\n$ E[Y|T=1] - E[Y|T=0] = E[Y_1|T=1] - E[Y_0|T=0] + E[Y_0|T=1] - E[Y_0|T=1] $\n$ E[Y|T=1] - E[Y|T=0] = {ATT} + $\n\n\n\n\n\nì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ ë°”ë¼ë³´ëŠ” 3ê°€ì§€ ê´€ì \n\nPotential Outcomes : ë¬¼ìŒí‘œ ì±„ìš°ê¸°\nStructural Causal Models : DAG\nRegression â†’ ë‹¤ìŒ ì±•í„°ê°€ Regressionì¸ ì´ìœ !\n\nì˜¤ì°¨í•­ ê°€ì •(Gaussian Assumption)ê³¼ ë‚´ìƒì„± (Endogenity) â†’ ë„êµ¬ë³€ìˆ˜ ì°¸ì¡°\n\n\n\n\n\nSelection Bias í•´ê²°í•˜ê¸° ìœ„í•´ ì´ë²ˆ ì±•í„°ì—ì„œëŠ” Random Assignment ë„ì…\n\nì¦‰, ì‹¤í—˜ ëŒ€ìƒë¥¼ ë™ì „ë˜ì§€ê¸°ë¡œ ë‚˜ëˆ ì„œ Treatment ì—¬ë¶€ë¥¼ ê²°ì •\n\n\n\n\nRandom Assignmentì´ ê°€ì¥ ì¢‹ì€ ë°©ë²•ì´ì§€ë§Œ, Research Design í•„ìš”ì„± ì¡´ì¬\n\nëª©ì  :\n\ní•˜ì§€ë§Œ, í•­ìƒ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ RCTë¥¼ í™œìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ë§ìŒ\nìœ„ì™€ ê°™ì€ ê²½ìš°, Counterfactualê³¼ ìµœëŒ€í•œ ë¹„ìŠ·í•œ Control Groupë¥¼ ì‹¤í—˜ ë””ìì¸ì„ í†µí•´ ì°¾ì•„ë‚˜ê°€ì•¼ í•¨ â†”ï¸ Selection Bias ì¤„ì´ê¸°\n\n\n\n\nì‹¤í—˜ ë””ìì¸ ë°©ë²•ë¡  (To be continued)\n\nInstrumental Varibles (2SLS / Regression, Chapter 3)\nRDD (Regression Discontinuity Design, Chapter 4)\nDID (Difference In Difference, Chapter 5)\nSynthetic Control\n\n\n\n\n\n\n\nLLN (Law of Large Numbers) ë‚˜ì˜¤ê²Œ ëœ ë°°ê²½\n\nê²°êµ­ ëª¨ì§‘ë‹¨ì„ ì˜ ëŒ€í‘œí•˜ëŠ” í‘œë³¸(Sample)ì„ ì„ ì •í•˜ê¸° ìœ„í•´, ì¶©ë¶„í•œ ìˆ˜ì˜ í‘œë³¸ì´ í•„ìš”\nê¶ê·¹ì ìœ¼ë¡œ, ì‹¤í—˜êµ°ê³¼ ëŒ€ì¡°êµ°ì€ ë™ì¼í•œ ëª¨ì§‘ë‹¨ì—ì„œ ìƒì„± â†’ ê·¸ë£¹ê°„ì— ë¹„êµ ê°€ëŠ¥í•œ íŠ¹ì„±ì„ ê°€ì ¸ì•¼í•¨ â†’ í•´ë‹¹ ì¡°ê±´ ë‹¬ì„±ì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ í‘œë³¸ì´ í•„ìš”\nê³¼ì—° Sample SizeëŠ” ì–´ë–¤ ìˆ˜ì¤€ì´ ì ì ˆí• ê¹Œ?\n\nStatistical Power(ê²€ì •ë ¥)ê³¼ Sample Size\nProduct Active User ê·œëª¨ë¥¼ ê³ ë ¤\n\n\n\n\n\nHypothesis Testing\n\nëª©ì  : ëª¨ì§‘ë‹¨ (Population Data)ì˜ íŠ¹ì„±ì— ëŒ€í•´ ì„¤ê³„í•œ í†µê³„ì  ê°€ì„¤ (\\(H_0\\) / \\(H_1\\))ì„ ëª¨ì§‘ë‹¨ì˜ ì¶”ì¶œí•œ ìƒ˜í”Œ ë°ì´í„° (Sample Data)ë¥¼ ì´ìš©í•´ ê²€ì¦í•˜ëŠ” ê³¼ì •\nì¸ê³¼ì¶”ë¡ ê³¼ ê°€ì„¤ ê²€ì • : ê²°êµ­, ì¸ê³¼ì ì¸ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ ëª¨ì§‘ë‹¨ì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì´ìš©í•´, ê°œì… (Intervention)ì— ë”°ë¥¸ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ê²€ì¦\n\nê·¸ë˜ì„œ, í›„ë°˜ë¶€ì— ê°€ì„¤ ê²€ì • T-test â†”ï¸ Two Sample T-test (â†”ï¸ Hausman Test)ì´ ë‚˜ì˜¤ê²Œëœ ê²ƒ ê°™ìŒ\n\n\n\n\n\n\n\n\nA/B í…ŒìŠ¤íŠ¸ë¥¼ í•  ìˆ˜ ìˆëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ë° ìë£Œ\n\nìì²´ OCE (Online Controlled Experiment) í”Œë«í¼ì´ ìˆë‹¤ë©´ ìµœì ì˜ í™˜ê²½\nì‹¤ì œë¡œ 3rd Party íˆ´ (Amplitude / Braze / Firebase ë“±)ì„ í†µí•´ì„œ A/B Testë¥¼ í•´ë³¼ ìˆ˜ ìˆìŒ\nGrowthBookê³¼ ê°™ì´ ì˜ ì•Œë ¤ì§„ ì˜¤í”ˆ ì†ŒìŠ¤ë¥¼ í†µí•´, í˜„ì—…ì— ì ìš©ì´ ê°€ëŠ¥\nì˜¤í”ˆ ì†ŒìŠ¤ ì •ë¦¬ìë£Œ ë§í¬ : https://posthog.com/blog/best-open-source-ab-testing-tools\n\n\n\n\nì‹¤í—˜ í”Œë«í¼\n\nì§ê´€ì´ ì•„ë‹Œ ì‹¤í—˜ìœ¼ë¡œì˜ ì˜ì‚¬ê²°ì •ì€ ì¡°ì§ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê³¼ì œ\nì°¸ê³  ë„ì„œ : ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì˜ ì‹¤í—˜\n\n\n\n\n\n\n(ì¢…ì–¸) ì‹¤í—˜ì— ì‚¬ìš©í•  ìƒ˜í”Œ ì‚¬ì´ì¦ˆëŠ” ì–´ë–»ê²Œ ê°€ì ¸ê°€ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?\n\n(ì´ì‚­) : í†µê³„í•™ì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” Statistical Powerê³¼ ìƒ˜í”Œ ì‚¬ì´ì¦ˆê°€ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ ìˆì–´, ê²€ì •ë ¥ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒ˜í”Œ ì‚¬ì´ì¦ˆê°€ ì ì ˆí•œì§€ íŒë‹¨í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤,\n(ì†Œí¬) : ì ì • ìƒ˜í”Œ ì‚¬ì´ì¦ˆë¥¼ ê³„ì‚°í•´ì£¼ëŠ” ì‚¬ì´íŠ¸ë¥¼ ì‚¬ìš©í•´ë´¤ì–´ìš”.\n(ì§„ìˆ˜) : ì˜¤íˆë ¤ ë„ˆë¬´ ì ì€ ê·¹ë‹¨ì ì¸ ì¼€ì´ìŠ¤ëŠ” Biasê°€ ë§ê³  ì¼ë°˜ì ìœ¼ë¡œ ì •ìƒì ì¸ ë²”ì£¼ì—ì„œ ë²—ì–´ë‚¬ë‹¤ê³  ìƒê°í•´, ì‹¤í—˜ ëŒ€ìƒìœ¼ë¡œëŠ” ì œì™¸í–ˆë˜ ê¸°ì–µì´ ìˆìŠµë‹ˆë‹¤. ì›ë¡ ì ì¸ ë‹µë³€ì´ì§€ë§Œ í•­ìƒ Productì˜ Active User ìˆ˜ë¥¼ ê³ ë ¤í•´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n\n\n\n\n(ì´ì‚­) ì˜ë£Œ ë³´í—˜ ê°€ì…ìœ¼ë¡œ ê±´ê°• ì¦ì§„ì˜ íš¨ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆëŠ”ë°, ë¯¸êµ­ì˜ ì‚¬ë¡€ì—¬ì„œ ê·¸ëŸ°ê±¸ê¹Œìš”?\n\n(ì •í˜„) : ì˜ë£Œ ë³´í—˜ ì²´ê³„ê°€ ë‹¤ë¥´ì§€ë§Œ í•œêµ­ë„ ë¹„ìŠ·í•˜ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì‹¤í—˜ì„ í•´ë³´ì§€ ì•Šì•„ì„œ ì§ê´€ì ì¸ íŒë‹¨ì— ì£¼ì˜í•´ì•¼í•  ê²ƒ ê°™ì•„ìš”.\n\n\n\n\n\n\nê³ ìˆ˜ë“¤ì˜ ê³„ëŸ‰ê²½ì œí•™ Chapter 1. ë¬´ì‘ìœ„ ì‹œí–‰ &lt;Joshua D. AngristÂ ,Â Jorn-Steffen Pischke ì €&gt;\nCausal Inference for the brave and true Chapter 1. Introduction to Causality \nKorea Summer Workshop on Causal Inference 2022\nIntroduction to Causal Inference\nìœ  í€´ì¦ˆ ì˜¨ ë” ë¸”ëŸ­ - êµ¬ì¤€ì—½í¸"
  },
  {
    "objectID": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#chapter-summary",
    "href": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#chapter-summary",
    "title": "Randomised Controlled Trial",
    "section": "",
    "text": "ë°°ìš´ë‚´ìš©ì„ ì–´ë–»ê²Œ í˜„ì—…ì—ì„œ í™œìš©í• ì§€ & ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŠë‚€ì ì— ì¤‘ì \në‹¨ìˆœ ì±… ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ê±´ ì•ìœ¼ë¡œ ì‹¤ì œ ì—…ë¬´ í™œìš© ì‹œ, ë„ì›€ì´ ì•ˆë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³¸ì¸ë§Œì˜ ë°©ì‹ìœ¼ë¡œ ì¬êµ¬ì¡°í™”\n\n\n\n\nëª©ì  : í•´ë‹¹ ì±•í„°ë¥¼ ë°°ìš°ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?\n\nì‹¤í—˜ ë””ìì¸ ì‹œ, ì´ìƒì ì´ì§€ë§Œ, Treatment ì¸ê³¼íš¨ê³¼ ì¶”ì •ì— ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•\n\n\n\n\në‚´ìš© : ì´ë²ˆ ì±•í„°ì—ì„œ ì–´ë–¤ ë©”ì„¸ì§€ë¥¼ ì „ë‹¬í•˜ë ¤ê³  í•˜ëŠ”ê°€?\n\nëª¨ì§‘ë‹¨ì„ ê°€ì§€ê³  ì‹¤í—˜ì„ í•˜ëŠ” ê²ƒì€ í˜„ì‹¤ì ìœ¼ë¡œ ì–´ë ¤ì›€. ë”°ë¼ì„œ, ëª¨ì§‘ë‹¨ê³¼ ìœ ì‚¬í•œ ê·¸ë£¹ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ Random Assignment í•˜ëŠ” ê²ƒì´ í•µì‹¬\nRandomised Experiments ìƒí™©ì—ì„œ, Association = Causation\n\nê²°êµ­, ìœ ì €ë¥¼ ëœë¤í•˜ê²Œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” í™˜ê²½ì´ë©´, ë³µì¡í•œ ì‹¤í—˜ë””ìì¸ or í†µê³„ ëª¨í˜•ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ Causal Effect ì¶”ì •ì´ ê°€ëŠ¥\n\nRCT ì´í›„ì— ë°°ìš¸ ë‚´ìš© IV, DID, RDD, Matching, Synthetic Controlì´ ì™œ í•„ìš”í•œì§€ ë‚´í¬í•˜ëŠ” ì±•í„°\n\nì°¸ê³ ë¡œ, ìœ„ì˜ ë°©ë²•ë¡ ì€ ì‹¤í—˜ ë””ìì¸ì„ ì˜ í™œìš©í•´ì„œ, ë‘ ê·¸ë£¹ì„ ë¹„êµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë°©ë²•\n\n\n\n\n\nëŠë‚€ì  : ë°°ìš´ ë‚´ìš©ì„ ì–´ë–»ê²Œ í˜„ì—…ì—ì„œ ì ìš©í•´ë³¼ ìˆ˜ ìˆëŠ”ê°€? ì™œ ì ìš©ì´ ì–´ë ¤ìš¸ê¹Œ?\n\nì§ê´€ê³¼ ì‹¤í—˜ : ì–´ë–»ê²Œ ë³´ë©´ ë‹¹ì—°í•œ, ê²€ì¦ ì‘ì—…ì— ì§ê´€ë³´ë‹¤ëŠ” ì‹¤í—˜ì´ í•„ìš”í•˜ì§€ë§Œ, ì•„ë˜ ìƒí™©ë“¤ë¡œ ì¸í•´ ì—¬ì˜ì¹˜ ì•ŠìŒ\nì¡°ì§ ë¬¸í™” : OCE (Online Controlled Experiment) í”Œë«í¼ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì€ í˜¼ì í•  ìˆ˜ ì—†ìœ¼ë©°, ì¡°ì§ ì°¨ì›ì—ì„œ ì‹¤í—˜ì˜ ì¤‘ìš”ì„±ì„ ì´í•´í•˜ê³  ìˆì–´ì•¼ í•¨\n\nNAVER Search A/B Test í”Œë«í¼ [Blog Link] / [Video]\nì˜¤ëŠ˜ì˜ ì§‘ A/B ì‹¤í—˜ í”Œë«í¼ êµ¬ì¶•ê¸° [Link]\n\në¦¬ì†ŒìŠ¤ : ì´í•´ ë¶€ì„œì™€ ì‹¤í—˜ ì„¤ê³„ë¥¼ ê°™ì´í•˜ê³  ë¶„ì„ì„ ì§„í–‰í•˜ëŠ” ê²ƒì€ ì‹œê°„ê³¼ ë¹„ìš© ì†Œëª¨ê°€ í° ì‘ì—…. ë”°ë¼ì„œ, ì‹¤í—˜ ì„¤ê³„ë‹¨ê³„ì—ì„œ ê° ì¡°ì§ì˜ ì„±ê³¼ ì§€í‘œ Alignì´ ë˜ì—ˆëŠ”ì§€ í™•ì¸ í•„ìš”\nìœ ì € ê²½í—˜ : ë™ì¼ ë¹Œë“œì—ì„œ ì–´ë–¤ ìœ ì €ì—ê²Œë§Œ í”„ë¡œëª¨ì…˜ì„ í•˜ëŠ” ê²ƒì€ ë¼ì´ë¸Œ ì„œë¹„ìŠ¤ ì¸¡ë©´ì—ì„œ ì•ˆì¢‹ì„ ìˆ˜ ìˆìŒ\në¶„ì„ê°€ì˜ ê³ ì¶©\n\nê¸°íš/ê°œë°œ ë‹¨ê³„ì—ì„œ ê²€ì¦í•´ì•¼í•  ì§€í‘œê°€ ì œëŒ€ë¡œ í˜‘ì˜/ë…¼ì˜ë˜ì§€ ì•ŠìŒ\nData Generating Process : ë¡œê·¸ëŠ” ì˜ ë‚¨ê³  ìˆì„ê¹Œ?\nì‹¤í—˜ì„ ì§„í–‰í•  ìœ ì €êµ°ì´ ëœë¤í•˜ê²Œ ë‚˜ë‰˜ì–´ì§€ì§€ ì•ŠìŒ (Selection Bias)\nConfounding Factor ì œëŒ€ë¡œ í†µì œí•˜ì§€ ëª»í•˜ëŠ” ì¼€ì´ìŠ¤ê°€ ì¡´ì¬ (ê¸°ê°„ / ê·¸ë£¹íŠ¹ì„±)\n\n\n\n\n\nì–´ë ¤ìš´ì  : ì–´ë–¤ ë¶€ë¶„ì´ í•´ë‹¹ ì±•í„°ë¥¼ ë‹¤ë£° ë•Œ ì–´ë µê³  ìƒì†Œí–ˆë‚˜?\n\në„ë©”ì¸ ì§€ì‹ : ë¯¸êµ­ì˜ ì˜ë£Œë³´í—˜ ì²´ê³„ ë° ì‚¬íšŒì ì¸ ë°°ê²½ì„ ì˜ ëª°ë¼ì„œ, ë‚´ìš©ì„ ì´í•´í•˜ëŠ”ë° ê¹Œë‹¤ë¡œì› ìŒ\nê°€ì„¤ê²€ì •ê³¼ ì¸ê³¼ì¶”ë¡  : ì–´ë–¤ ë¶€ë¶„ì—ì„œ ê´€ë ¨ì´ ìˆëŠ”ì§€ ìƒê°í•˜ëŠ”ë°, ì‹œê°„ì„ ë§ì´ ìŸìŒ"
  },
  {
    "objectID": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#introduction-to-causal-inference",
    "href": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#introduction-to-causal-inference",
    "title": "Randomised Controlled Trial",
    "section": "",
    "text": "ğŸ’¡Causal Inference is concerned with a very specific kind of prediction problem\n\nPredicting the results of an action, manipulation, or Intervention - â€œMaking Things Happenâ€ (2003, Woodward)\n\n\n\n\n\nì¸ê³¼ì¶”ë¡  (Causal Inference)ì´ë€ ë¬´ì—‡ì¼ê¹Œ?\n\në¬¸ì œì— ëŒ€í•œ ì›ì¸ì„ ì°¾ê³  í•´ë‹¹ ì›ì¸ì— ëŒ€í•œ íš¨ê³¼ë¥¼ ì¶”ë¡ í•˜ëŠ” ê²ƒ\nì¦‰, Treatmentë¥¼ ì£¼ì—ˆì„ ë•Œ, ì´ì— ë”°ë¥¸ Outcomeì´ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ë¥¼ ì¶”ì •\n\nì´ë²¤íŠ¸(Treatment)ë¥¼ ì§„í–‰í•˜ë©´, ìœ ì €ì˜ ì”ì¡´ìœ¨(Outcome)ì´ ë†’ì•„ì§ˆê¹Œ?\n\n\n\n\n\nê·¸ëŸ¬ë©´ MLê³¼ ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€? &lt; Causal Inference vs Machine Learning &gt;\n\nCausal Inference : Potential outcomesê¹Œì§€ ê³ ë ¤\nMachine Learning : Observed outcomesë§Œì„ ê³ ë ¤\n\n\n\n\nìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ. Why? Confounding Factors\n\n\n\nimg\n\n\n\nâœ… Standard Approach & Causal Approach\n\nê¸°ë³¸ ì ‘ê·¼ : \\(X\\)ì˜ ë³€í™”ê°€ \\(Y\\)ì˜ ë³€í™”ì™€ ì–´ë–»ê²Œ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ ì •ëŸ‰í™”í•˜ëŠ”ë° ê´€ì‹¬\n\n\\(Y = Î²_0 + Î²_1X+ Îµ\\) â†’ \\(E(Y|X=x+1) - E(Y|X=x)\\)\nRegression (Chapter 2)ì—ì„œ ë‹¤ë£° ê¸°ë³¸ì ì¸ ì ‘ê·¼\n\nì¸ê³¼ì¶”ë¡  ì ‘ê·¼ : \\(X\\) (ì›ì¸)ì˜ ë³€í™”ê°€ \\(Y\\)ì˜ ë³€í™”ë¥¼ ìœ ë°œí•˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ”ë° ê´€ì‹¬\n\ní•´ë‹¹ ì ‘ê·¼ì€ Yê°€ ë³€í•˜ëŠ” ì´ìœ (ì›ì¸)ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µì„ í•  ìˆ˜ ìˆìŒ\në§Œì•½ \\(X\\)ì™€ \\(Y\\)ê°€ ì¸ê³¼ì ìœ¼ë¡œ ê´€ê³„ê°€ ìˆë‹¤ë©´, \\(Y\\)ì˜ ë³€í™”ëŠ” \\(X\\)ì˜ ë³€í™”ë¡œ ì„¤ëª… ê°€ëŠ¥"
  },
  {
    "objectID": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#experiment-key-takeaway",
    "href": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#experiment-key-takeaway",
    "title": "Randomised Controlled Trial",
    "section": "",
    "text": "â–¶ï¸ ê±´ê°•ë³´í—˜ì´ ê±´ê°•ì— ë¯¸ì¹˜ëŠ” ì¸ê³¼íš¨ê³¼ ì¶”ì • &lt;ê³ ìˆ˜ë“¤ì˜ ê³„ëŸ‰ê²½ì œí•™ Chapter 1. ë¬´ì‘ìœ„ ì‹œí–‰&gt;\n\n\n\n\n\nëª©ì  : ë³´í—˜ ê°€ì…ì´ ì‹¤í—˜ ëŒ€ìƒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ 2ê°€ì§€ ê´€ì ì—ì„œ íŒŒì•… â†’ ì‹¤í—˜ ê·œëª¨ì™€ ë¹„ìš©ì„ ê³ ë ¤ ì‹œ, ì—¬ëŸ¬ ê°€ì§€ì˜ ëª©í‘œ ì§€í‘œ ì„¤ì •ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ í•  ìˆ˜ ìˆìŒ\n\nê°€ì„¤ 1 : ë³´í—˜ ì˜ë£Œ ê°€ê²©ì´ ê°ì†Œí•˜ë©´, ì‹¤ì œë¡œ ì˜ë£Œ ì„œë¹„ìŠ¤ë¥¼ ë” ì‚¬ìš©í•  ê²ƒì´ë‹¤. â†’ Yes\nê°€ì„¤ 2 : ë³´í—˜ ê°€ì…ì„ í†µí•´, ê±´ê°• ì¦ì§„ì˜ ì¸ê³¼ì  íš¨ê³¼ê°€ ìˆì„ ê²ƒì´ë‹¤. â†’ No\n\n\n\n\n\n\n\në°°ê²½ : êµ­ê°€ ì •ì±…ì„ ìœ„í•´, ì˜í–¥ì„ ë°›ëŠ” ëª¨ë“  êµ­ë¯¼ì„ ëŒ€ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œ ë°œìƒ í•  ìˆ˜ ìˆìŒ\n\nêµ­ê°€ì ìœ¼ë¡œ ë„ˆë¬´ë‚˜ í° ë¦¬ì†ŒìŠ¤ê°€ ë“¤ì–´ê°€ë©° í†µì œ í•˜ê¸° ì‰½ì§€ ì•ŠìŒ\në˜í•œ, Random Assignment ê³¼ì •ì—ì„œ ìœ¤ë¦¬ì ì¸ ë¶€ë¶„ì´ ì´ìŠˆê°€ ë  ìˆ˜ ìˆìŒ\n\n\n\n\nëª©ì  : ëª¨ì§‘ë‹¨ (Population)ì„ ì˜ ë°˜ì˜í•˜ëŠ” ì‹¤í—˜ ìœ ì €êµ° (Sampling Group)ì„ ì ì ˆíˆ ìƒ˜í”Œë§\n\nSampling Bias ìµœì†Œí™”\nApple to Apple (Random Assignment)\n\n\n\n\nì‹¤í—˜ ê·¸ë£¹ ì„¤ê³„ : ì˜¬ë°”ë¥¸ ì‹¤í—˜ì„¤ê³„ë¥¼ í†µí•´, ëª©ì ì— ë§ëŠ” ì¸ê³¼íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•¨\n\ní…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ê°€ì„¤ ì„¤ê³„ ë° ëª©í‘œ ì§€í‘œ ì„¤ê³„\n\nPrimary Index &lt;ê¶ê·¹ì ì¸ ëª©í‘œê°€ ë˜ëŠ” ì§€í‘œ&gt; : ì§€ì¶œí•œ ì˜ë£Œë¹„ / ê±´ê°• ì§€í‘œ\nSecondary Index &lt;ì‹¤í—˜ê³¼ ì§ì ‘ì  ì—°ê´€ì´ ë˜ëŠ” ì§€í‘œ&gt; : ë¶€ë‹´ ë³´í—˜ë£Œ\nGuardrail Index &lt;ì‹¤í—˜ ê³¼ì •ì— ë¶€ì •ì  ì˜í–¥ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ì§€í‘œ&gt; : ê±´ê°• ì§€í‘œ í‘¸ì‹œë¥¼ í–ˆëŠ”ë° ì´íƒˆí•˜ëŠ” ìœ ì €ê°€ ë°œìƒí•œ Nexon ì‚¬ë¡€\n\n\n\n\nì‹¤í—˜êµ°/ëŒ€ì¡°êµ° ì„¤ì • : ë³´í—˜ì˜ ë³´ì¥ ìˆ˜ì¤€ (ê°€ì…ì ë¶€ë‹´ ë³´í—˜ë£Œ ìˆ˜ì¤€)ì— ë”°ë¼ 5ê°œ ìƒí’ˆìœ¼ë¡œ êµ¬ë¶„\n\nControl Group : ë¬´ë³´í—˜ ìƒíƒœì— ê°€ê¹Œìš´ ë³´í—˜ ìˆ˜ì¤€ì„ ê°€ì§„ ìƒí’ˆ (ì¬ë‚œì  í”Œëœ)\nTreatment Group : ê·¸ ì™¸ ë³´í—˜ ë³´ì¥ì´ ë˜ëŠ” ìƒí’ˆêµ° (4ê°€ì§€)\n\n\n\n\në°©ë²• : ë³´í—˜ ë¯¸ê°€ì…ìë¥¼ ëŒ€ìƒìœ¼ë¡œ 5ê°œì˜ ë³´í—˜ ìƒí’ˆì— Random Assignment\n\n\n\nê·¸ë£¹ ê²€ì¦ : ê³¼ì—° ëœë¤í•˜ê²Œ ë‚˜ëˆˆ ì‹¤í—˜êµ°ê³¼ ëŒ€ì¡°êµ°ì´ ì„œë¡œ ë¹„êµ ê°€ëŠ¥í•œê°€ (Ceteris Paribus)\n\nì‹¤í—˜ ëŒ€ìƒ (ìœ ì €/êµ­ë¯¼)ì„ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” ë¹„êµ ë³€ìˆ˜ ì„¤ì • (Ex. ë‚˜ì´/ì—°ë ¹/ê³¼ê¸ˆ ìˆ˜ì¤€ ë“±)\ní†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸ â†’ ìƒí™©ì— ë”°ë¼ A/A í…ŒìŠ¤íŠ¸ ì§„í–‰ì„ í•˜ê¸°ë„ í•¨\ní‘œë³¸ì˜ ìˆ˜ê°€ ì¶©ë¶„í•œì§€ (Law of Large Numbers ë‚˜ì˜¨ ë°°ê²½) & ë™ì¼í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì œëŒ€ë¡œ ë‚˜ë‰˜ì—ˆëŠ”ì§€ ì²´í¬ í•„ìš”\n\nâ†’ í•´ë‹¹ ì‹¤í—˜ì—ì„œ ê·¸ë£¹ê°„ ì°¨ì´ëŠ” ë¬´ì‹œí•´ë„ ë˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì—¬ì§\n\n\n\n\n\n\nì‹¤í—˜ ì§„í–‰ ì‹œ ì²´í¬ ì‚¬í•­\n\nì§€í‘œ ëª¨ë‹ˆí„°ë§ : ì‹¤í—˜ì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ, ì‹¤í—˜ ëŒ€ìƒ/ìœ ì €ê°€ ë°›ê²Œë  ê²½í—˜ì— ë¶€ì •ì ì¸ ìš”ì†Œê°€ ìˆëŠ”ì§€ & ì‹¤í—˜ì— ì˜í–¥ì„ ì£¼ëŠ” ì™¸ë¶€ ìš”ì¸ì´ ìˆëŠ”ì§€ ëª¨ë‹ˆí„°ë§\në¡œê·¸ í™•ì¸ : ì‹¤í—˜ ë¶„ì„ì— ì§„í–‰ë  ë¡œê·¸ê°€ ì˜ ìŒ“ì´ê³  ìˆëŠ”ì§€ í™•ì¸\n\n\n\n\nì‹¤í—˜ ë¶„ì„ : ë¦¬í¬íŠ¸ ë° ì‹¤í—˜ Dashboard ì œê³µ\n\ní•´ë‹¹ ì‹¤í—˜ì„ í†µí•´, ìœ ì €ì˜ ê²½í—˜ì„ ì–´ë–¤ ì¸¡ë©´ì—ì„œ ê°œì„ í–ˆëŠ”ì§€ ì‚¬ì „ ì„¤ê³„ ì§€í‘œ ë° ì‹¤í—˜ ê·¸ë£¹ì„ ë°”íƒ•ìœ¼ë¡œ ì„±ê³¼ ë¶„ì„\n\n\n\n\n\n\n\nëª©ì  :\n\nì‹¤í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì¡°ì§ ë‚´ ì˜ì‚¬ê²°ì •ì— í™œìš© â†’ ë³´í—˜ ê°€ì…ì´ ê±´ê°• ì¦ì§„ì— ë„ì›€ì´ ë ê¹Œ? No\nì´ë²ˆ ì‹¤í—˜ì—ì„œ ì–»ì€ Insightì™€ ë³´ì™„ì ì„ í†µí•´, ì´í›„ ì‹¤í—˜ ê³¼ì • ê°œì„ \n(ë°˜ë³µ ì‹¤í—˜ì´ ê°€ëŠ¥í•˜ë‹¤ë©´) ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‹¤í—˜ì„ ê°œì„  ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë…¸ë ¥\n\n\n\n\nê±´ê°• ë³´í—˜ ì‹¤í—˜ Feedback\n\nì‹¤í—˜ ì„¤ê³„ì˜ ë¬¸ì œ : ê±´ê°•ë³´í—˜ì´ë¼ëŠ” êµ­ê°€ì  ì‹¤í—˜ì—ì„œ ì™„ë²½í•œ Random Assignmentê°€ ëœ ê²ƒì´ ë§ì„ê¹Œ?\n\nSampling Bias : ëª¨ì§‘ë‹¨ (ì‹¤ì œë¡œ ë³´í—˜ì— ê°€ì…í•˜ì§€ ì•Šì€ ì‚¬ëŒë“¤)ê³¼ ìƒ˜í”Œ ê·¸ë£¹ (ì¬ë‚œì  í”Œëœì— ê°€ì…ëœ Control Group) ê°„ì˜ ì°¨ì´ê°€ ì¡´ì¬\nUnobserved Confounders : ê´€ì¸¡ëœ ì¸êµ¬í†µê³„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ê³„ë¥¼ í–ˆì„ ë•Œ, ì•½ê°„ì˜ Sampling Bias ì¡´ì¬. ê·¸ë ‡ë‹¤ë©´, ê´€ì¸¡ë˜ì§€ ì•Šì€ ë³€ìˆ˜ì—ì„œëŠ” í•´ë‹¹ ë¶€ë¶„ì´ ë” í¬ê²Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?\nCensoring Issue : ì‹¤í—˜ ì¤‘ë„ ì´íƒˆì / ì‹¤í—˜êµ°ì´ì§€ë§Œ, ì˜í–¥ì„ ë°›ì§€ ì•Šì€ ìœ ì €ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬ë¥¼ í–ˆëŠ”ê°€? ì œì™¸í–ˆë‹¤ë©´ Selection Biasê°€ ì•„ë‹Œê°€?\n\n\n\n\nê²°ê³¼ì˜ ìœ íš¨ì„± : ë³´í—˜ ì¦ì§„ìœ¼ë¡œ ê±´ê°• ê°œì„ ì„ í•  ìˆ˜ ì—†ì—ˆë˜ ê²ƒì´ ë§ì„ê¹Œ?\n\nëª©í‘œ ì§€í‘œ : ê±´ê°• ì§€í‘œëŠ” ê³¼ì—° ê°ê´€ì ìœ¼ë¡œ ì •ëŸ‰í™”ê°€ ê°€ëŠ¥í•œ ë¶€ë¶„ì¸ê°€?\nì‹¤í—˜ ê°œì„  : ë³´í—˜ ê°€ì…ì— ì˜í•œ íš¨ê³¼ê°€ ì—†ë‹¤ë©´ ì´í›„ì˜ ì‹¤í—˜ ê°œì„ ì€? ì˜¤ë¦¬ê±´ì˜ ê±´ê°• ë³´í—˜ ì‹¤í—˜"
  },
  {
    "objectID": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#theoretical-backgroud",
    "href": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#theoretical-backgroud",
    "title": "Randomised Controlled Trial",
    "section": "",
    "text": "ëª©ì  : ê²°êµ­, ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ ì´í•´í•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•´ë‚˜ê°€ê¸° ìœ„í•¨\n\nTreatment : ì ‘ì† ì´ë²¤íŠ¸ ì°¸ì—¬ ì—¬ë¶€\n\nì‹¤í—˜êµ° (Treatment Group) : ì´ë²¤íŠ¸ ì°¸ì—¬ì— ë°°ì •ëœ ìœ ì €êµ°\nëŒ€ì¡°êµ° (Control Group) : ì´ë²¤íŠ¸ ì°¸ì—¬ì— ë°°ì •ë˜ì§€ ì•Šì€ ìœ ì €êµ°\n\nOutcome : ìœ ì € ì”ì¡´ìœ¨\n\në¹„êµ ê·¸ë£¹ê°„ ìœ ì˜ë¯¸í•œ ì°¨ì´ (ATE, Average Treatment Effect)ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ì§€í‘œ\n\n\n\n\n\në¬¸ì œ : Counterfactuals (Counter to fact, ì¼ì–´ë‚˜ì§€ ì•Šì€ ìƒí™©ì„ ê°€ì •)\n\nì‹¤í—˜ ì§„í–‰ ì‹œ, ìœ ì €ëŠ” ì°¸ì—¬/ë¯¸ì°¸ì—¬ ì¤‘ í•œ ê°œì˜ ìƒíƒœë¡œë§Œ ì¡´ì¬í•  ìˆ˜ ìˆìŒ\në”°ë¼ì„œ, ì‹¤í—˜ ë‹¹ì‹œ ë¯¸ì°¸ì—¬ ìœ ì €ê°€ ê·¸ë ‡ì§€ ì°¸ì—¬í•œ ìƒí™©(ì‹¤ì œë¡œ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ)ì„ ê°€ì •\ní•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” íƒ€ì„ë¨¸ì‹ ì´ ì—†ê¸° ë•Œë¬¸ì— ë™ì¼í•œ ìœ ì €ì— ëŒ€í•´ì„œ 2ê°€ì§€ ì‚¬í•­ì„ ê´€ì¸¡ ë¶ˆê°€\nâ†’ ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œ \n\n\n\nimg\n\n\n\n\n\n\n\n\ní•˜ë‚˜ì˜ ì‹¤í—˜ ëŒ€ìƒì— ëŒ€í•´ Treatmentì— ëŒ€í•œ Potential Outcomes ëª¨ë‘ ê´€ì°° ë¶ˆê°€\n\nSelection Bias ë°œìƒí•˜ëŠ” ì´ìœ ?\n\nì¸ê³¼ì¶”ë¡  ê·¼ë³¸ì ì¸ ë¬¸ì œ : Control Group â‰  Counterfactuals\níŠ¹ì„±ì´ ë‹¤ë¥¸ ë‹¤ë¥¸ ëŒ€ìƒê³¼ ë¹„êµ ì‹œ Selection Biasê°€ ë°œìƒ : &lt;ì¿ ì¦ˆë‹¤ë¥´ì™€ ë§ˆë¦¬ì•„ ì‚¬ë¡€&gt;\n\nê²°êµ­, ì´ ë¬¸ì œê°€ í•´ê²°ì´ ë˜ì–´ì•¼ Treatmentì— ë”°ë¥¸ ì¸ê³¼ì ì¸ íš¨ê³¼ íŒŒì•…ì´ ê°€ëŠ¥\n\n\n\n\nIndividual Treatment Effect (ITE) : ê°œë³„ ìœ ì € i ì— ëŒ€í•´ Treatment ì²˜ì§€ íš¨ê³¼\n\nìœ ì € i ì—ê²ŒëŠ” 2ê°€ì§€ Potential Outcomesì´ ì¡´ì¬\n\n\\(T = 1\\) : ì´ë²¤íŠ¸ ì°¸ì—¬ / \\(T = 0\\) : ì´ë²¤íŠ¸ ë¯¸ì°¸ì—¬\n2ê°€ì§€ Potential Outcomes ì¤‘ì—ì„œ í•˜ë‚˜ë§Œ ì¡´ì¬í•  ìˆ˜ ìˆìŒ\nìœ ì € i ì— ëŒ€í•œ ê°œë³„ ì¸ê³¼íš¨ê³¼ (ITE)\n$ ITE = Y_{0i} - Y_{1i} $\n\n\nAverage Treatment Effect (ATE) : ìœ ì € ê·¸ë£¹ì— ëŒ€í•œ Treatment ì²˜ì§€ íš¨ê³¼\n\nìœ ì € ê°œì¸í™” ê´€ì ì—ì„œëŠ” ITEê°€ ì´ìƒì ì´ê³  ì¤‘ìš”í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì€ ìœ ì € ê·¸ë£¹ë‹¨ìœ„ì˜ ì‹¤í—˜ì´ ì¼ë°˜ì ì´ë©° ê°œê°œì¸ì— ëŒ€í•´ ITEë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ì¡´ì¬\në”°ë¼ì„œ, ìœ ì € ê°œì¸ì˜ ì¸ê³¼íš¨ê³¼ë¥¼ í‰ê· ì„ ë‚´ì–´ ì§‘ë‹¨ ë ˆë²¨ì—ì„œ ì„¤ëª…\në§Œì•½, ì œëŒ€ë¡œ ëœ ì‹¤í—˜ ì„¤ê³„ë¥¼ í•˜ì§€ ì•Šê³  ATEë¥¼ ê³„ì‚°í•œë‹¤ë©´? ì•„ë˜ì™€ ê°™ì€ Selection Biasê°€ ìƒê¹€ &lt;Causal Inference for the brave and true Chapter1. ìˆ˜ì‹ ì°¸ì¡°&gt;\n\n$ E[Y|T=1] - E[Y|T=0] = E[Y_1|T=1] - E[Y_0|T=0] + E[Y_0|T=1] - E[Y_0|T=1] $\n$ E[Y|T=1] - E[Y|T=0] = {ATT} + $\n\n\n\n\n\nì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ ë°”ë¼ë³´ëŠ” 3ê°€ì§€ ê´€ì \n\nPotential Outcomes : ë¬¼ìŒí‘œ ì±„ìš°ê¸°\nStructural Causal Models : DAG\nRegression â†’ ë‹¤ìŒ ì±•í„°ê°€ Regressionì¸ ì´ìœ !\n\nì˜¤ì°¨í•­ ê°€ì •(Gaussian Assumption)ê³¼ ë‚´ìƒì„± (Endogenity) â†’ ë„êµ¬ë³€ìˆ˜ ì°¸ì¡°\n\n\n\n\n\nSelection Bias í•´ê²°í•˜ê¸° ìœ„í•´ ì´ë²ˆ ì±•í„°ì—ì„œëŠ” Random Assignment ë„ì…\n\nì¦‰, ì‹¤í—˜ ëŒ€ìƒë¥¼ ë™ì „ë˜ì§€ê¸°ë¡œ ë‚˜ëˆ ì„œ Treatment ì—¬ë¶€ë¥¼ ê²°ì •\n\n\n\n\nRandom Assignmentì´ ê°€ì¥ ì¢‹ì€ ë°©ë²•ì´ì§€ë§Œ, Research Design í•„ìš”ì„± ì¡´ì¬\n\nëª©ì  :\n\ní•˜ì§€ë§Œ, í•­ìƒ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ RCTë¥¼ í™œìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ë§ìŒ\nìœ„ì™€ ê°™ì€ ê²½ìš°, Counterfactualê³¼ ìµœëŒ€í•œ ë¹„ìŠ·í•œ Control Groupë¥¼ ì‹¤í—˜ ë””ìì¸ì„ í†µí•´ ì°¾ì•„ë‚˜ê°€ì•¼ í•¨ â†”ï¸ Selection Bias ì¤„ì´ê¸°\n\n\n\n\nì‹¤í—˜ ë””ìì¸ ë°©ë²•ë¡  (To be continued)\n\nInstrumental Varibles (2SLS / Regression, Chapter 3)\nRDD (Regression Discontinuity Design, Chapter 4)\nDID (Difference In Difference, Chapter 5)\nSynthetic Control\n\n\n\n\n\n\n\nLLN (Law of Large Numbers) ë‚˜ì˜¤ê²Œ ëœ ë°°ê²½\n\nê²°êµ­ ëª¨ì§‘ë‹¨ì„ ì˜ ëŒ€í‘œí•˜ëŠ” í‘œë³¸(Sample)ì„ ì„ ì •í•˜ê¸° ìœ„í•´, ì¶©ë¶„í•œ ìˆ˜ì˜ í‘œë³¸ì´ í•„ìš”\nê¶ê·¹ì ìœ¼ë¡œ, ì‹¤í—˜êµ°ê³¼ ëŒ€ì¡°êµ°ì€ ë™ì¼í•œ ëª¨ì§‘ë‹¨ì—ì„œ ìƒì„± â†’ ê·¸ë£¹ê°„ì— ë¹„êµ ê°€ëŠ¥í•œ íŠ¹ì„±ì„ ê°€ì ¸ì•¼í•¨ â†’ í•´ë‹¹ ì¡°ê±´ ë‹¬ì„±ì„ ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ í‘œë³¸ì´ í•„ìš”\nê³¼ì—° Sample SizeëŠ” ì–´ë–¤ ìˆ˜ì¤€ì´ ì ì ˆí• ê¹Œ?\n\nStatistical Power(ê²€ì •ë ¥)ê³¼ Sample Size\nProduct Active User ê·œëª¨ë¥¼ ê³ ë ¤\n\n\n\n\n\nHypothesis Testing\n\nëª©ì  : ëª¨ì§‘ë‹¨ (Population Data)ì˜ íŠ¹ì„±ì— ëŒ€í•´ ì„¤ê³„í•œ í†µê³„ì  ê°€ì„¤ (\\(H_0\\) / \\(H_1\\))ì„ ëª¨ì§‘ë‹¨ì˜ ì¶”ì¶œí•œ ìƒ˜í”Œ ë°ì´í„° (Sample Data)ë¥¼ ì´ìš©í•´ ê²€ì¦í•˜ëŠ” ê³¼ì •\nì¸ê³¼ì¶”ë¡ ê³¼ ê°€ì„¤ ê²€ì • : ê²°êµ­, ì¸ê³¼ì ì¸ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ ëª¨ì§‘ë‹¨ì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì´ìš©í•´, ê°œì… (Intervention)ì— ë”°ë¥¸ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ê²€ì¦\n\nê·¸ë˜ì„œ, í›„ë°˜ë¶€ì— ê°€ì„¤ ê²€ì • T-test â†”ï¸ Two Sample T-test (â†”ï¸ Hausman Test)ì´ ë‚˜ì˜¤ê²Œëœ ê²ƒ ê°™ìŒ"
  },
  {
    "objectID": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#ab-í…ŒìŠ¤íŠ¸-open-source",
    "href": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#ab-í…ŒìŠ¤íŠ¸-open-source",
    "title": "Randomised Controlled Trial",
    "section": "",
    "text": "A/B í…ŒìŠ¤íŠ¸ë¥¼ í•  ìˆ˜ ìˆëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ë° ìë£Œ\n\nìì²´ OCE (Online Controlled Experiment) í”Œë«í¼ì´ ìˆë‹¤ë©´ ìµœì ì˜ í™˜ê²½\nì‹¤ì œë¡œ 3rd Party íˆ´ (Amplitude / Braze / Firebase ë“±)ì„ í†µí•´ì„œ A/B Testë¥¼ í•´ë³¼ ìˆ˜ ìˆìŒ\nGrowthBookê³¼ ê°™ì´ ì˜ ì•Œë ¤ì§„ ì˜¤í”ˆ ì†ŒìŠ¤ë¥¼ í†µí•´, í˜„ì—…ì— ì ìš©ì´ ê°€ëŠ¥\nì˜¤í”ˆ ì†ŒìŠ¤ ì •ë¦¬ìë£Œ ë§í¬ : https://posthog.com/blog/best-open-source-ab-testing-tools\n\n\n\n\nì‹¤í—˜ í”Œë«í¼\n\nì§ê´€ì´ ì•„ë‹Œ ì‹¤í—˜ìœ¼ë¡œì˜ ì˜ì‚¬ê²°ì •ì€ ì¡°ì§ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê³¼ì œ\nì°¸ê³  ë„ì„œ : ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì˜ ì‹¤í—˜"
  },
  {
    "objectID": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#q-a",
    "href": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#q-a",
    "title": "Randomised Controlled Trial",
    "section": "",
    "text": "(ì¢…ì–¸) ì‹¤í—˜ì— ì‚¬ìš©í•  ìƒ˜í”Œ ì‚¬ì´ì¦ˆëŠ” ì–´ë–»ê²Œ ê°€ì ¸ê°€ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?\n\n(ì´ì‚­) : í†µê³„í•™ì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” Statistical Powerê³¼ ìƒ˜í”Œ ì‚¬ì´ì¦ˆê°€ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ ìˆì–´, ê²€ì •ë ¥ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒ˜í”Œ ì‚¬ì´ì¦ˆê°€ ì ì ˆí•œì§€ íŒë‹¨í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤,\n(ì†Œí¬) : ì ì • ìƒ˜í”Œ ì‚¬ì´ì¦ˆë¥¼ ê³„ì‚°í•´ì£¼ëŠ” ì‚¬ì´íŠ¸ë¥¼ ì‚¬ìš©í•´ë´¤ì–´ìš”.\n(ì§„ìˆ˜) : ì˜¤íˆë ¤ ë„ˆë¬´ ì ì€ ê·¹ë‹¨ì ì¸ ì¼€ì´ìŠ¤ëŠ” Biasê°€ ë§ê³  ì¼ë°˜ì ìœ¼ë¡œ ì •ìƒì ì¸ ë²”ì£¼ì—ì„œ ë²—ì–´ë‚¬ë‹¤ê³  ìƒê°í•´, ì‹¤í—˜ ëŒ€ìƒìœ¼ë¡œëŠ” ì œì™¸í–ˆë˜ ê¸°ì–µì´ ìˆìŠµë‹ˆë‹¤. ì›ë¡ ì ì¸ ë‹µë³€ì´ì§€ë§Œ í•­ìƒ Productì˜ Active User ìˆ˜ë¥¼ ê³ ë ¤í•´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n\n\n\n\n(ì´ì‚­) ì˜ë£Œ ë³´í—˜ ê°€ì…ìœ¼ë¡œ ê±´ê°• ì¦ì§„ì˜ íš¨ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆëŠ”ë°, ë¯¸êµ­ì˜ ì‚¬ë¡€ì—¬ì„œ ê·¸ëŸ°ê±¸ê¹Œìš”?\n\n(ì •í˜„) : ì˜ë£Œ ë³´í—˜ ì²´ê³„ê°€ ë‹¤ë¥´ì§€ë§Œ í•œêµ­ë„ ë¹„ìŠ·í•˜ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì‹¤í—˜ì„ í•´ë³´ì§€ ì•Šì•„ì„œ ì§ê´€ì ì¸ íŒë‹¨ì— ì£¼ì˜í•´ì•¼í•  ê²ƒ ê°™ì•„ìš”."
  },
  {
    "objectID": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#reference",
    "href": "posts/entertainment_basic_study_1/1. Randomised Controlled Trial.html#reference",
    "title": "Randomised Controlled Trial",
    "section": "",
    "text": "ê³ ìˆ˜ë“¤ì˜ ê³„ëŸ‰ê²½ì œí•™ Chapter 1. ë¬´ì‘ìœ„ ì‹œí–‰ &lt;Joshua D. AngristÂ ,Â Jorn-Steffen Pischke ì €&gt;\nCausal Inference for the brave and true Chapter 1. Introduction to Causality \nKorea Summer Workshop on Causal Inference 2022\nIntroduction to Causal Inference\nìœ  í€´ì¦ˆ ì˜¨ ë” ë¸”ëŸ­ - êµ¬ì¤€ì—½í¸"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html",
    "href": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html",
    "title": "09. Unobserved Confounding Analysis",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì˜ ìµœì€í¬, ê¹€ìƒëˆì…ë‹ˆë‹¤.\nIntroduction to Causal Inference ê°•ì˜ì˜ ì—¬ëŸë²ˆì§¸ ì±•í„°ì´ë©°, í•´ë‹¹ ì±•í„°ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\nê°•ì˜ ì˜ìƒ ë§í¬ : https://youtu.be/IXNMYqUsBBQ\nì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#contents",
    "href": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#contents",
    "title": "09. Unobserved Confounding Analysis",
    "section": "Contents",
    "text": "Contents\n\nÂ Overview\n\nPotential Outcomes, ATE ë¦¬ë§ˆì¸ë“œ\n\nBounds\n\nObservational-Counterfactual Decomposition\nNo-Assumptions Bound\nMonotone Treatment Response (MTR)\nMonotone Treatment Selection (MTS)\nOptimal Treatment Selection (OTS)\n\nSensitivity Analysis\n\nLinear Single Confounder\nTowards More General Settings"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#overview",
    "href": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#overview",
    "title": "09. Unobserved Confounding Analysis",
    "section": "Overview",
    "text": "Overview\n\nPotential outcomes\n\n\nì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ ìš°ë¦¬ê°€ ê°€ì§„ Control groupì„ í™œìš©í•˜ì—¬ ê´€ì¸¡ë˜ì§€ ì•Šì€ Counterfactual(Unobserved confounders)ê³¼ ìµœëŒ€í•œ ê°™ì•„ì§€ê²Œ í•´ì•¼í•¨\nì¸ê³¼ì¶”ë¡ ì˜ Consistency ì›ì¹™ (ë™ì¼ Tì˜ ê²½ìš° ê·¸ì— ëŒ€í•œ ê²°ê³¼ë„ ë™ì¼í•´ì•¼í•¨)Potential Outcomes\n\ní•˜ì–€ ìƒì¥ì˜ ê²½ìš° Consistí•˜ì§€ ì•Šê³  Counterfactualí•¨ (ì´ëŸ° ê²½ìš°ê°€ ë°”ë¡œ unobserved confounding factors)\n\n\n\n2. ATEì— ëŒ€í•´ ë‹¤ì‹œ í•œë²ˆ ì§šì–´ë³´ì.\n\\(\\mathbb{E}[Y(1)-Y(0)]=\\mathbb{E}_W[\\mathbb{E}[Y\\,|\\, T=1,W]-\\mathbb{E}[Y\\,|\\, T=0,W]]\\)\n\nWhat is ATE(Average Treatment Effect)?\n\nì¼ë°˜ì ìœ¼ë¡œ A/B testingì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¶„ì„ ë°©ë²•. ê°œì¸ì˜ ì¸ê³¼íš¨ê³¼ë¥¼ í‰ê· ì„ ë‚´ì–´ ì§‘ë‹¨ ë ˆë²¨ì—ì„œ ì„¤ëª…. ë³´í†µì˜ ê²½ìš° êµë€ë³€ìˆ˜(Confounders) \\(W\\)ê°€ ê´€ì¸¡ë  ê²½ìš° ì²˜ì¹˜ \\(T\\)(treatment)ì™€ ê²°ê³¼ \\(Y\\)ì— ëŒ€í•œ ì¸ê³¼ë¥¼ ì•„ë˜ì™€ ê°™ì€ ATE ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆìŒ\nìœ„ ATEì‹ì´ ì„±ë¦½ ê°€ëŠ¥í•œ ê²½ìš°: Confounderê°€ ëª¨ë“  ê·¸ë£¹ì— ë™ì¼í•˜ê²Œ ì‘ìš©í• ë•Œ ë‘ ê·¸ë£¹ì— Confounding factorsê°€ ë™ì§ˆí•˜ê²Œ ì‘ìš©í•˜ê¸° ìœ„í•´ì„ ? RCTë¥¼ í†µí•´ ê°€ëŠ¥!\n\n\n\n\n\n\ní•˜ì§€ë§Œ ATEì˜ ê²½ìš° Outlierì— ì·¨ì•½í•œ ë‹¨ì  ë˜í•œ ìˆìŒ\n\ne.g.Â íƒ€ê²Ÿêµ°ê³¼ ëŒ€ì¡°êµ°ì˜ ë§¤ì¶œ ë¹„êµ ì§„í–‰ ì‹œ, ë§Œì•½ ëŒ€ì¡°êµ°ì— íƒ€ê²Ÿêµ°ì˜ êµ¬ë§¤ì•¡ì„ í•©ì¹œ ìˆ˜ì¤€ì˜ í•µê³¼ê¸ˆëŸ¬ê°€ í•œ ëª…ì´ë¼ë„ ì¡´ì¬í•œë‹¤ë©´?\n\n\n\n\n\n\nIdentify a point [Identification]\n\nëª¨ë“  ë³€ìˆ˜ê°€ ê´€ì¸¡ê°€ëŠ¥í•˜ë‹¤ê³  ê°•í•œ ê°€ì •ì„ í•œë‹¤ë©´, ìš°ë¦¬ëŠ” í¬ì¸íŠ¸ê°€ ë˜ëŠ” ì§€ì ì„ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n\n3. ë§Œì•½ ê´€ì¸¡ ë¶ˆê°€(Unobserved Confounders)í•œ \\(U\\)ë¥¼ ë°œê²¬í•˜ì˜€ì„ë• ì–´ë–¡í•´ì•¼ í• ê¹Œ?\n\nì‚¬ì‹¤ ê´€ì¸¡ ë¶ˆê°€í•œ \\(U\\)ëŠ” ê±°ì˜ ëª¨ë“  ì—°êµ¬ì—ì„œ ê³ ë ¤í•´ì•¼í•  ì‚¬í•­\n\n\\(\\begin{aligned}\\mathbb{E}[Y(1)-Y(0)] &=\\mathbb{E}_W,_U[\\mathbb{E}[Y\\,|\\, T=1,W,U]-\\mathbb{E}[Y\\,|\\, T=0,W,U]] \\\\ &\\approx \\mathbb{E}_W[\\mathbb{E}[Y\\,|\\, T=1,W]-\\mathbb{E}[Y\\,|\\, T=0,W]]\\end{aligned}\\)\n\nIdentify an interval (Partial identification)\n\nATEë¥¼ í™œìš©í•˜ì—¬ ìµœëŒ€í•œ ê·¼ì‚¬ì¹˜ë¥¼ ì°¾ì•„ë³´ì\ní•œê°€ì§€ ê°€ì •ì„ ë‚´ë¦¬ëŠ” ê²ƒì´ ì•„ë‹Œ ê°€ì •ì˜ ë²”ìœ„ë¥¼ ë„“í˜€ [ê·¼ì‚¬ì¹˜], ì¦‰ ê·¸ ê°„ê²©(Interval)ì„ ì¶”ì •í•´ë³´ì\n\n\n\n\n\n\nğŸ’¡Â â€˜NoÂ UnobservedÂ Confoundingâ€™Â isÂ Unrealistic.\nÂ  Â í˜„ì‹¤ ì„¸ê³„ì—ì„œ ëª¨ë“  ë³€ìˆ˜ë¥¼ ê´€ì¸¡í•œë‹¤ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤.\n\n\n\n\n\n\n\n\n\n\n1\nNo-Assumptions Bound\nê°€ì •ì´ ì—†ë‹¤.\nInterval ë§¤ìš° ê¹€\n\n\n\n\n2\nMonotone Treatment Response\nì²˜ì¹˜(\\(T\\))ëŠ” ì–¸ì œë‚˜ ê²°ê³¼(\\(Y\\))ì— ì˜í–¥ì„ ì¤€ë‹¤.\nÂ \n\n\n3\nMonotone Treatment Selection\nì²˜ì¹˜(\\(T\\))ë¥¼ ë°›ì€ êµ°ì´ ì–¸ì œë‚˜ ì¢‹ì€ Potential Outcomesì„ ë„ì¶œí•œë‹¤.\nÂ \n\n\n4\nOptimal Treatment Selection\nê°œê°œì¸ì€ ì–¸ì œë‚˜ ìµœì ì˜ ì²˜ì¹˜(\\(T\\))ë¥¼ ë°›ëŠ”ë‹¤.\n- Interval ì§§ì•„ì§\n\n\n\nğŸ‘‰ ê°€ì •ì˜ ì •ë„ê°€ ì˜¬ë¼ê°ˆìˆ˜ë¡ í˜„ì‹¤ ì„¸ê³„ì—ì„œ ë°œìƒë˜ëŠ” í˜„ìƒì— ëŒ€í•œ ì„¤ëª…ë ¥ì´ ë–¨ì–´ì§„ë‹¤ëŠ” í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ê°•í•œ ê°€ì • ì•„ë˜ì—ì„œ ë‚˜ì˜¨ ê²°ë¡ ì¼ìˆ˜ë¡ ê·¸ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ë–¨ì–´ì§„ë‹¤ëŠ” ëœ». (â€œThe credibility of inference decreases with the strength the assumptions maintained.â€ Manski)\nğŸ‘‰ Intervalì´ ì§§ì•„ì§ˆìˆ˜ë¡ ê²°ê³¼ì— ëŒ€í•œ ì‹ ë¢°ë„ë„ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ (ê°œê°œì¸ì´ ì–¸ì œë‚˜ ìµœì ì˜ ì²˜ì¹˜ë¥¼ ë°›ì„ í™•ë¥ ì€?)"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#bounds",
    "href": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#bounds",
    "title": "09. Unobserved Confounding Analysis",
    "section": "Bounds",
    "text": "Bounds\n\nÂ ğŸ’¡Â ì§€ê¸ˆë¶€í„°Â ê´€ì¸¡ì´Â ì–´ë ¤ìš´Â ë³€ìˆ˜ë“¤ì˜Â ë²”ìœ„ë¥¼Â ì¢íˆëŠ”Â ë°©ë²•ë¡ ë“¤ì—Â ëŒ€í•´Â ì•Œì•„ë´ë³´ì.\n\n\nObservational-Counterfactual Decomposition\nì´ì¯¤ì—ì„œ ë‹¤ì‹œë³´ëŠ” ATE. \\(\\mathbb{E}[Y(1)-Y(0)]=\\mathbb{E}_W[\\mathbb{E}[Y\\,|\\, T=1,W]-\\mathbb{E}[Y\\,|\\, T=0,W]]\\)\n\nObservational : ê´€ì¸¡ë˜ëŠ” ë¶€ë¶„\nCounterfactual : ê°€ì •ì„ í†µí•´ ë²”ìœ„ë¥¼ ì¢íˆëŠ” ë¶€ë¶„\n\n\n\nğŸ’¡Observational-Counterfactual Decomposition\n\\(\\begin{aligned}\\mathbb{E}[Y(1)-Y(0)] = \\pi\\mathbb{E}[Y|T=1] + (1-\\pi)\\mathbb{E}[Y(1)|T=0] \\\\ -\\,\\pi\\mathbb{E}[Y(0)|T=1] - (1-\\pi)\\mathbb{E}[Y|T=0] \\\\ whereÂ \\;\\;Â \\piÂ \\triangleqÂ P(T=1)\\end{aligned}\\)\n\n\n\n[1] No-Assumptions Bound\n\nê°€ì •: ë²”ìœ„(Bound)ì— ëŒ€í•œ ê°€ì •ì´ ì—†ì„ ë•Œ Interval Length êµ¬í•´ë³´ê¸°\n\nBounded Potential Outcomes\n\\(Y(0)\\)ê³¼ \\(Y(1)\\)ì´ \\(0\\)ê³¼ \\(1\\) ì‚¬ì´ì— ìˆë‹¤ê³  ê°€ì •í–ˆì„ ë•Œ \\(\\mathbb{E}Y(1)-Y(0)\\)ì˜ ê²½ìš° ìŒì˜ 0ê³¼ 1, ì–‘ì˜ 0ê³¼ 1 ì‚¬ì´ì— ìˆë‹¤.\n\\(-1 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 1\\)\nì´ë¥¼ ì¼ë°˜í™”í•œë‹¤ë©´, \\(\\forall t, a \\le Y(t) \\le b\\)\n\\[ a-b \\le \\mathbb{E}[Y(1)-Y(0)] \\le b-a \\]\n\nTrival length limit : \\(2(b-a)\\)\n\nNo-Assumptions Bound\n\nobservational-counterfactual decomposition ì™€ ê° ë°©ë²•ë¡ ì˜ ê°€ì •ì„ í™œìš©í•˜ì—¬ lower boundì™€ upper boundë¥¼ ì¶”ì •í•˜ëŠ” ê³¼ì •\n\nupper bound\n\n\\(\\mathbb{E}[Y(1)-Y(0)] \\le \\pi\\mathbb{E}[Y|T=1]+(1-\\pi)b -\\pi a-(1-\\pi)\\mathbb{E}[Y|T=0]\\)\n\nlower bound\n\n\\(\\mathbb{E}[Y(1)-Y(0)] \\ge \\pi\\mathbb{E}[Y|T=1]+(1-\\pi)a -\\pi b-(1-\\pi)\\mathbb{E}[Y|T=0]\\)\\[ Interval\\; Length = b-a \\]\n\n\nìµœì´ˆì˜ ì‹œì‘ì—ì„œ ë²”ìœ„ë¥¼ ë°˜ìœ¼ë¡œ ì¤„ì´ê²Œ ë˜ëŠ” ê²ƒ. \\(2(b-a) \\longrightarrow b-a\\)\nì–¸ì œë‚˜ 0ì„ í¬í•¨í•œë‹¤.\n\n\nğŸ’¡ (ì•ìœ¼ë¡œ ê³„ì† ë‚˜ì˜¬) ì˜ˆì‹œ\n(1) Potential outcomes ê°€ \\(0(a)\\) ì™€ \\(1(b)\\) ì‚¬ì´ì— ìˆìŒ\n(2) \\(\\pi = 0.3\\) \\(\\mathbb{E}[Y|T=1]=0.9\\) \\(\\mathbb{E}[Y|T=0]=0.2\\)\n\\[ -0.17 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.83 \\]\n\n\n\n[2] Monotone Treatment Response (MTR)\n\nê°€ì •: ì²˜ì¹˜(T)ëŠ” ì–¸ì œë‚˜ ê²°ê³¼(Y)ì— ì˜í–¥ì„ ì¤€ë‹¤ëŠ” ê°€ì • (Nonnegative, Nonpositive)\n\n\n\n\n\n\n\n\nNonnegative MTR\nNonpositive MTR\n\n\n\n\n\\(\\forall i\\; Y_i(1) \\ge Y_i(0)\\)\n\\(\\forall i\\; Y_i(1) \\le Y_i(0)\\)\n\n\n\\(0Â \\leÂ \\mathbb{E}[Y(1)-Y(0)]Â \\leÂ 0.83\\)\n\\(-0.17 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0\\)\n\n\n\n\nNonnegative MTR (lower bound ë¬´ì‹œ)\n\nì²˜ì¹˜ëŠ” ì–¸ì œë‚˜ ê²°ê³¼ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ì¤€ë‹¤.\n\n\n\\(\\forall i\\; Y_i(1) \\ge Y_i(0)\\)\n\nITE (Individual Treatment Effect) \\(a - b \\ge 0\\)\nATE (Average Treatment Effect) \\(\\mathbb{E}[Y(1)-Y(0)] \\ge 0\\)\n\n\nNonpositive MTR (upper bound ë¬´ì‹œ)\n\nì²˜ì¹˜ëŠ” ì–¸ì œë‚˜ ê²°ê³¼ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤€ë‹¤.Â \n\n\n\\(\\forall i\\; Y_i(1) \\le Y_i(0)\\)\n\nITE (Individual Treatment Effect) \\(a - b \\le 0\\)\nATE (Average Treatment Effect) \\(\\mathbb{E}[Y(1)-Y(0)] \\le 0\\)\n\n\n\n[3]Monotone Treatment Selection (MTS)\n\nê°€ì •: ì²˜ì¹˜(\\(T\\))ë¥¼ ë°›ì€ íƒ€ê²Ÿêµ°(Target)ì´ ëŒ€ì¡°êµ°(Control)ë³´ë‹¤ ì–¸ì œë‚˜ ì¢‹ì€ Potential Outcomes ë„ì¶œí•œë‹¤. (MTS Upper Bound)\n\n\n\\(\\mathbb{E}[Y(1)|T=1] \\ge \\mathbb{E}[Y(1)|T=0]\\)\n\\(\\mathbb{E}[Y(0)|T=1] \\ge \\mathbb{E}[Y(0)|T=0]\\)\n\n\\[ \\mathbb{E}[Y(1)-Y(0)] \\le \\mathbb{E}[Y|T=1] - \\mathbb{E}[Y|T=0] \\]\n\nğŸ’¡ ê·¸ëŸ¬ë©´ ì´ì œ MTS Upper Boundì— nonnegative MTRì„ í•©ì³ë³´ì.\nì´ë¥¼ í†µí•´ ìš°ë¦¬ëŠ” ë” ì¢ì€ Interval lengthë¥¼ ê²Ÿí•  ìˆ˜ ìˆë‹¤!\n\n\n\n\nNo Assumptions\n\\(-0.17 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.83\\)\n\n\n\n\nMTS Upper Bound\n\\(-0.17 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.7\\)\n\n\nnonnegative MTR\n\\(0 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.83\\)\n\n\ndo combine. (MTS + MTR)\n\\(0 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.7\\)\n\n\n\n\n\n[4]Optimal Treatment Selection (OTS)\n\nê°€ì •: ê°œê°œì¸ì€ ì–¸ì œë‚˜ ê·¸ë“¤ì—ê²Œ ìµœì ì˜ ì²˜ì¹˜(Optimal Treatment)ë¥¼ ë°›ëŠ”ë‹¤.\n\n\nğŸ’¡ OTS Assumption\n(1) Treatment Group (íƒ€ê²Ÿêµ°) \\(T_i = 1 \\Longrightarrow Y_i(1) \\ge Y_i(0)\\)\n(2) Nontreatment Group (ëŒ€ì¡°êµ°) \\(T_i = 0 \\Longrightarrow Y_i(0) &gt; Y_i(1)\\)Â \n\n[4-1] ë°©ë²•ë¡ 1\nobservational-counterfactual decomposition\n\nOTS Upper Bound 1 : \\(\\mathbb{E}[Y(1)|T=0] \\le \\mathbb{E}[Y|T=0]\\)\nOTS Lower Bound 2 : \\(-\\mathbb{E}[Y(0)|T=1] \\ge -\\mathbb{E}[Y|T=1]\\)\n\n\\[ Interval\\; Length = \\pi\\mathbb{E}[Y|T=1]+(1-\\pi)\\mathbb{E}[Y|T=0]-a \\]\n\n\n\nNo Assumptions\n\\(-0.17 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.83\\)\n\n\n\n\nOTS Bound 1\n\\(-0.14 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.27\\)\n\n\n\nì—¬ì „íˆ 0ì„ í¬í•¨í•˜ê³  ìˆëŠ”ë°â€¦..\n[4-2] ë°©ë²•ë¡ 2 : Bound the identifies the sign!\nobservational-counterfactual decomposition\n\nOTS Upper Bound 2 : \\(\\mathbb{E}[Y(1)|T=0] \\le \\mathbb{E}[Y|T=1]\\)\nOTS Lower Bound 2 : ì•Œì•„ì„œ í•´ë³´ë˜ìš”â€¦\n\n\n\n\nNo Assumptions\n\\(-0.17 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.83\\)\n\n\n\n\nOTS Bound 1\n\\(-0.14 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.27\\)\n\n\nOTS Bound 2\n\\(0.07 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.76\\)\n\n\ndo combine\n\\(0.07 \\le \\mathbb{E}[Y(1)-Y(0)] \\le 0.27\\)"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#unobserved-confoundingì´-ê³¼ì—°-ê´€ì¸¡-ì—°êµ¬ì—ë§Œ-ì ìš©ë˜ëŠ”-ì´ìŠˆì¼ê¹Œ",
    "href": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#unobserved-confoundingì´-ê³¼ì—°-ê´€ì¸¡-ì—°êµ¬ì—ë§Œ-ì ìš©ë˜ëŠ”-ì´ìŠˆì¼ê¹Œ",
    "title": "09. Unobserved Confounding Analysis",
    "section": "Unobserved Confoundingì´ ê³¼ì—° ê´€ì¸¡ ì—°êµ¬ì—ë§Œ ì ìš©ë˜ëŠ” ì´ìŠˆì¼ê¹Œ?",
    "text": "Unobserved Confoundingì´ ê³¼ì—° ê´€ì¸¡ ì—°êµ¬ì—ë§Œ ì ìš©ë˜ëŠ” ì´ìŠˆì¼ê¹Œ?\n\nğŸ’¡ ì‹¤í—˜ ì—°êµ¬(Experimental Study)ì—ì„œ ë°œìƒí•˜ëŠ” Unobserved Confounding factors\n\n\nê´€ì¸¡ë˜ì§€ ì•ŠëŠ” êµë€ ë³€ìˆ˜ê°€ RCT ê¸°ë°˜ì˜ ì‹¤í—˜ì—ì„œëŠ” ì „í˜€ ì´ìŠˆê°€ ë˜ì§€ ì•Šì„ê¹Œ?\n\nRCT (Randomised Controlled Trial) ëœë¤í™” ì‹¤í—˜ (RCT ìˆ˜í–‰ ëª©ì : ê·¸ë£¹ì˜ ë™ì§ˆì„±)\nìš°ë¦¬ê°€ ì›í•˜ëŠ” ì´ìƒì ì¸ RCT ê¸°ë°˜ A/B testëŠ”â€¦\níƒ€ê²Ÿêµ°ê³¼ ëŒ€ì¡°êµ°ì„ ëœë¤í•˜ê²Œ ë‚˜ëˆ„ì–´ ë™ì§ˆ ê·¸ë£¹(homogeneous group)ìœ¼ë¡œ ë¶„í• í•˜ê³ , íƒ€ê²Ÿêµ°ì—ë§Œ ì²˜ì¹˜(Treatment)ë¥¼ ì§„í–‰í•˜ì—¬ ë‚˜ì˜¨ ê°’ì—ì„œ ëŒ€ì¡°êµ°ì˜ ê°’ì„ ì°¨ê°í•´ì¤€ ê²°ê³¼ëŠ” ì²˜ì¹˜ë¡œ ì¸í•œ ê²°ê³¼ë‹¤! \\(T \\longrightarrow Y\\) (ì²˜ì¹˜ì™€ ê²°ê³¼ ì‚¬ì´ì—ëŠ” ì¸ê³¼ê´€ê³„ê°€ ì¡´ì¬í•œë‹¤.)\n\n\n\n\n\n\n(íŠ¹íˆ ë§ˆì¼€íŒ…ì—ì„œ)ì¸ê³¼ì¶”ë¡ ì„ ì˜ ëª¨ë¥´ëŠ” ì˜ì‚¬ê²°ì •ìì˜ í”í•œ ë°˜ë¡ . ì›ë˜ [ìƒí’ˆ/ì´ë²¤íŠ¸/ê¸°ëŠ¥ê°œì„ /ì—…ë°ì´íŠ¸]ê°€ ì´ì „ë³´ë‹¤ ê³ ê°ì—ê²Œ ë§¤ë ¥ì ì´ì–´ì„œ ê·¸ëŸ°ê±° ì•„ëƒ?\ní”í•œ ë¶„ì„ê°€ì˜ ì„¤ëª…: íƒ€ê²Ÿêµ°ê³¼ ëŒ€ì¡°êµ° ëª¨ë‘ì—ê²Œ ë™ì§ˆí•˜ê²Œ ì ìš©ë˜ëŠ” ì™¸ë¶€ë³€ìˆ˜ì„.\n\nğŸ¤” ê³¼ì—° ì§„ì§œ ê·¸ëŸ´ê¹Œ? ì˜ì™¸ë¡œ ë‚ ì¹´ë¡œìš´ ì§€ì ì´ì—ˆì„ ìˆ˜ë„\n\n\n\n\n\n\në…¼ë¬¸ì†Œê°œ. (Microsoft) Common Metric Interpretation Pitfall in A/B test ì›ë¬¸ì€ ì—¬ê¸°\n\n\nğŸ’¡ A/B testë¥¼ ì§„í–‰í•  ë•Œ ë°˜ë“œì‹œ ì•„ë˜ 4ê°€ì§€ë¥¼ í™•ì¸í•´ì•¼ í•¨\n(1) Data Quality : í™œìš©ëœ ë°ì´í„°ê°€ ì‹ ë¢°í• ë§Œí•œê°€? e.g.Â íƒ€ê²Ÿêµ°ê³¼ ëŒ€ì¡°êµ°ì˜ ë™ì¼í•œ ëª¨ì§‘ë‹¨ì—ì„œ ëœë¤ ìƒ˜í”Œë§ë˜ì—ˆëŠ”ì§€.\n(2) OverallEvaluation Criteria : ì²˜ì¹˜ê°€ ì„±ê³µì ì´ì—ˆëŠ”ì§€, ì„±ê³µí–ˆë‹¤ë©´ ì–´ëŠ ì •ë„ì˜ íš¨ê³¼ì˜€ëŠ”ì§€, ê·¸ íš¨ê³¼ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œì§€\n(3) Guardrail : ì‹¤í—˜ì— ì˜í–¥ì„ ì£¼ëŠ” êµë€ì€ ì–´ëŠ ì •ë„ì˜€ëŠ”ê°€ (ì „ì‚¬ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ìœ¼ë©´ ì•ˆë˜ëŠ” ì§€í‘œ)\n(4) Local Feature and Diagnostic : ìœ ì €ì˜ ì•¡ì…˜ì„ ì„¸ë°€í•œ ìˆ˜ì¤€ì—ì„œ ë¶„ì„í•˜ì—¬ ì²˜ì¹˜ì˜ ì„±ê³µ ì—¬ë¶€ì™€ êµë€ ë³€ìˆ˜ì˜ ì˜í–¥ ì •ë„ë¥¼ íŒŒì•…Â \n\n\n\nì‚¬ë¡€1. Counfounding factor - Metric Sample Ratio Mismatch(e.g.)\n\në§í¬ ì´ë™ ë°©ë²•ì— ë”°ë¥¸ ìœ ì € ê²½í—˜ ìµœì í™”\në§í¬ë¥¼ ìƒˆë¡œìš´ íƒ­ì—ì„œ ë„ìš°ëŠ” ê²ƒì´ í™ˆí˜ì´ì§€ ë¡œë“œ íƒ€ì„ì„ ì¦ê°€ì‹œì¼°ë‹¤ (?)\n\n\n\n\n\n\n\n\n\n\n\nÂ \nê°€ì„¤\nê²°ê³¼ (í˜ì´ì§€ ë¡œë“œ ìˆ˜)\n\n\n\n\níƒ€ê²Ÿêµ°\nMSN í™ˆí˜ì´ì§€ì—ì„œ í´ë¦­ëœ ì–´ëŠ ë§í¬ë“ , ìƒˆë¡œìš´ íƒ­ìœ¼ë¡œ í˜ì´ì§€ê°€ ëœ¬ë‹¤.\n~8.4M\n\n\nëŒ€ì¡°êµ°\nMSN í™ˆí˜ì´ì§€ì—ì„œ í´ë¦­ëœ ì–´ëŠ ë§í¬ë“ , ì˜¤í”ˆë˜ì–´ ìˆëŠ” íƒ­ì—ì„œ ë§í¬ë¡œ ì´ë™ëœë‹¤.\n~9.2M\n\n\n\nê²°ë¡ : ì‹¤í—˜êµ°(íƒ€ê²Ÿêµ°)ì—ì„œ í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì´ ì•½ 8.32% ì¦ê°€í–ˆë‹¤. -&gt; ê¸°ëŒ€ ì´ìƒì˜ í° ì¦ê°€\nğŸŒŸÂ Confounding factor\n\ní™ˆí˜ì´ì§€ íƒ­ì´ ë¯¸ë¦¬ ì¼œì ¸ ìˆì–´ reloadí•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ìƒí™©\nëŒ€ì¡°êµ°ì˜ ê²½ìš° ë¸Œë¼ìš°ì €ì— ë‚¨ì€ í™ˆí˜ì´ì§€ ìºì‹œ\n\nğŸ’¡ ì´ëŸ° ìš”ì¸ë“¤ì„ ì–´ë–»ê²Œ ë°œê²¬ í•´ì•¼í• ê¹Œ?\n\në¹„ìœ¨ ì§€í‘œë¥¼ ë¶„í•´í•˜ì—¬ ì–´ëŠ ë¶€ë¶„ì—ì„œ ì°¨ì´ê°€ ë°œìƒí•˜ëŠ”ì§€ íŒŒì•…\në¹„ìœ¨ mismatchì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ” ì‹¤í—˜êµ°, ëŒ€ì¡°êµ° ê°„ì˜ ë™ë“±í•˜ê²Œ ë¹„êµí•  ìˆ˜ ìˆëŠ” subsetì„ ì°¾ì•„ì„œ ì‹ ë¢° ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë¶„ì„\n\n\nì‚¬ë¡€2. Counfounding factor - Novelty and Primacy Effects\n\n(e.g.) CRMì¸¡ë©´ì—ì„œì˜ ê° ì„¸ê·¸ë¨¼íŠ¸ ë³„ ì²˜ì¹˜ì— ë”°ë¥¸ íš¨ê³¼ ì¸¡ì •\nê·¸ë£¹ Aì˜ ì²˜ì¹˜ íš¨ê³¼ê°€ ê·¸ë£¹ Bì˜ ì²˜ì¹˜ íš¨ê³¼ë³´ë‹¤ ì¢‹ì•˜ë‹¤(?)\n\n\n\n\n\n\n\n\n\n\nÂ \nê·¸ë£¹ A\nê·¸ë£¹ B\n\n\n\n\nì²˜ì¹˜ íš¨ê³¼\n(ì²˜ì¹˜êµ°ì´ ëŒ€ì¡°êµ°ì— ë¹„í•´) + 126% ìƒìŠ¹\n(ì²˜ì¹˜êµ°ì´ ëŒ€ì¡°êµ°ì— ë¹„í•´) + 108% ìƒìŠ¹\n\n\n\nì—¬ê¸°ì„œ ë‚¼ ìˆ˜ ìˆëŠ” ê²°ë¡ : ê·¸ë£¹ Aê°€ CRM ì¸¡ë©´ì—ì„œ ê·¸ë£¹ Bë³´ë‹¤ ë” ì¢‹ì€ ì„¸ê·¸ë¨¼íŠ¸ë‹¤. ê·¸ë£¹ Aë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì „ëµì„ ìˆ˜ë¦½í•´ë³´ì(?)\n\n\nğŸŒŸÂ Confounding factor - ê·¸ë£¹ Aì—ì„œì˜ Novelty effect\n\nNovelty effect : ê¸ì •ì ì¸ íš¨ê³¼ê°€ ë‹¨ê¸°ê°„ í˜¹ì€ ì´ˆë°˜ì—ë§Œ ë°œìƒí•˜ê³ , ê¸°ê°„ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ê·¸ íš¨ê³¼ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šì„ ë•Œ (ì´ˆë‘íš¨ê³¼)\nPrimacy effect : ì´ˆë°˜ì—ëŠ” ìœ ì €ì˜ ë°˜ì‘ì´ ê·¹ì ì´ì§€ ì•Šì§€ë§Œ, ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ user learningì´ ë°œìƒí•˜ì—¬ ì²˜ì¹˜ ìµœì í™”ê°€ ì˜ ì´ë£¨ì–´ì§ˆë•Œ\n\n\nğŸ’¡ ì´ëŸ° ìš”ì¸ë“¤ì„ ì–´ë–»ê²Œ ë°œê²¬ í•´ì•¼í• ê¹Œ?\n(1) ì²˜ì¹˜ íš¨ê³¼ë¥¼ ë‹¤ì–‘í•œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ìª¼ê°œì–´ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³¸ë‹¤.\n(2) ì²˜ì¹˜ì˜ íš¨ê³¼ë¥¼ ì¼ë³„ë¡œ ìª¼ê°œì–´ í™•ì¸í•´ë³¸ë‹¤.\n(3) ì‹¤í—˜ì˜ ê¸°ê°„ì„ ëŠ˜ë ¤ ì¥ê¸°ì ìœ¼ë¡œ í™•ì¸í•´ë³¸ë‹¤.\n\n\nê²°ë¡ \n\nìš°ë¦¬ê°€ ì§„í–‰í•œ í…ŒìŠ¤íŠ¸ ì„¤ê³„ê°€ ì˜ë˜ì–´ìˆë‹¤ëŠ” êµ³ì€ ë¯¿ìŒì—ì„œ ë²—ì–´ë‚˜ ê²°ê³¼ë¥¼ ì£¼ì–´ì§„ ë¦¬ì†ŒìŠ¤ ì•ˆì—ì„œ ìµœëŒ€í•œ ë¹„íŒì ìœ¼ë¡œ í•´ì„í•´ì•¼í•œë‹¤.\nA/B testë¥¼ ì§„í–‰í•  ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” êµë€ ë³€ìˆ˜ì˜ ë²”ìœ„ë¥¼ ì •í•œë‹¤ í•˜ë”ë¼ë„ ì„¸ë¶€ì ì¸ ë³€ìˆ˜ë“¤ì„ ëª¨ë‘ íŒŒì•…í•˜ê¸°ë€ í˜ë“¤ë‹¤. í•˜ì§€ë§Œ ìœ„ ë…¼ë¬¸ì—ì„œ ì†Œê°œí•œëŒ€ë¡œ ì—¬ëŸ¬ Metricì— ëŒ€í•´ ì •ì˜ë¥¼ ì§„í–‰í•˜ê³  ê²½í—˜ì ìœ¼ë¡œ ìŒ“ì¸ ë°œìƒ ê°€ëŠ¥í•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ê´€ë¦¬ ì‹œìŠ¤í…œì„ ì˜ êµ¬ì¶•í•œë‹¤ë©´ ë” ë°œì „ëœ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\nA/B test ë¶„ì„ì„ ì§„í–‰í•˜ëŠ” ë¶„ì„ê°€ì˜ ë„ë©”ì¸ì— ëŒ€í•œ ì´í•´ë„, ì‚¬ì „ ì§€ì‹ì´ ì¤‘ìš”í•  ìˆ˜ ìˆë‹¤. ë„ë©”ì¸ ì§€ì‹ ì •ë„ì— ë”°ë¼ ì¶”ì¸¡í•  ìˆ˜ ìˆëŠ” êµë€ ë³€ìˆ˜ì˜ ë²”ìœ„ë„ ë„“ì–´ì§ˆ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#sensitivity-analysis",
    "href": "posts/Introduction_to_causal_inference_Unobserved_confounding_Analysis/Unobserved_Confounding_Analysis.html#sensitivity-analysis",
    "title": "09. Unobserved Confounding Analysis",
    "section": "Sensitivity analysis",
    "text": "Sensitivity analysis\nì¼ë°˜ì ìœ¼ë¡œÂ ê´€ì°°Â ë°ì´í„°ë¡œÂ ì¸ê³¼ì¶”ë¡ Â ë¶„ì„ì„Â í• Â ë•Œì—ëŠ”Â ê´€ì¸¡ë˜ì§€Â ì•Šì€Â êµë€Â ìš”ì¸ì´Â ì—†ë‹¤ëŠ”Â ê°€ì •Â í•˜ì—ì„œÂ ë¶„ì„ì„Â ì§„í–‰í•©ë‹ˆë‹¤.Â ì¼ë°˜ì ìœ¼ë¡œÂ ì´ëŸ¬í•œÂ ê°€ì •ì€Â ë§Œì¡±ë Â ìˆ˜Â ì—†ìŠµë‹ˆë‹¤.Â ë”°ë¼ì„œÂ ê°€ì •ì´Â ìœ„ë°˜ë˜ì—ˆì„Â ë•Œ,Â ì¸ê³¼Â ê´€ê³„ì—Â ë¯¸ì¹˜ëŠ”Â ì˜í–¥ì„Â ì •ëŸ‰ì ìœ¼ë¡œÂ í™•ì¸í•˜ëŠ”Â ì ˆì°¨,Â ì¦‰Â ê°•ê±´ì„±ì„Â í™•ì¸í•˜ëŠ”Â ì ˆì°¨ê°€Â í•„ìš”í•©ë‹ˆë‹¤. ì´ê²ƒì„Â ì¸¡ì •í•˜ê¸°Â ìœ„í•´Â ë‹¤ìŒê³¼Â ê°™ì€Â ì§ˆë¬¸ì„Â í•´ë³¼Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.\nê´€ì°° ë°ì´í„°ë¡œ ì¶”ì •í•œ ì¸ê³¼íš¨ê³¼ë¥¼ \\(0\\)ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ì„œ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì´ \\(T\\)ì™€ \\(Y\\)ì— ì–¼ë§ˆë‚˜ ë§ì€ ì˜í–¥ì„ ë¼ì³ì•¼ í•˜ëŠ”ê°€? ë¯¼ê°ë„ë¶„ì„ì€ ì´ëŸ¬í•œ ì§ˆë¬¸ì— ë‹µì„ í•˜ê¸° ìœ„í•œ ì •ëŸ‰ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.Â \n\nSensitivity Basiscs in Linear Setting\në¨¼ì €Â ê°€ì¥Â ê°„ë‹¨í•œÂ ì„¸íŒ…ìœ¼ë¡œÂ linearÂ settingì„Â ê³ ë ¤í•´ë³´ê² ìŠµë‹ˆë‹¤.Â ì˜¤ë¥¸ìª½Â ê·¸ë¦¼ì„Â ë³´ë©´Â \\(W\\)ëŠ”Â ê´€ì°°ëœÂ êµë€ìš”ì¸ì´ê³ ,Â \\(U\\)ëŠ”Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸ì„Â ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.Â \\(U\\)ì˜Â íš¨ê³¼ë¥¼Â ë¬´ì‹œí–ˆì„Â ë•Œ,Â ë‚˜íƒ€ë‚˜ëŠ”Â í¸í–¥ì€Â ì–´ëŠÂ ì •ë„ì¼ê¹Œìš”?\n\n\n\nlinearÂ settingì—ì„œÂ \\(T\\)ì™€Â \\(Y\\)ëŠ”Â ë‹¤ìŒê³¼Â ê°™ì´Â í‘œí˜„í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned} &T := \\alpha_w W + \\alpha_u U \\\\ &Y := \\beta_w W + \\beta_u U + \\delta T \\\\ \\end{aligned}\\]\në¨¼ì €Â \\(T,Â \\,Â U\\)ê°€Â ì£¼ì–´ì¡Œì„Â ë•Œë¥¼Â ê°€ì •í•˜ë©´,Â ATEëŠ”Â \\(\\delta\\)ë¡œÂ ê³„ì‚°ë©ë‹ˆë‹¤.\n\\[E[Y(1) - Y(0)] = E_{W, U}[E[Y|T=1,W,U] - E[Y|T=0, W, U]] = \\delta\\]\ní•˜ì§€ë§ŒÂ \\(U\\)ëŠ”Â ê´€ì°°í• Â ìˆ˜Â ì—†ëŠ”Â êµë€ìš”ì¸ì´ë¯€ë¡œ,Â ì‹¤ì œë¡œëŠ”Â \\(W\\)ë§ŒÂ ì¡°ì •í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â ë”°ë¼ì„œÂ \\(U\\)ì˜Â ì˜í–¥ìœ¼ë¡œÂ confoundingÂ biasê°€Â \\(\\frac{\\beta_u}{\\alpha_u}\\)Â ë§Œí¼Â ë°œìƒí•©ë‹ˆë‹¤.\n\n\\[ E_W[E[Y|T=1, W]-E[Y|T=0, W]] - E_{W,U}[E[Y|T=1, W, U] - E[Y|T=0, W, U]] = \\frac{\\beta_u}{\\alpha_u} \\]\n\n\\(proof\\)\n\\[ \\begin{aligned} E_W[E[Y|T=t, W]] &= E_W[E[\\beta_w W + \\beta_u U + \\delta T|T=t, W]] \\\\ &= E_W[\\beta_w W + \\beta_u E[U|T=t, W] + \\delta t] \\\\ \\newline &=E_w \\left[\\beta_w W + \\beta_u \\frac{t-\\alpha_w W}{\\alpha_u} + \\delta t\\right] \\\\ &=E_w \\left[\\beta_w W + \\frac{\\beta_u}{\\alpha_u}t - \\frac{\\beta_u \\alpha_w}{\\alpha_u}W + \\delta t\\right] \\\\ &=\\beta_w E[W] + \\frac{\\beta_u}{\\alpha_u}t - \\frac{\\beta_u \\alpha_w}{\\alpha_u}E[W] + \\delta t \\\\ &=\\left(\\delta + \\frac{\\beta_u}{\\alpha_u}\\right)t + \\left(\\beta_w - \\frac{\\beta_u \\alpha_w}{\\alpha_u}\\right)E[W] \\end{aligned} \\]\n\\[ \\begin{aligned} &E_W[E[Y|T=1, W]-E[Y|T=0, W]] \\\\ &= \\left(\\delta + \\frac{\\beta_u}{\\alpha_u}\\right)(1) + \\left(\\beta_w - \\frac{\\beta_u \\alpha_w}{\\alpha_u}\\right)E[W] - \\left[\\left(\\delta + \\frac{\\beta_u}{\\alpha_u}\\right)(0) + \\left(\\beta_w - \\frac{\\beta_u \\alpha_w}{\\alpha_u}\\right)E[W]\\right] \\\\ &= \\delta + \\frac{\\beta_u}{\\alpha_u} \\end{aligned}\\]\n\\[ \\begin{aligned} Bias &= E_W[E[Y|T=1, W]-E[Y|T=0, W]] \\\\ &- E_{W,U}[E[Y|T=1, W, U] - E[Y|T=0, W, U]] \\\\ &= \\delta + \\frac{\\beta_u}{\\alpha_u} - \\delta \\\\ &= \\frac{\\beta_u}{\\alpha_u} \\end{aligned} \\]\n\n\nSensitivity Contour Plots\ntrueÂ ATEì¸Â \\(\\delta\\)ëŠ”Â ì•„ë˜ì™€Â ê°™ì´Â ë‹¤ì‹œÂ ì •ë¦¬í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.\n\\[ E_W[E[Y|T=1, W]-E[Y|T=0, W]] = \\delta + \\frac{\\beta_u}{\\alpha_u} \\]\n\\[\\begin{aligned} \\delta = E_W[E[Y|T=1, W]-E[Y|T=0, W]] - \\frac{\\beta_u}{\\alpha_u} \\end{aligned}\\]\nì‹ì„Â í•œë²ˆÂ ë”Â í•´ì„í•´ë³´ë©´Â ë‹¤ìŒê³¼Â ê°™ìŠµë‹ˆë‹¤.\n\n\\(U\\)ì— ì˜í•´ ìƒê¸°ëŠ” biasì¸ \\(\\frac{\\beta_u}{\\alpha_u}\\)ì´ í¬ë‹¤ë©´ \\(E_W[E[Y|T=1, W]-E[Y|T=0, W]]\\)ëŠ” ìƒì‡„ë˜ê³ , \\(\\delta\\)ëŠ” \\(0\\)ì— ê°€ê¹Œì›Œì§\n\\(U\\)ì— ì˜í•´ ìƒê¸°ëŠ” biasì¸ \\(\\frac{\\beta_u}{\\alpha_u}\\)ì´ ì‘ë‹¤ë©´ \\(E_W[E[Y|T=1, W]-E[Y|T=0, W]]\\)ëŠ” í¬ê²Œ ë³€í™”í•˜ì§€ ì•Šê³ , \\(\\delta\\)ëŠ” \\(E_W[E[Y|T=1, W]-E[Y|T=0, W]]\\)ê³¼ í° ì°¨ì´ê°€ ì—†ìŒ\n\nì´ì—Â ëŒ€í•´ì„œÂ ê·¸ë˜í”„ë¡œÂ ë‚˜íƒ€ë‚´ë³´ë©´Â ë‹¤ìŒê³¼Â ê°™ìŠµë‹ˆë‹¤.\n\nê·¸ë¦¼ì€Â (\\(\\frac{1}{\\alpha_u}\\),Â \\(\\beta_u\\))ì—Â ë”°ë¥¸Â \\(\\delta\\)ì˜Â ë³€í™”Â ê·¸ë˜í”„ì…ë‹ˆë‹¤.Â ë¨¼ì €Â greenÂ curveÂ ê°’ì„Â í•´ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.Â \\(E_W[E[Y|T=1,Â W]-E[Y|T=0,Â W]]=25\\)ë¡œÂ ê³ ì •í–ˆì„Â ë•Œ,Â \\(\\frac{1}{\\alpha_u}Â =Â 1\\)ì´ê³ ,Â \\(\\beta_uÂ =Â 25\\)ë¼ë©´Â \\(\\deltaÂ =Â 0\\)ì´Â ë©ë‹ˆë‹¤. ë”°ë¼ì„œÂ greenÂ curveì¼Â ê²½ìš°Â (\\(\\frac{1}{\\alpha_u}\\),Â \\(\\beta_u\\))ì˜Â ë³€í™”ì—Â ë”°ë¥¸Â \\(\\deltaÂ =Â 0\\)Â ì¼Â ë•Œì—Â í•´ë‹¹í•˜ëŠ”Â ê³¡ì„ ì…ë‹ˆë‹¤.Â \\(\\deltaÂ =Â 0\\)ì´ë¼ëŠ”Â ì˜ë¯¸ëŠ”Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€Â ìš”ì¸Â \\(U\\)ì—Â ì˜í•´Â \\(E_W[E[Y|T=1,Â W]-E[Y|T=0,Â W]]\\)ì„Â ì „ë¶€Â ì„¤ëª…í•œë‹¤ëŠ”Â ì˜ë¯¸ì™€Â ê°™ìœ¼ë©°,Â \\(U\\)ëŠ”Â ê°•ë ¥í•œÂ êµë€ìš”ì¸ì´ë¼ê³ Â ë³¼Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œÂ greenÂ curveì—Â ê°€ê¹Œì›Œì§€ê±°ë‚˜Â í˜¹ì€Â greenÂ curveë¥¼Â ë„˜ì–´ì„¤Â ê²½ìš°Â êµë€ìš”ì¸ì—Â ì˜í•´Â ì˜í–¥ì„Â ë§ì´Â ë°›ìœ¼ë¯€ë¡œ,Â ê°•ê±´ì„±ì´Â ë–¨ì–´ì§„ë‹¤ê³ Â ë³¼Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤. ì •ë¦¬í•˜ë©´,Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸ì´Â treatmentì™€Â outcomeì—Â ëŒ€í•´Â ë¯¸ì¹˜ëŠ”Â ì˜í–¥ì˜Â ë°©í–¥ì€Â ì•ŒÂ ìˆ˜Â ì—†ìŠµë‹ˆë‹¤.Â ì´ì—Â ë”°ë¼Â ë¯¼ê°ë„Â ë¶„ì„ì—ì„œëŠ”Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸ì˜Â **í¬ê¸°**ë¥¼Â ê³ ë ¤í•©ë‹ˆë‹¤.Â ìœ„ì˜Â ê·¸ë¦¼Â ì˜ˆì‹œì²˜ëŸ¼Â ë¯¼ê°ë„Â ë¶„ì„ì„Â í†µí•´Â ì¶”ì •ëœÂ íš¨ê³¼ë¥¼Â ì—†ì•¨Â ì •ë„ë¡œÂ ì¶”ì •ëŸ‰ì„Â ë³€ê²½í•˜ë ¤ë©´Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸ì˜Â í¬ê¸°ê°€Â ì–´ëŠÂ ì •ë„ì—¬ì•¼Â í•˜ëŠ”ì§€ë¥¼Â í™•ì¸í• Â ìˆ˜Â ìˆê³ ,Â ë„ë©”ì¸Â ì§€ì‹ì„Â í™œìš©í•˜ì—¬Â ì¶”ì •ëœÂ íš¨ê³¼ì˜Â ê°•ê±´ì„±ì„Â ì£¼ê´€ì ìœ¼ë¡œÂ ê²°ì •í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.\n\n\nMore General Settings\nê°•ì˜ì—ì„œëŠ” ë” ì¼ë°˜ì ì¸ ë¯¼ê°ë„ ë¶„ì„ ë°©ë²•ì— ëŒ€í•´ ì†Œê°œí•©ë‹ˆë‹¤.Â \n\nAssess-ing Sensitivity to an Unobserved Binary Covariate in an Observational Study with Binary Outcome(1983)\nMaking sense of sensitivity: extending omitted variable bias(2020)\n\nì´ ì¤‘ ë‘ ë²ˆì§¸ ë…¼ë¬¸ì— ëŒ€í•´ì„œ ì§§ê²Œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.Â \nì´ ë…¼ë¬¸ì—ì„œëŠ” partial \\(R^2\\)ë¥¼ í™œìš©í•´ì„œ linear regressionì—ì„œ ë¯¼ê°ë„ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ë¦¬í¬íŒ…í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.Â \n**ì£¼ìš”Â ê¸°ì—¬ì‚¬í•­**\n1.Â Â RobustnessÂ valueÂ ê°œë°œ\n2.Â Â partialÂ \\(R^2\\)ë¥¼Â í™œìš©í•œÂ ë¯¼ê°ë„ë¶„ì„Â íˆ´Â ê°œë°œ\n3.Â Â extremeÂ scenarioì—Â ëŒ€í•œÂ ë¶„ì„Â ê·¸ë˜í”„Â ê°œë°œ\ní•´ë‹¹Â ë…¼ë¬¸Â ì €ìê°€Â ê°œë°œí•œÂ íˆ´ì€Â R,Â Python,Â StataÂ ë“±ì—ì„œÂ ì´ìš©í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.\n\n\n\npython :Â \n\nPySensemakr\npywhy\n\nR : sensemakrÂ \n\në…¼ë¬¸ì—ì„œ ë‚˜ì˜¤ëŠ” exampleì´ ì§ê´€ì ì´ì§€ ì•Šì•„ì„œ ë‹¤ë¥¸ ë¸”ë¡œê·¸ ì˜ˆì‹œë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.Â \n\nì°¸ê³  : https://matteocourthoud.github.io/post/ovb/\nêµìœ¡ê¸°ê°„ê³¼ ì„ê¸ˆ ì‚¬ì´ì˜ ê´€ê³„ì— ê´€ì‹¬ì´ ìˆìŒ\nêµìœ¡ê¸°ê°„ê³¼ ì„ê¸ˆ ì‚¬ì´ì—ëŠ” ë§ì€ unobserved confounderê°€ ì¡´ì¬í•¨\nì˜ˆì‹œë¥¼ ìœ„í•´ unobserved confounderë¡œ abilityê°€ ìˆë‹¤ê³  ê°€ì •\nabilityëŠ” omitted variableì´ì§€ë§Œ educationê³¼ wageì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•¨\n\n\n\n\nlibrary(sensemakr)\nlibrary(tidyverse)\n#setwd(\"./posts/2023-02-07-sensitivity-analysis\")\ndf &lt;- read.csv(\"ex_data.csv\")[, -1] %&gt;% \n    mutate(gender = as.factor(gender))\ndf %&gt;% head(2)\n\n  age gender education wage\n1  62   male         6 3800\n2  44   male         8 4500\nfit &lt;- lm(wage ~ ., df)\nsummary(fit)\n\nCall:\nlm(formula = sleep_total ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7435 -2.6495  0.0466  1.3376  7.0845 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.618464   0.862433  14.631 4.59e-11 ***\nbrainwt     -9.246523  14.305157  -0.646    0.527    \nbodywt      -0.009216   0.013960  -0.660    0.518    \n---\nSignif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n\nResidual standard error: 3.476 on 17 degrees of freedom\nMultiple R-squared:  0.4766,    Adjusted R-squared:  0.4151 \nF-statistic: 7.741 on 2 and 17 DF,  p-value: 0.004072\níšŒê·€ë¶„ì„ ê²°ê³¼, ì¶”ì •ëœ ATEëŠ” ì•½Â 96ìœ¼ë¡œ, êµìœ¡ ê¸°ê°„ì´ í•œ ë‹¨ìœ„ ì¦ê°€í•  ë•Œ, ì„ê¸ˆì€ ì•½ 96 ì¦ê°€í•©ë‹ˆë‹¤(education =Â 95.94). í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸(ability)ì´ ì¡´ì¬í•˜ë¯€ë¡œ, í•´ë‹¹ ì¶”ì •ëŸ‰ì€ í¸í–¥ ì¶”ì •ëŸ‰ì…ë‹ˆë‹¤(í•´ë‹¹ ë°ì´í„°ëŠ” ê°€ìƒì˜ ë°ì´í„°ì´ë¯€ë¡œ, ì‹¤ì œë¡œëŠ” ìˆ˜ ë§ì€ êµë€ìš”ì¸ì´ ì¡´ì¬í•©ë‹ˆë‹¤).\nê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì˜ í¬ê¸°ì— ë”°ë¼ ì¶”ì •ëœ ATEì˜ ê°•ê±´ì„±ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ë¯¼ê°ë„ ë¶„ì„ì„ ìˆ˜í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤. sensemakr íŒ¨í‚¤ì§€ì˜Â sensemakr()Â í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ê°„ë‹¨íˆ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nsens &lt;- sensemakr(model = fit, treatment = \"education\")\nsummary(sens)\n\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: wage ~ age + gender + education\n\nNull hypothesis: q = 1 and reduce = TRUE \n-- This means we are considering biases that reduce the absolute value of the current estimate.\n-- The null hypothesis deemed problematic is H0:tau = 0 \n\nUnadjusted Estimates of 'education': \n  Coef. estimate: 95.9437 \n  Standard Error: 38.7521 \n  t-value (H0:tau = 0): 2.4758 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.1176 \n  Robustness Value, q = 1: 0.3044 \n  Robustness Value, q = 1, alpha = 0.05: 0.0627 \n\nVerbal interpretation of sensitivity statistics:\n\n-- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 11.76% of the residual variance of the treatment to fully account for the observed estimated effect.\n\n-- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 30.44% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 30.44% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0.\n\n-- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 6.27% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 6.27% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0, at the significance level of alpha = 0.05.\nUnadjusted Estimates of â€™ education â€™:ëŠ” ê¸°ì¡´ íšŒê·€ë¶„ì„ ê²°ê³¼ì™€ ê°™ìŠµë‹ˆë‹¤. Sensitivity Statistics:ì„ ë³´ë©´ partial \\(R^2\\)ì™€ Robustness Value(RV) ë“±ì´ í‘œê¸°ë©ë‹ˆë‹¤. ë˜í•œ í•´ë‹¹ ì§€í‘œì— ëŒ€í•œ í•´ì„ë„ í•¨ê»˜ ì œì‹œë©ë‹ˆë‹¤.\n\n\\(RV_1\\)Â : ì¸¡ì •ë˜ì§€ ì•Šì€ êµë¸ìš”ì¸ì´ êµìœ¡ê³¼ ì„ê¸ˆì˜ ì”ì°¨ ë³€ë™ì˜ 30.44%ë¥¼ ì„¤ëª…í•œë‹¤ë©´ ì¶”ì •ëŸ‰ì„ 0ìœ¼ë¡œ ë§Œë“¤ê¸° ì¶©ë¶„í•¨ or ì¶©ë¶„í•˜ì§€ ì•ŠìŒ\n\n30.44%ê°€ ì¶©ë¶„í•œì§€ or ì¶©ë¶„í•˜ì§€ ì•Šì€ì§€ëŠ” ë¶„ì„ê°€ì˜ íŒë‹¨ì— ì˜ì¡´í•©ë‹ˆë‹¤. ë˜ëŠ”Â benchmark_covariatesÂ ì˜µì…˜ì„ í†µí•´ ê´€ì°°ëœ \\(X\\)Â ë³€ìˆ˜ì™€ ë¹„êµí•¨ìœ¼ë¡œì¨ ì¶”ê°€ì ì¸ í•´ì„ì„ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nÂ Robustness valueÂ \n\\(RV_q\\)ëŠ”Â ì¶”ì •ëœÂ treatmentÂ effectì˜Â ì•½Â \\((100Â \\timesÂ q)\\)%ë¥¼Â ì„¤ëª…í•˜ê¸°Â ìœ„í•´ì„œÂ unobservedÂ confounderì˜Â íš¨ê³¼ê°€Â ì–¼ë§ˆë‚˜Â ê°•ë ¥í•´ì•¼Â í•˜ëŠ”ì§€ë¥¼Â ì„¤ëª…í•˜ëŠ”Â ì§€í‘œì…ë‹ˆë‹¤.\n\n\n\n\\(ZÂ \\simÂ Y\\)ì˜Â ì˜í–¥ê³¼Â \\(ZÂ \\simÂ D\\)ì˜Â ì˜í–¥ì´Â ë™ì¼í•˜ë‹¤ê³ Â ê°€ì •í• Â ê²½ìš°Â \\(RV_q\\)ë¥¼Â ìœ ë„í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.\n\\(R^2_{Y\\simÂ Z|X,Â D}Â =Â R^2_{D\\simÂ Z|X}=RV_q\\)ì¼Â ë•Œ,\n\\[ \\begin{aligned} RV_qÂ =Â \\frac{1}{2}(\\sqrt{f^4_qÂ +Â 4f^2_qÂ -Â f^2_q}),Â \\quadÂ fÂ =Â q\\cdot|\\frac{t}{df}|\\,\\,Â (cohen's\\,f) \\end{aligned} \\]\n\n\\(RV_q \\approx 1\\)ì¼ ê²½ìš° \\(Z\\)ê°€ \\(Y\\)ì™€ \\(D\\)ì˜ ëª¨ë“  ì”ì°¨ ë³€ë™ì„ ì„¤ëª…í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨Â  Â \n\nstrong confounder\n\n\\(RV_q \\approx 0\\)ì¼ ê²½ìš° \\(Z\\)ê°€ \\(Y\\)ì™€ \\(D\\)ì˜ ëª¨ë“  ì”ì°¨ ë³€ë™ì„ ì„¤ëª…í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨Â  Â  Â \n\nweak confonder\n\n\n\n\nSensitivity Contour plotÂ \nRobustness ValueëŠ” \\(R^2_{Y\\sim Z|X, D} = R^2_{D\\sim Z|X}\\)ì¼ ë•Œë¥¼ ê°€ì •í•˜ë¯€ë¡œ, \\(R^2_{Y\\sim Z|X, D} \\neq R^2_{D\\sim Z|X}\\)ì¼ ê²½ìš° ê·¸ë˜í”„ë¥¼ í†µí•´ ëŒ€ëµì ì¸ ì¶”ì´ë¥¼ íŒŒì•…í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â \nplot(sens, xlab = \"Partial R^2 of ability with education\", \n     ylab = \"Partial $R^2$ of ability with wage\")\n\nê·¸ë˜í”„ë¥¼Â ë³´ë©´Â UnadjustedëŠ”Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸(ability)ê°€Â ì˜í–¥ì„Â ì£¼ì§€Â ì•Šì„Â ë•Œë¥¼Â ì˜ë¯¸í•©ë‹ˆë‹¤.Â ì¦‰,Â íšŒê·€ë¶„ì„Â ê²°ê³¼ì™€Â ë™ì¼í•©ë‹ˆë‹¤.Â ìš°ì¸¡Â ìƒë‹¨ìœ¼ë¡œÂ ê°ˆìˆ˜ë¡Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸ì˜Â íš¨ê³¼ê°€Â ì¦ê°€í•˜ê³ ,Â ë¹¨ê°„ìƒ‰Â dotÂ lineì—Â ë„ë‹¬í–ˆì„Â ë•Œ,Â \\(\\hat{\\beta}_{education}Â =Â 0\\)ì´Â ë©ë‹ˆë‹¤.Â \nê·¸ë˜í”„ë¥¼Â ë³´ë©´Â ë¹¨ê°„ìƒ‰Â dotÂ lineÂ ìœ„ì—Â ìˆëŠ”Â ê°’ìœ¼ë¡œÂ \\(R^2_{YÂ \\simÂ Z|D,X}Â =Â 0.3\\),Â $Â R^2_{DÂ \\simÂ Z|X}Â =Â 0.3$Â ì •ë„ë¡œÂ ìƒê°í•´ë³¼Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â ë…¼ë¬¸ì—ì„œÂ ìœ ë„í•œÂ ê³µì‹ì„Â ì´ìš©í•´ì„œÂ biasÂ ì¶”ì •ì¹˜ë¥¼Â êµ¬í•´ë³´ê³ Â í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.Â \në…¼ë¬¸ì—ì„œÂ partialÂ \\(R^2\\)ë¥¼Â ì´ìš©í•´ì„œÂ ìœ ë„í•œÂ biasÂ ì¶”ì •ì¹˜ëŠ”Â ë‹¤ìŒê³¼Â ê°™ìŠµë‹ˆë‹¤.Â \n\\[ |\\hat{bias}|Â =Â \\sqrt{\\frac{R^2_{YÂ \\simÂ Z|D,X}Â \\cdotÂ R^2_{DÂ \\simÂ Z|X}}{1Â -Â R^2_{DÂ \\simÂ Z|X}}}Â \\cdotÂ \\frac{sd(Y^{\\perpÂ X,Â D})}{sd(D^{\\perpÂ X})} \\]\nR_YZ = 0.3\nR_DZ = 0.3 \n\nDperpX &lt;- lm(education ~ age + gender, df)$residuals\nYperpXD &lt;- lm(wage ~ ., df)$residuals\n\nbias &lt;- sqrt((R_YZ*R_DZ/(1 - R_DZ)))*(sd(YperpXD)/sd(DperpX))\n\n95.94 - bias\n\n[1] 1.697537\n\\(\\hat{\\beta}_{education}Â =Â 95.94\\)ì´ê³ ,Â \\(|\\hat{bias}|Â =Â 94.24\\)ì´ë¯€ë¡œ,Â ëŒ€ëµÂ \\(0\\)ì—Â ê°€ê¹Œìš´Â ê²ƒì„Â í™•ì¸í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â \n\n\nSensitivityÂ contourÂ plotÂ usingÂ benchmarkÂ covariatesÂ \nsensitivity contour plotì„ ë´¤ì„ ë•Œ, ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì˜ treatmentì™€ outcomeì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ë”°ë¥¸ bias íš¨ê³¼ë¥¼ í™•ì¸í•´ë³¼ ìˆ˜ ìˆì§€ë§Œ, ì‹¤ì œë¡œ ì´ biasê°€ í°ì§€ í˜¹ì€ ì‘ì€ì§€ëŠ” ì£¼ê´€ì ì¸ íŒë‹¨ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì¦‰, ì´ì „ ì˜ˆì‹œì—ì„œ \\(R^2_{Y \\sim Z|D,X} = 0.3\\), \\(R^2_{D \\sim Z|X} = 0.3\\) ì¼ ë•Œ, ì¶”ì •ëœ íšŒê·€ê³„ìˆ˜ëŠ” \\(0\\)ì´ ë˜ë§Œ, \\(R^2_{Y \\sim Z|D,X} = 0.3\\), \\(R^2_{D \\sim Z|X} = 0.3\\) ê°’ì´ í°ì§€ í˜¹ì€ ì‘ì€ì§€ì˜ ê¸°ì¤€ì€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.Â \nì´ëŸ¬í•œ ê¸°ì¤€ ì„¤ì •ì˜ ì–´ë ¤ì›€ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ ê´€ì°°ëœ ì„¤ëª…ë³€ìˆ˜ë¥¼ ë²¤ì¹˜ë§ˆí¬í•˜ì—¬ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì´ ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ í¬ê¸°ë¥¼ ëŒ€ëµì ìœ¼ë¡œ ì¶”ì¸¡í•©ë‹ˆë‹¤.Â \n\\[ \\begin{aligned} k_DÂ :=Â \\frac{R^2_{DÂ \\simÂ Z|X_{-j}}}{R^2_{DÂ \\simÂ X_j|X_{-j}}},Â \\quad k_YÂ :=Â \\frac{R^2_{YÂ \\simÂ Z|X_{-j},Â D}}{R^2_{YÂ \\simÂ X_j|X_{-j},Â D}} \\end{aligned} \\]\n\\(k_D \\ge 1\\)ì¼ ë•Œ, \\(R^2_{D \\sim Z|X_{-j}} \\ge R^2_{D \\sim X_j|X_{-j}}\\)ì´ë¯€ë¡œ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì´ ì‚¬ì „ì— ì§€ì •í•œ ì„¤ëª…ë³€ìˆ˜ ëŒ€ë¹„ Treatmentì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í¬ë‹¤ëŠ” ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\(k_Y \\ge 1\\)ì¼ ë•Œë„ ë§ˆì°¬ê°€ì§€ë¡œ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì´ ì‚¬ì „ì— ì§€ì •í•œ ì„¤ëª…ë³€ìˆ˜ ëŒ€ë¹„ ë°˜ì‘ë³€ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í¬ë‹¤ëŠ” ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nsens2 &lt;- sensemakr(model = fit, treatment = \"education\", \n                  benchmark_covariates = \"age\", \n                  kd = c(0.5, 1, 2), \n                  ky = c(0.5, 1, 2))\n\nplot(sens2)\n\nsensemakr()Â í•¨ìˆ˜ì—ëŠ”Â `benchmark_covariates`Â ì˜µì…˜ì´Â ì¡´ì¬í•˜ë©°,Â \\(k_D\\),Â \\(k_Y\\)ì˜Â í¬ê¸°ë¥¼Â ì‚¬ì „ì—Â ì§€ì •í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â ê·¸ë˜í”„ë¥¼Â í•´ì„í•´ë³´ë©´Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸(ability)ê°€Â ageì˜Â ë‘Â ë°°Â ì •ë„ì˜Â ì„¤ëª…ë ¥ì„Â ê°–ë”ë¼ë„,Â \\(\\hat{\\beta}_{education}Â =Â 67.61\\)ë¡œÂ ê°’ì˜Â ë³€í™”ëŠ”Â ìˆì§€ë§ŒÂ ë¶€í˜¸ëŠ”Â ì—¬ì „íˆÂ positiveì¸Â ê²ƒì„Â ë³¼Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â \nì˜µì…˜ì„Â ì¶”ê°€í• Â ê²½ìš°Â í•´ë‹¹Â plotì—ì„œÂ í†µê³„ì Â ìœ ì˜ì„±Â ë˜í•œÂ ì²´í¬í•´ë³¼Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â \nplot(sens2, sensitivity.of = \"t-value\")\n\ní†µê³„ì  ìœ ì˜ì„±ì„ ë³´ë©´ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸(ability)ê°€ \\(1 \\times age\\)ë³´ë‹¤ ì•½ê°„ í° ì •ë„ì˜ ì„¤ëª…ë ¥ì„ ê°–ëŠ”ë‹¤ë©´, \\(\\hat{\\beta}_{education}\\)ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•Šê²Œ ë°”ë€” ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.Â \nplot(sens2)\nadd_bound_to_contour(r2dz.x = 0.3, \n                     r2yz.dx = 0.3, \n                     bound_label = \"Something related to\\nboth outcome and treatment\")\n\nì¶”ê°€ì ìœ¼ë¡œÂ ì„ì˜ë¡œÂ partialÂ \\(R^2\\)ë¥¼Â ì„¤ì •í•´ì„œÂ ì ì„Â ì°ì–´ë³¼Â ìˆ˜ë„Â ìˆìŠµë‹ˆë‹¤.Â \n\n\nSensitivityÂ plotsÂ ofÂ extremeÂ scenarios\nextremeÂ scenarioëŠ”Â outcomeì˜Â ê±°ì˜Â ëª¨ë“ Â ì”ì°¨Â ë³€ë™ì„Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸ì´Â ì„¤ëª…í•œë‹¤ëŠ”Â ì˜ë¯¸ë¡œ,Â \\(R_{YÂ \\simÂ Z|D,X}Â =Â 1\\)ì„Â ì˜ë¯¸í•©ë‹ˆë‹¤(ì˜µì…˜ìœ¼ë¡œÂ \\(0.75,Â 0.5\\)ì¼Â ë•Œë„Â í•¨ê»˜Â ì œì‹œë¨).Â ê·¸ë˜í”„ë¥¼Â í™œìš©í•˜ì—¬Â extremeÂ scenarioì¼Â ë•Œ,Â \\(R^2_{DÂ \\simÂ Z|X}\\)ì˜Â ë³€í™”ì—Â ë”°ë¼Â ì¶”ì •ëœÂ íšŒê·€ê³„ìˆ˜ê°€Â ì–´ë–»ê²ŒÂ ë°”ë€ŒëŠ”ì§€ë¥¼Â ì‹œê°í™”í•´ë³¼Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â \nsens3 &lt;- sensemakr(model = fit, treatment = \"education\", \n                  benchmark_covariates = \"age\", \n                  kd = c(1, 2, 3, 4), \n                  ky = c(1, 2, 3, 4))\nplot(sens3, type = \"extreme\", lim = 0.5)\nresult &lt;- plot(sens3, type = \"extreme\", lim = 0.5)\n\nresult$scenario_r2yz.dx_1[117:119,]\n\n    r2dz.x r2yz.dx adjusted_estimate\n117  0.116       1         0.7348541\n118  0.117       1         0.2712232\n119  0.118       1        -0.1912156\n\nÂ x axis : \\(R^2_{D \\sim Z|X}\\)Â \ny axis : \\(\\hat{\\tau} = \\hat{\\tau}_{res} - \\hat{bias}\\)Â \nline : \\(R^2_{Y \\sim Z|D,X}\\)\n\nê·¸ë˜í”„ë¥¼Â ë³´ë©´Â ë¹¨ê°„ìƒ‰Â dotÂ lineì€Â ì¶”ì •ëœÂ íšŒê·€ê³„ìˆ˜ê°€Â 0ì´Â ë˜ëŠ”Â ê²½ìš°ì—Â í•´ë‹¹í•©ë‹ˆë‹¤.Â solidÂ lineì€Â \\(R^2_{YÂ \\simÂ Z|D,X}Â =Â 1\\)ì¼Â ë•Œì—Â í•´ë‹¹í•©ë‹ˆë‹¤.Â \\(R^2_{YÂ \\simÂ Z|D,X}Â =Â 1\\)ì¼Â ë•Œ,Â ì¶”ì •ëœÂ íšŒê·€ê³„ìˆ˜ê°€Â 0ì´Â ë˜ê¸°Â ìœ„í•´ì„œëŠ”Â \\(R^2_{DÂ \\simÂ Z|X}Â \\approxÂ 0.117\\)Â ì •ë„Â ë˜ì–´ì•¼Â í•˜ëŠ”Â ê²ƒì„Â í™•ì¸í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â Â \në˜í•œ, ì™¼ìª½ í•˜ë‹¨ì— ë¹¨ê°„ìƒ‰ vertical lineì€ benchmarkë¡œ ì„¤ì •í•œ ë³€ìˆ˜ì— ë¹„í•´ confounderê°€ ëª‡ ë°° ë” ê°•ë ¥í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ `kd = c(0.5, 1, 2)` ì˜µì…˜ì—ì„œ ì„¤ì •í•œ ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.Â \nê´€ì°°ëœ ì„¤ëª…ë³€ìˆ˜ ageê°€ educationì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ì— ë¹„í•´ì„œ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì´ educationì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ê°€ ë„¤ ë°° ì •ë„ í´ ë•Œ, \\(\\hat{\\beta}_{education} \\approx 0\\)ì´ ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â \nìƒì‹ì ìœ¼ë¡œÂ êµìœ¡ê¸°ê°„ê³¼Â ë‚˜ì´ëŠ”Â ì¶©ë¶„í•œÂ ì–‘ì˜Â ìƒê´€ê´€ê³„ê°€Â ì¡´ì¬í•œë‹¤ê³ Â ìƒê°í• Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤.Â ë”°ë¼ì„œÂ ê´€ì°°ëœÂ ì„¤ëª…ë³€ìˆ˜Â ageê°€Â educationì—Â ë¯¸ì¹˜ëŠ”Â íš¨ê³¼ì—Â ë¹„í•´ì„œÂ ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸ì´Â educationì—Â ë¯¸ì¹˜ëŠ”Â íš¨ê³¼ê°€Â ë„¤Â ë°°ì •ë„Â í´Â ë•Œ,Â \\(\\hat{\\beta}_{education}Â \\approxÂ 0\\)ì´Â ëœë‹¤ëŠ”Â ì˜ë¯¸ëŠ”Â ATEÂ ì¶”ì •ëŸ‰ì´Â ì¶©ë¶„íˆÂ ì‹ ë¢°í• Â ìˆ˜Â ìˆê³ ,Â ê´€ì°°ë˜ì§€Â ì•Šì€Â êµë€ìš”ì¸ì—Â ê°•ê±´í•˜ë‹¤ëŠ”Â ê²ƒì„Â ì˜ë¯¸í•œë‹¤ê³ Â ë³¼Â ìˆ˜Â ìˆìŠµë‹ˆë‹¤(ì£¼ê´€ì ì¸ í•´ì„).Â Â \n\n\npywhy tutorialÂ \nimport dowhy\nfrom dowhy import CausalModel\nimport pandas as pd\nimport numpy as np\nimport dowhy.datasets\nimport os\nfrom dowhy import CausalModel\ndat = pd.read_csv(\"posts/2023-02-07-sensitivity-analysis/ex_data.csv\", index_col = 0)\ndat.head()\ngdot = \"\"\"graph[directed 1 node[id \"age\" label \"age\"]\n                    node[id \"gender\" label \"gender\"]\n                    node[id \"education\" label \"education\"]\n                    node[id \"wage\" label \"wage\"]\n                    \n                    edge[source \"education\" target \"wage\"]\n                    edge[source \"gender\" target \"wage\"]\n                    edge[source \"age\" target \"wage\"]]\"\"\"\nmodel = CausalModel(\n            data = dat,\n            treatment=\"education\",\n            outcome=\"wage\", \n            graph = gdot\n        )\nidentified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\nprint(identified_estimand)\nestimate = model.estimate_effect(identified_estimand,\n                                 method_name=\"backdoor.linear_regression\")\nrefute = model.refute_estimate(identified_estimand, \n                               estimate,\n                               method_name = \"add_unobserved_common_cause\",\n                               simulated_method_name = \"linear-partial-R2\",\n                               benchmark_common_causes = \"age\",\n                               effect_fraction_on_treatment = [ 1,2,3]\n                              )"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Motivation/Motivation.html",
    "href": "posts/Introduction_to_causal_inference_Motivation/Motivation.html",
    "title": "02. Motivation",
    "section": "",
    "text": "Contents\n\nCausal Inference ë€ ë¬´ì—‡ì¸ê°€?\nì‹¬ìŠ¨ì˜ ì—­ì„¤ (Simpsonâ€™s Paradox)\nìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤\nCausation in Observational studies\n\nâ—¦Â ê°•ì˜ ì˜ìƒ ë§í¬ :Â Chapter 1 -Â AÂ BriefÂ IntroductionÂ toÂ CausalÂ InferenceÂ (CourseÂ Preview)\nì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!\n\n\n\n(1) Causal Inference ë€ ë¬´ì—‡ì¸ê°€?\n\nCausal InferenceÂ is concerned withÂ a very specific kind of prediction problemÂ :\nPredicting the results of an action, manipulation, or Intervention\nâ€œMaking Things Happenâ€ (2003, Woodwrad)\n\n\nì •ì˜ : í˜„ìƒ(ë¬¸ì œ)ì— ëŒ€í•œ ì›ì¸ì„ ì°¾ê³  í•´ë‹¹ ì›ì¸ì— ëŒ€í•œ íš¨ê³¼ë¥¼ ì¶”ë¡ í•˜ëŠ” ê²ƒ\nëª©í‘œ : ë°œìƒí•œ í˜„ìƒì— ëŒ€í•œ â€˜Whyâ€™ ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” ê²ƒ (Causal Structureë¥¼ ê¸°ë°˜ìœ¼ë¡œ)\nExample : Effect ofÂ \\(X\\)Â (ë…ë¦½ë³€ìˆ˜) on \\(Y\\) (ì¢…ì†ë³€ìˆ˜)\n\n\nÂ - ì´ë²ˆ í• ì¸ ì´ë²¤íŠ¸(\\(X\\))ë¡œ ê³ ê³¼ê¸ˆ PU(\\(Y\\))ê°€ ì¦ê°€í•œ ê²ƒ ê°™ì€ë°, ì–´ëŠ ì •ë„ íš¨ê³¼ê°€ ìˆì—ˆì„ê¹Œìš”?\nÂ - ì–´ë– í•œ ìº í˜ì¸(\\(X\\))ì„ ë…¸ì¶œì‹œí‚¤ë©´, CTR(\\(Y\\))ë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆì„ê¹Œìš”?\n\n\nì¸ê³¼ê´€ê³„ì˜ 3ê°€ì§€ ë‹¨ê³„ (The Ladder of Causation)\n\n\n\nAssociation : \\(P(Y|observe(X))\\) &lt; Supervised Learning &gt;\nÂ  Â  â—¦ ê´€ì°°ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³€ìˆ˜ê°„ì˜ ì—°ê´€ì„±ì„ íŒŒì•…í•˜ëŠ” ë‹¨ê³„ (What If I see ?)\nIntervention : \\(P(Y|do(X))\\) &lt; \\(do\\) :Â ì‹¤í—˜ ê°œì…(í†µì œ)ì˜ ì˜ë¯¸Â &gt;\nÂ  Â  â—¦ ë§Œì•½ \\(X\\)(ê°œì…, í–‰ë™)ìœ¼ë¡œ ì¸í•´, \\(Y\\)(ê²°ê³¼)ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ íŒŒì•…í•˜ëŠ” ë‹¨ê³„\nCounterfactuals (Counter to fact) : ê°€ì •ë²•\nÂ  Â  â—¦ ê°€ìƒì˜ í˜„ì‹¤ (ì‹¤ì œë¡œ ê´€ì¸¡ë˜ì§€ ì•ŠëŠ” ìƒí™©)ì„ ìƒìƒí•˜ëŠ” ë‹¨ê³„\nÂ  Â  â—¦ ì‹¤ì œë¡œ ì¼ì–´ë‚˜ì§€ ì•Šì•˜ì§€ë§Œ, í•´ë‹¹ ìƒí™©ì´ ë°œìƒí–ˆë‹¤ë©´ (\\(X'\\)) ê²°ê³¼(\\(Y\\))ê°€ ë‹¬ë¼ì¡Œì„ê¹Œ?\nÂ  Â  â—¦ ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œ (Fundamental Problem of Causal Inference, 2ì¥)\n\n\n\n\n\n(2) ì‹¬ìŠ¨ì˜ ì—­ì„¤ (Simpsonâ€™s paradox)\n\nì •ì˜ : ë°ì´í„°ë¥¼ Subgroupìœ¼ë¡œ ë‚˜ëˆ ì„œ ë³´ì•˜ì„ ë•Œì™€ ì „ì²´ ë°ì´í„°ë¥¼ í•©í•´ì„œ ë´¤ì„ ë•Œ,\nÂ  Â  Â  Â  Â  Â ê²°ê³¼ê°€ ì„œë¡œ ë‹¤ë¥¸ ê²½ìš°Â (í†µê³„ì  ì—°ê´€ì„±ì´ ìœ ì§€ë˜ì§€ ì•ŠëŠ” ê²½ìš°)\nì˜ˆì‹œ : COVID-27ì— ëŒ€í•œ ì¹˜ë£Œë²•\n\n\nâ—¦Â  ëª©ì Â : COVID-27ì— í™•ì§„ëœ í™˜ìì˜ ì‚¬ë§ìœ¨ì„ ë‚®ì¶”ëŠ” Treatment (A, B)ë¥¼ ì„ íƒ\nâ—¦Â  ìƒí™©Â : ì¹˜ë£Œë²• BëŠ” Aë³´ë‹¤ ë” ê·€í•¨ (ì¹˜ë£Œë²• Aë¥¼ ë°›ëŠ” ë¹„ì¤‘ : 73%, Bë¥¼ ë°›ëŠ” ë¹„ì¤‘ : 27%)\nâ—¦Â  ë°ì´í„° í•´ì„Â :Â \nÂ  Â  -Â ë°ì´í„° ì „ì²´ë¡œ ë³¸ ê²½ìš° :Â ì¹˜ë£Œë²• Aë¥¼ ë°›ì€ í™˜ì ì‚¬ë§ìœ¨ì€ ì¹˜ë£Œë²• Bë³´ë‹¤Â ë‚®ìŒ\nÂ  Â  - Subgroupìœ¼ë¡œ ë‚˜ëˆ ì„œ ë³¸ ê²½ìš° : ê° í™˜ì Conditionë³„ ì‚¬ë§ìœ¨ì€Â ì¹˜ë£Œë²• Aê°€ Bë³´ë‹¤ ë†’ìŒ\n\n\nâ—¦ ë™ì¼í•œ ë°ì´í„°ì¸ë°, ê²°ê³¼ê°€ ë‹¤ë¥¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?\nâ†’ Weighted Sum : ì¹˜ë£Œë²• A, Bì— ëŒ€í•œ ì‚¬ë§ìœ¨ì˜Â ê° Condition(Subgroup)ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n&lt;Non-uniformity of allocation of people to groups&gt;\n\n\nâ—¦ í™˜ìì˜ ì‚¬ë§ìœ¨ì„ ë‚®ì¶”ë ¤ë©´ ì–´ë–¤ ì¹˜ë£Œë²•ì„ ì„ íƒí•´ì•¼ í• ê¹Œìš”? í™˜ìì˜ ìƒíƒœë¥¼ ëª¨ë¥¸ë‹¤ë©´, ì¹˜ë£Œë²•ì„ ì œê³µí•  ìˆ˜ ì—†ëŠ” ê±¸ê¹Œìš”?\nâ†’ ë°ì´í„°ì˜Â Casual Structureì— ë”°ë¼, ì¹˜ë£Œë²• ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤!\n\n\n\n\n(3) ìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤\n\nCorrelation : ì—„ë°€í•˜ê²ŒëŠ” ë³€ìˆ˜ê°„ ì„ í˜•ì ì¸ í†µê³„ì  ê´€ê³„ë¥¼ ì˜ë¯¸ (LinearÂ Statistical Dependence)\nSpurious Correlations : ì„œë¡œ ì—°ê´€ì„±ì´ ì—†ëŠ” ë³€ìˆ˜ê°€ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ê²½ìš°\n\nÂ  Â  Â  Â â†’ ë°ì´í„°ë¥¼ í†µí•œ ì˜ì‚¬ê²°ì • ê³¼ì •ì—ì„œ, ì˜ëª»ëœ íŒë‹¨ì„ í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n\ní†µê³„í•™ë¥¼ ë°°ìš°ë©´ í•­ìƒ ë‚˜ì˜¤ëŠ” ì´ì•¼ê¸° ì…ë‹ˆë‹¤. ì‚¬ë¡€ë¥¼ í†µí•´ ì´í•´í•´ë³´ë„ë¡ í•´ìš”!\nì‚¬ë¡€ 1Â : ì—°ê°„ ë‹ˆì½œë¼ìŠ¤ ì¼€ì´ì§€ì˜ ì˜í™” ì¶œí˜„ íšŸìˆ˜ì™€ ì—°ê°„ ìµì‚¬ ì‚¬ë§ì‚¬ê³  ê±´ìˆ˜Â \n\n\nâ—¦Â  ì—°ê°„ ë‹ˆì½œë¼ìŠ¤ ì¼€ì´ì§€ì˜ ì˜í™” ì¶œí˜„ íšŸìˆ˜ì™€ ì—°ê°„ ìµì‚¬ ì‚¬ë§ê±´ìˆ˜ëŠ” ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì…ë‹ˆë‹¤.\nâ—¦Â  ê·¸ëŸ¬ë©´, ì¼€ì´ì§€ê°€ ë§ì€ ìˆ˜ì˜í•˜ëŠ” ì‚¬ëŒë“¤ì´ ìˆ˜ì˜ì¥ì— ë›°ì–´ë“¤ë„ë¡ ë¶€ì¶”ê¸´ê±¸ê¹Œìš”? No No!\n\n\n\nì‚¬ë¡€ 2Â : ì‹ ë°œì„ ì‹ ê³  ìëŠ” ê²ƒê³¼ ë‘í†µìœ¼ë¡œ ì¼ì–´ë‚¬ì„ ë•Œ ë‘í†µì„ í˜¸ì†Œí•˜ëŠ” ê²ƒÂ Â \n\n\nâ—¦Â  ìƒí™© : ì‹ ë°œì„ ì‹ ê³  ìëŠ” ê²ƒê³¼ ë‘í†µìœ¼ë¡œ ì¼ì–´ë‚¬ì„ ë•Œ ë‘í†µì„ í˜¸ì†Œí•˜ëŠ” ê²ƒì€ í° ìƒê´€ê´€ê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\nâ—¦Â  ëª©í‘œ : ìš°ë¦¬ëŠ” í•´ë‹¹ ì‹¤í—˜ì—ì„œ, ì‹ ë°œì„ ì‹ ê³  ìëŠ” ê²ƒì´ ì¼ì–´ë‚¬ì„ ë•Œ ë‘í†µì„ ìœ ë°œí•˜ëŠ” ì§€Â ì¸ê³¼ê´€ê³„ë¥¼ ì°¾ê³  ì‹¶ì–´ìš”!\nâ—¦Â Â ë°©í•´ìš”ì¸Â : ë‘ ë³€ìˆ˜ì˜ ê³µí†µìœ¼ë¡œ ì˜í–¥ì„ ì£¼ëŠ” ë³€ìˆ˜ &lt; ì „ë‚  ìˆ ì„ ë§ˆì‹  ê²ƒ &gt;\nÂ  Â  -Â ConfounderÂ : \\(X\\)(ì›ì¸)ì™€ \\(Y\\)(ê²°ê³¼)ì— ë™ì‹œì— ì˜í–¥ì„ ì£¼ëŠ” ë³€ìˆ˜\nÂ  Â  -Â ColliderÂ : \\(X\\)(ì›ì¸)ì™€ \\(Y\\)(ê²°ê³¼)ì— ë™ì‹œì— ì˜í–¥ì„ ë°›ëŠ” ë³€ìˆ˜\nÂ  Â  â†’Â ì¸ê³¼ì¶”ë¡ ì„ ì–´ë µê²Œ ë§Œë“œëŠ” ìš”ì¸Â ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\nâ—¦Â  What to do?Â \nÂ  â†’ ì¼ì–´ë‚¬ì„ ë•Œ ë‘í†µì´ ìˆëŠ” ê²ƒ(\\(Y\\))ì˜ ì›ì¸ì´ ì‹ ë°œì„ ì‹ ê³  ì ì—ë“  ê²½ìš°(\\(X\\))ë¼ê³ Â ê²°ë¡ ì„ ë‚´ë¦¬ë ¤ë©´, ì „ë‚  ìŒì£¼ ì—¬ë¶€ (Confounder)ì— ëŒ€í•œ ë¶€ë¶„ì„ í†µì œí•´ì•¼ í•©ë‹ˆë‹¤.\n\n\n\nTotalÂ Association =Â ConfoundingÂ Association +Â CausalÂ AssociationÂ \nâ†’ ì´ ì‹ì„ í†µí•´ ë³¸ ê²ƒ ì²˜ëŸ¼, ìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\nCorrelationì´Â Causationê³¼ ê°™ë‹¤ëŠ” ê²ƒì€Â Cognitive Biasì— í•´ë‹¹í•©ë‹ˆë‹¤. (ì¸ì§€í¸í–¥, ê²½í—˜ì— ì˜í•œ ë¹„ë…¼ë¦¬ì  ì¶”ë¡ )\n\n\nâ—¦Â Â Availability heuristicÂ : ì˜ì‚¬ê²°ì • ì‹œ, ì‚¬ëŒì˜ ë¨¸ë¦¿ì†ì— ë‹¹ì¥ ë– ì˜¤ë¥´ëŠ” ê²ƒì— ì˜ì¡´í•˜ëŠ” ê²½í–¥\nâ—¦Â Â Motivated ReasoningÂ : ê²°ë¡ ì— ëŒ€í•œ ëª©í‘œë¥¼ ì •í•´ë†“ê³ , ê·¸ ì£¼ì œì— ëŒ€í•´ì„œë§Œ ìƒê°í•˜ëŠ” ê²½í–¥\nâ—¦Â  í•´ë‹¹ ì¸ì§€ì  í¸í–¥ìœ¼ë¡œ ì¸í•´, ì‹ ë°œì„ ì‹ ê³  ìì„œ ì¼ì–´ ë‚¬ì„ ë•Œ ë‘í†µì´ ë°œìƒ(?)\nâ—¦Â Â Bias : Causationê³¼ Associationì„ ë‹¤ë¥´ê²Œ ë§Œë“œëŠ” ìš”ì†Œ\nÂ  Â  â†’ ì´ëŸ¬í•œ ê³¼ì •ì—ì„œÂ Correlationì„Â Causationìœ¼ë¡œ ì°©ê°í•˜ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤.\n\n\nâ†’ ì•ìœ¼ë¡œ ì¸ê³¼ì¶”ë¡ ì—Â ë°©í•´ë˜ëŠ” ìš”ì†Œë¥¼ ì–´ë–»ê²Œ í†µì œí•  ì§€ì— (Bias Adjustment) ëŒ€í•´ í•™ìŠµí•  ì˜ˆì •ì´ì—ìš”!\n\n\n(4) Causation in Observational studies\nâ†’ ê´€ì¸¡ í™˜ê²½ (Observational Studies, í†µì œë˜ì§€ ì•Šì€ í™˜ê²½)ì—ì„œ, ì¸ê³¼ì¶”ë¡ ì„ ì–´ë–»ê²Œ í• ê¹Œìš”?\n\nâ—¦Â  ì¸ê³¼ì¶”ë¡ ì„ ë°”ë¼ë³´ëŠ” ê´€ì  : Potential Outcomes (Chapter 2), Causal Models (Chapter4)\nâ—¦Â  ì‹¤í—˜ ì„¤ê³„ : Experiment Design (Chapter 5, 6)\nâ—¦Â  ì¸ê³¼íš¨ê³¼ ì¶”ì • : IPTW / Meta-Learner (Chapter 7), DID (Chapter 10), IV (Chapter 9)Â \nâ—¦Â  ê·¸ ì™¸ì—ë„ ì—¬ëŸ¬ê°€ì§€ ì¶”ì • ë°©ë²•ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n\n\nTreatment : ì¸ê³¼ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•œ ì›ì¸ ë³€ìˆ˜ì— í•´ë‹¹Â \nTreatment Effect : Treatmentì— ë”°ë¥¸ íš¨ê³¼\n\nÂ  Â  Â  Â  â—¦ ITE (Individual Treatment Effect) : Treatmentì— ëŒ€í•œÂ ê°œê°œì¸ì˜ íš¨ê³¼ë¥¼ ì¸¡ì •\nÂ  Â  Â  Â  â—¦ ATE (Average Treatment Effect) : Treatmentì— ëŒ€í•œÂ ì „ì²´Â í‰ê·  íš¨ê³¼ë¥¼ ì¸¡ì •\nÂ  Â  Â  Â  Â  Â â†’ ê°œê°œì¸ì— ëŒ€í•´ ITEë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ATEë¥¼ ì‚¬ìš©í•˜ê³¤ í•©ë‹ˆë‹¤.Â  Â  Â  Â  Â  Â Â \n\n\nObservational Study (ê´€ì¸¡ ì—°êµ¬) vs Experimental Study (ì‹¤í—˜ ì—°êµ¬)\n\nÂ  Â  Â  Â  â—¦ ExperimentalÂ StudyÂ :Â ì—°êµ¬ìê°€Â ì„¤ëª…ë³€ìˆ˜ì˜Â í• ë‹¹Â ìˆ˜ì¤€ì—Â ëŒ€í•´Â ê°œì…,Â ì¡°ì ˆì´Â ê°€ëŠ¥\nÂ  Â  Â  Â  â—¦ ObservationalÂ StudyÂ :Â ì—°êµ¬ìê°€Â XÂ (ì„¤ëª…ë³€ìˆ˜)ì—Â ëŒ€í•´Â ì¡°ì‘,Â ê°œì…ì—†ì´Â ë‹¨ìˆœíˆÂ ê´€ì°°\nÂ  Â  Â  Â  Â  &lt;Â ~ëŒ€ë¶€ë¶„ Analystê°€ ë¶„ì„í•˜ëŠ” í™˜ê²½ì€ Observational Study ì´ì§€ ì•Šì„ê¹Œìš”??..~&gt;\n\na. Potential(Counterfactual) Outcomes ê´€ì \n\nì •ì˜ : Treatment Optionì—ì„œ ë³¼ ìˆ˜ ìˆëŠ”Â ëª¨ë“  ì ì¬ì ì¸ ê²°ê³¼ë¥¼ ë°˜ì˜í•œÂ Causal Effectë¥¼ ë°”ë¼ë³´ëŠ” ê´€ì Â \nÂ  Â  Â  Â  Â  &lt; ì‹¤ì œë¡œÂ ê´€ì¸¡ë˜ì§€ ì•Šì€ Counterfactualí•œ ê²°ê³¼ë„ í¬í•¨ &gt;\nì˜ˆì‹œ :Â \n\n\nì‚¬ë¡€ 1) ê´‘ê³  ë…¸ì¶œê³¼ í´ë¦­ìœ¨Â \nâ—¦Â  Treatment : ìœ ì €ì—ê²Œ ê²Œì„ ê´‘ê³  ë…¸ì¶œ (Treatment Option - ìº í˜ì¸ A, ìº í˜ì¸ B)\nâ—¦Â Â Outcome : \\(Y\\_i(1)\\) - í´ë¦­, \\(Y\\_i(0)\\) - í´ë¦­í•˜ì§€ ì•ŠìŒ\nì‚¬ë¡€ 2) ì•½ê³¼ ë‘í†µì•½Â \nâ—¦Â  Â Treatment : ì•½ì„ ë¨¹ëŠ” ê²½ìš° - \\(do(T=1)\\) / ì•½ì„ ë¨¹ì§€ ì•ŠëŠ” ê²½ìš° - \\(do(T=0)\\)\nâ—¦Â  Â Outcome : \\(Y\\_i(1)\\) - ë‘í†µ í•´ì†Œ, \\(Y\\_i(0)\\) - ë‘í†µ ì§€ì†\n\n\nCausal Quantity(Estimand, ì¸ê³¼ ì¶”ì •ê°’)ì™€Â Statistical Quantity(Estimand, í†µê³„ì  ì¶”ì •ê°’)Â ë¹„êµ\n\n\nâ—¦Â Â ìƒí™©Â : Causal QuantityëŠ” Counterfactualsë¡œ ì¸í•´, ì§ì ‘ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nâ—¦Â Â ëŒ€ì•ˆÂ : í•´ë‹¹ ë¶€ë¶„ ëŒ€ì‹ , Treatmentê°€ ì£¼ì–´ì§„ ìƒí™©ì—ì„œì˜ Outcomeì¸ Statistical Quantityë¡œ ê³„ì‚°í•  ìˆ˜ ìˆì–´ìš”.\nâ—¦Â Â ë¬¸ì œÂ :Â Confounding Associationìœ¼ë¡œÂ Causal QuantityÂ â‰ Â Statistical Quantity\nÂ  Â  â†’ ê·¸ëŸ¬ë©´, Confounding Associationì„ ì–´ë–»ê²Œ ì—†ì• ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?\n\nâ—¦Â Â í•´ê²°ë°©ë²•Â : Randomized Controlled Trial (RCT)ê°€ í•´ë‹¹ ë¶€ë¶„ì„ í•´ê²°í•˜ëŠ”ë° ë‹µì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\nâ—¦Â Â RCTÂ : Control Group (ëŒ€ì¡°êµ°)ê³¼ Treatment Group (ì‹¤í—˜êµ°)ì„ ëœë¤í•˜ê²Œ í• ë‹¹í•´,\nÂ  Â  Â  Â  Â  Â  Â Xê°€ Yì— ì˜í–¥ì„ ë¯¸ì³¤ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹¤í—˜ ì„¤ê³„ì…ë‹ˆë‹¤. (ì‹¤ë¬´ì—ì„œëŠ” A/B í…ŒìŠ¤íŠ¸ë¼ê³  í•´ìš”)\n\nâ—¦Â  RCT ê¸°ëŒ€íš¨ê³¼Â :Â \nÂ  Â  Â 1) ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì˜ ê·¸ë£¹ê°„ ë™ì§ˆì„±ì„ ê°€ì •í•  ìˆ˜ ìˆìŒ (Comparable)\nÂ  Â  Â 2) ì ì¬ì ì¸Â Confounderë¥¼ í‰ê· ì ìœ¼ë¡œ ë™ì¼í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” íš¨ê³¼ (Confounder íš¨ê³¼ ì œê±°)\nÂ  Â  Â  Â  Â  â†’ ì´ë¡œ ì¸í•´, Causal Effect ì¸¡ì •ì´ ê°€ëŠ¥í•´ ì§‘ë‹ˆë‹¤!\nâ—¦Â  Randomization ì–´ë ¤ì›€ :Â ë§¤ìš° ì´ìƒì ì´ë‚˜, ì•„ë˜ 3ê°€ì§€ ì´ìœ ë¡œ í•­ìƒ Treatmentë¥¼ ëœë¤í™”í•˜ëŠ” ê±´ ì–´ë µìŠµë‹ˆë‹¤â€¦..\nÂ  Â  Â 1) ìœ¤ë¦¬ì  ì´ìœ  : ë‹´ë°°ë¥¼ í”¼ì§€ ì•ŠëŠ” ì‚¬ëŒì—ê²Œ, ì‹¤í—˜ì„ ìœ„í•´ ë‹´ë°°ë¥¼ í”¼ìš°ê²Œ í•œë‹¤ë©´???\nÂ  Â  Â 2) ì‹¤í–‰ ê°€ëŠ¥í•˜ì§€ ì•ŠìŒ : Country-levelì˜ ì‹¤í—˜ì¸ ê²½ìš°, ì „ì„¸ê³„ì˜ ëŒ€í†µë ¹ì´ ë˜ì–´ì•¼í•´ìš”â€¦\nÂ  Â  Â 3) ë¶ˆê°€ëŠ¥ : ì•”ì˜ íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´, íƒœì–´ë‚  ë•Œ ì‚¬ëŒì˜ DNAë¥¼ ë°”ê¾¸ëŠ” ê±´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤â€¦\nÂ  Â  Â 4) ë²ˆì™¸ë¡œ, A/B í…ŒìŠ¤íŠ¸ë¥¼ í•˜ëŠ”ë° ìì›(ì‹œê°„ê³¼ ë¹„ìš©)ì´ ë§ì´ ë“¤ì–´ê°€ìš”ã…œã…œ\n\n\n\nì•ìœ¼ë¡œ ë‚˜ì˜¬ ë‚´ìš© : ê´€ì¸¡ í™˜ê²½ì´ ì‹¤í—˜ í™˜ê²½ê³¼(RCT)ë¹„ìŠ·í•˜ê²Œ ë” ë§Œë“¤ì–´ ì£¼ëŠ” ê°€ì •ì— ëŒ€í•´ ë°°ì›ë‹ˆë‹¤.\n\nÂ  Â  Â  Â â†’ Identifiability Conditions (Causal Quantityì™€ Statistical Quantityê°€ ê°™ì•„ì§€ê¸° ìœ„í•œ ì¡°ê±´)\n\nâ—¦Â  Â Unconfoundedness : ì‹¤í—˜êµ°ê³¼ ëŒ€ì¡°êµ°ì€ êµí™˜ (ë¹„êµ) ê°€ëŠ¥!\nâ—¦Â  Â Positivity : Causal Effectë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ìˆ˜í•™ì ì¸ ê°€ì •!\nâ—¦Â  Â No Interference : ë‚˜ì˜ Outcomeì€ ë‹¤ë¥¸ ì‚¬ëŒì˜ Outcomeì— ì˜í–¥ì„ ë°›ì§€ ì•Šì•„ì•¼ í•¨!\nâ—¦Â  Â Consistency : Treatmentì— ëŒ€í•´ì„œëŠ” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤˜ì•¼ í•¨!\n\n\n\nb. Causal Models ê´€ì \n\nì •ì˜ : Causal Graph (DAG)ë¥¼ ë°”íƒ•ìœ¼ë¡œ, Causal Effectë¥¼ ë°”ë¼ë³´ëŠ” ê´€ì Â \nì§ˆë¬¸ :Â \n\n\n\nQ : Causal Modelì„ ë°”íƒ•ìœ¼ë¡œ Causal Effectë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ ì–´ë– í•œ ë°©ë²•ì´ í•„ìš”í• ê¹Œìš”?\nÂ  Â  Â A : Confounding Associationì´ ìƒê¸°ì§€ ì•Šë„ë¡ Confounderë¥¼ ì¡°ì ˆ/í†µì œí•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•´ìš”.\nÂ  Â  Â  Â  Â  ì•„ë˜ ê·¸ë¦¼ì€ Wê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Confounding Associationì´ ì‚¬ë¼ì§„ ë¶€ë¶„ì„ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤.Â \nÂ  Â  Â  Â  Â  ì´ ë•Œ, W (ê·¸ë¦¼ì—ì„œëŠ” C)ë¥¼ Sufficient adjustment setì´ë¼ê³  ì •ì˜í•´ìš”\nÂ  Â  â†’ Chapter 3, 4ì—ì„œ Confounderë¥¼ ì¡°ì ˆí•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤!Â (Back/Frontdoor Adjustment, Do-calculus)\n\nQ : ì¸ê³¼ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” êµ¬ì¡°ë¥¼ ë°œê²¬í•´ì•¼ í•  ê²ƒ ê°™ì€ë°, ì–´ë–»ê²Œ ë°œê²¬í•  ìˆ˜ ìˆë‚˜ìš”?\nÂ  Â  Â A : í•´ë‹¹ ë‚´ìš©ì€ Causal Discovery (Chapter 11/12)ì—ì„œ ê³µë¶€í•  ì˜ˆì •ì…ë‹ˆë‹¤.Â \n\n\nTo be continued) ì•ìœ¼ë¡œ ì¸ê³¼ì¶”ë¡ ì˜ Frameworkì™€ ì¸ê³¼ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•œ ë°©ë²•ì— ëŒ€í•´ ë°°ìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.\n\n\n\nReference\n\nâ—¦ Lecture Notes : 2021 Summer Session on Causal Inference (ë°•ì§€ìš© êµìˆ˜ë‹˜) [Link]\nâ—¦ Blog : Individualized treatment effect inference (van der Schaar êµìˆ˜ë‹˜, Figure1) [Link]\n\n\n\n\n\nCitationBibTeX citation:@online{shin2023,\n  author = {shin, Jinsoo},\n  title = {02\\textbackslash. {Motivation}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nshin, Jinsoo. 2023. â€œ02\\. Motivation.â€ November 14, 2023."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Estimation/Estimation.html",
    "href": "posts/Introduction_to_causal_inference_Estimation/Estimation.html",
    "title": "08. Estimation",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì˜ ë‚¨ê¶ë¯¼ìƒì…ë‹ˆë‹¤.\nIntroduction to Causal Inference ê°•ì˜ì˜ ì¼ê³± ë²ˆì§¸ ì±•í„°ì´ë©°, í•´ë‹¹ ì±•í„°ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#cate",
    "href": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#cate",
    "title": "08. Estimation",
    "section": "CATE",
    "text": "CATE\n\nATE\n\\[[\\tau(x)Â \\overset{\\Delta}{=}\\mathbb{E}[Y(1)-Y(0)\\,|\\,X=x]]\\]\nAssuming uncounfoundedness and positivity\n\\[[\\tauÂ \\overset{\\Delta}{=}Â \\mathbb{E}[Y(1)-Y(0)]=\\mathbb{E}_W[\\mathbb{E}[Y\\,|\\,Â T=1,W]-\\mathbb{E}[Y\\,|\\,Â T=0,W]]]\\]\nGiven W is a sufficient adjustment set\n\n\nCATE\n\\[\n\\begin{aligned}\n\\tau(x)Â &\\overset{\\Delta}{=}Â \\mathbb{E}[Y(1)-Y(0)\\,\\|\\,X=x]Â \\\\\\\n&=\\mathbb{E}_W[\\mathbb{E}[Y\\,\\|\\,Â T=1,X=x,W]-\\mathbb{E}[Y\\,\\|\\,Â T=0,X=x,W]]\n\\end{aligned}\n\\]\ngiven \\(W \\cup X\\) is a sufficient adjustment set\n[ê°œë…ì •ë¦¬]\n\nunconfoundedness = conditional exchangeability(ignorability)\n\n\\((Y(0),Y(1)) \\perp T\\,|\\,X\\)\nì´ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ potential outcomeì„ treatmentì— conditioningí•  ìˆ˜ ìˆìŒ\\[ \\begin{aligned} \\mathbb{E}[Y(1)-Y(0)|X] &= \\mathbb{E}[Y(1)|X]- \\mathbb{E}[Y(0)|X] \\\\ &= \\mathbb{E}[Y(1)|T=1,X]- \\mathbb{E}[Y(0)|T=0,X] \\\\ &= \\mathbb{E}[Y|T=1,X]- \\mathbb{E}[Y|T=0,X] \\end{aligned} \\]\n\npositivity\n\n\\(0&lt;P(T=1\\,|\\,X=x)&lt;1\\), \\(P(X=x)&gt;0\\), x for all \\(x\\)"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#com",
    "href": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#com",
    "title": "08. Estimation",
    "section": "COM",
    "text": "COM\nTarget of modeling: the conditional expectations of CATE\n\n\\(\\mu(1,W) = \\mathbb{E}[Y\\,|\\, T=1,W]\\)\n\\(\\mu(0,W) = \\mathbb{E}[Y\\,|\\, T=0,W]\\)\n\nì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ ì˜ˆì¸¡ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥\n\nCOM estimation of ATE\n\n[ = _i[(1,w_i)-(0,w_i)] ]\n\nCOM estimation of CATE\n\n\\[ \\begin{aligned} \\tau(x) &\\overset{\\Delta}{=} \\mathbb{E}[Y(1)-Y(0)\\,|\\,X=x] \\\\ &=\\mathbb{E}_W[\\mathbb{E}[Y\\,|\\, T=1,X=x,W]-\\mathbb{E}[Y\\,|\\, T=0,X=x,W]] \\end{aligned}\\]\ntarget of modeling:\n\\[ \\mu(t,x,w) \\overset{\\Delta}{=}\\mathbb{E}[Y\\,|\\, T=t,X=x,W=w] \\]\nCOM estimator of CATE:\n\\[ \\hat\\tau(x) = \\frac{1}{n_x}\\sum_{i:x_i=x}[\\hat\\mu(1,x,w_i)-\\hat\\mu(0,x,w_i)] \\]\nProblem with COM estimation in high dimensions\n\në§¤ìš° ì°¨ì›ì´ ë†’ì€ (input ë³€ìˆ˜ê°€ ë§ì€) ê²½ìš°, Tì˜ ì˜í–¥ë ¥ì´ ë‹¤ë¥¸ ë³€ìˆ˜ Wë“¤ì— ë¹„í•´ í¬ì§€ ì•Šìœ¼ë©´ Tì— ëŒ€í•œ weight ì—­ì‹œ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ì¶”ì •ëœë‹¤.\n\\(\\hat\\mu(1,w_i)-\\hat\\mu(0,w_i)\\) ê°€ 0ì— ë§¤ìš° ê°€ê¹Œì›Œì§\nê²°ë¡ : ì‹¤ì œ treatment effectê°€ ì¡´ì¬í•˜ë”ë¼ë„, scaleì˜ ì°¨ì´ ë•Œë¬¸ì— treatment ì¶”ì •ì¹˜ëŠ” 0ì— í¸í–¥ë  ìˆ˜ ìˆìŒ\n\nSolution: Grouped COM (GCOM) estimation\n\nCOM: \\(\\hat\\tau = \\frac{1}{n}\\sum_i[\\hat\\mu(1,w_i)-\\hat\\mu(0,w_i)]\\)\nGCOM: \\(\\hat\\tau = \\frac{1}{n}\\sum_i[\\hat\\mu_1(w_i)-\\hat\\mu_0(w_i)]\\)\n\nGCOMì˜ ê²½ìš°, TëŠ” ëª¨ë¸ì˜ inputìœ¼ë¡œ ë“¤ì–´ê°€ì§€ ì•ŠìŒ\nProblem: models have higher variance than they would if they were trained with all the data (since the splitted data might not efficient)"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#increasing-data-efficiency-tarnet-x-learner",
    "href": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#increasing-data-efficiency-tarnet-x-learner",
    "title": "08. Estimation",
    "section": "Increasing Data Efficiency: TARNet & X-Learner",
    "text": "Increasing Data Efficiency: TARNet & X-Learner\nTARNet\n\n\nNN ê¸°ë°˜ì¸ê±° ê°™ì€ë°,\n\nì¤‘ê°„ ëª¨ë¸: treatment-agnostic model; \\(\\hat \\mu\\)\nbranch model: treatment-specific model; T=1 ë°ì´í„°, T=0ì¸ ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµë¨\n\nì „ì²´ ëª¨ë¸ì´ ëª¨ë“  ë°ì´í„°ë¥¼ í™œìš©í•´ í•™ìŠµë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¯€ë¡œ ì—¬ì „íˆ data inefficiency ì¡´ì¬\n\nX-Learner\n\nEstimate \\(\\hat \\mu_1(x)\\) and \\(\\hat \\mu_0(x)\\) (assume \\(X\\) is a sufficient adjustment set and is all observed covariates)\nImpute ITEs\n\nTreatment group: \\(\\hat \\tau_{1,i}= Y_i(1)-\\hat \\mu_0(x_i)\\)\nControl group: \\(\\hat \\tau_{0,i}= \\hat \\mu_1(x_i) - Y_0(1)\\)\n\nFit a model \\(\\hat \\tau_1(x)\\) to predict \\(\\hat \\tau_{1,i}\\) from \\(x_i\\) in treatment group Fit a model \\(\\hat \\tau_0(x)\\) to predict \\(\\hat \\tau_{0,i}\\) from \\(x_i\\) in control group â†’ \\(\\hat \\tau_1(x)\\), \\(\\hat \\tau_0(x)\\)ëŠ” treatment/control groupì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ (GCOMì˜ ë¬¸ì œ í•´ê²°)\n\\(\\hat \\tau(x)=g(x)\\hat \\tau_0(x) + (1-g(x))\\hat \\tau_1(x)\\) where \\(g(x)\\) is a weight function btw 0 and 1 (e.g., propensity score)"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#propensity-score-ipw",
    "href": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#propensity-score-ipw",
    "title": "08. Estimation",
    "section": "Propensity Score & IPW",
    "text": "Propensity Score & IPW\nì§€ê¸ˆê¹Œì§€ëŠ” \\(\\mu(t,w)\\)ë¥¼ ëª¨ë¸ë§í•˜ì—¬ estimationì„ í–ˆìŠµë‹ˆë‹¤. ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” ê²½í–¥ ì ìˆ˜(propensity score)ë¼ëŠ” ê²ƒì„ ì´ìš©í•œ estimationì„ ì•Œì•„ë´…ì‹œë‹¤.\n\nê²½í–¥ ì ìˆ˜(Propensity score)ë€?\nìˆ˜í•™ì ìœ¼ë¡œ ë§í•˜ìë©´ ê²½í–¥ ì ìˆ˜ \\(e(w)\\)ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìŠ¤ì¹¼ë¼ ê°’ì…ë‹ˆë‹¤.\n\\[ e(w) \\triangleq P(T=1\\:|\\:W=w) \\]\nìš°ë¦¬ê°€ \\(W=w\\)ì¸ ì„ì˜ì˜ ì‚¬ë¡€ë¥¼ ê³¨ëì„ ë•Œ, í•´ë‹¹ ì¼€ì´ìŠ¤ê°€ ì²˜ì¹˜ ì§‘ë‹¨\\((T=1)\\)ì¼ ì¡°ê±´ë¶€ í™•ë¥ ì´ì£ .\nê·¸ë¦¬ê³  propensity score theoremì— ë”°ë¥´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì´ ì„±ë¦½í•©ë‹ˆë‹¤.\n\\[ (Y(1), Y(0))\\:{\\perp \\!\\!\\! \\perp}\\:T\\:|\\:W \\Rightarrow (Y(1), Y(0))\\:{\\perp \\!\\!\\! \\perp}\\:T\\:|\\:e(W) \\]\në¬¸ì ê·¸ëŒ€ë¡œ í’€ì–´ ì“°ìë©´, \\(W\\)ë¥¼ conditioning í–ˆì„ ë•Œ positivity, unconfoundednessê°€ ì„±ë¦½í•œë‹¤ë©´, \\(e(W)\\)ë¥¼ conditioning í–ˆì„ ë•Œë„ positivity, unconfoundednessê°€ ì„±ë¦½í•œë‹¤ëŠ” ì •ë¦¬ì…ë‹ˆë‹¤.\nì´ê²Œ ì™œ ì¤‘ìš”í• ê¹Œìš”? Chapter 2ì—ì„œ, ATEê°€ association differenceì™€ ê°™ì•„ì§€ë ¤ë©´ positivity, unconfoundednessê°€ ì„±ë¦½í•´ì•¼ í–ˆìŠµë‹ˆë‹¤. Propensity score Theoremì— ë”°ë¥´ë©´ \\(W\\)ë¥¼ conditioning í•´ì„œ ATEë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤ë©´, \\(e(W)\\)ë¥¼ conditioning í–ˆì„ ë•Œë„ ATEë¥¼ êµ¬í•  ìˆ˜ ìˆê²Œ ëœ ê±°ì£ !\nì•„ë˜ ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì´í•´ê°€ ì‰½ìŠµë‹ˆë‹¤.\n\nì™¼ìª½ ê·¸ë¦¼ì—ì„œ \\(W\\)ëŠ” \\(T\\)ì— causal effectë¥¼ ê°€ì§‘ë‹ˆë‹¤. ê·¸ effectëŠ” \\(e(W)\\)ì™€ ê°™ìœ¼ë¯€ë¡œ ì˜¤ë¥¸ìª½ê³¼ ê°™ì´ ê·¸ë¦´ ìˆ˜ë„ ìˆê² ì£ ? (\\(e(W)\\)ê°€ \\(W \\rightarrow T\\)ì˜ full mediator)\në”°ë¼ì„œ \\(W\\)ë¥¼ conditioningí•´ì„œ backdoorë¥¼ ë§‰ì„ ìˆ˜ ìˆë‹¤ë©´, \\(e(W)\\)ë¥¼ conditioiningí•´ì„œë„ ê°™ì€ íš¨ê³¼ë¥¼ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.\në¬¼ë¡  ìˆ˜ì‹ì„ ê¸¸ê²Œ ëŠ˜ì–´ë†“ì•„ ì¦ëª…í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤!\n\\[Â \\begin{aligned}P(T=1\\:|\\:Y(t),\\:e(W))\\:&=\\:E[T \\:|\\:Y(t),\\:e(W)] \\\\ &=E[E[T \\:|\\:Y(t),\\:e(W),W]\\:|\\:Y(t),e(W)] \\\\ &= E[E[T \\:|\\:Y(t),W]\\:|\\:Y(t),e(W)] \\\\ &= E[E[T \\:|\\:W]\\:|\\:Y(t),e(W)] \\\\ &= E[P(T=1\\:|\\:W)\\:|\\:Y(t),e(W)] \\\\ &= E[e(W)\\:|\\:Y(t),e(W)] \\\\ &= e(W)\\end{aligned} \\]\nì´ ê°’ì´Â \\(Y(t)\\)ì™€ ë…ë¦½ì´ë¯€ë¡œ\\((Y(1), Y(0)){\\perp \\!\\!\\! \\perp}T\\|e(W)\\)ì…ë‹ˆë‹¤.\n\nì•ì„  ì±•í„°ì—ì„œ positivity-unconfoundedness tradeoffë¥¼ ì´ì•¼ê¸°í–ˆëŠ”ë°, ê¸°ì–µ ë‚˜ì‹œë‚˜ìš”?.\në¹„êµì§‘ë‹¨ê³¼ ì²˜ì¹˜ì§‘ë‹¨ì„ ì œëŒ€ë¡œ ë¹„êµí•˜ë ¤ë©´ ê°™ì€ \\(W\\)ë¥¼ ê°€ì§„ ì§‘ë‹¨ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° \\(W\\)ì˜ ì°¨ì›ì´ ë†’ì•„ì§€ë©´ positivityê°€ ì‹¬ê°í•˜ê²Œ ì¤„ì–´ë“¤ì£ .\nê·¸ëŸ°ë° \\(W\\)ì˜ ì°¨ì›ì´ ì•„ë¬´ë¦¬ ë†’ì•„ì ¸ë„ \\(e(W)\\)ëŠ” 1ì°¨ì›ì˜ ìŠ¤ì¹¼ë¼ì…ë‹ˆë‹¤. ë”°ë¼ì„œ \\(e(W)\\)ë¥¼ conditioningí•˜ë©´ ì´ ë¬¸ì œë¥¼ ë§ˆë²•ì²˜ëŸ¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ê±°ì£ ! ì™€!\në¬¼ë¡  ì„¸ìƒì€ ê·¸ë ‡ê²Œ ì•„ë¦„ë‹µì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— ìš°ë¦¬ëŠ” \\(e(W)\\) í•¨ìˆ˜ë¥¼ ì•Œ ìˆ˜ ì—†ê±°ë“ ìš”. ë³´í†µì€ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ \\(e(W=w)\\)ë¥¼ êµ¬í•©ë‹ˆë‹¤.\n\nê¸°ì–µí•©ì‹œë‹¤!\nê²½í–¥ ì ìˆ˜ëŠ” covariateë¡œë¶€í„° ê³„ì‚°í•˜ëŠ” ìŠ¤ì¹¼ë¼ê°’ì…ë‹ˆë‹¤. unbiased estimate of ATEë¥¼ êµ¬í•˜ê³  ì‹¶ì„ ë•Œ, ê³ ì°¨ì›ì˜ \\(W\\)ë¥¼ conditioningí•˜ëŠ” ëŒ€ì‹  1ì°¨ì›ì˜ \\(e(W)\\)ë¥¼ conditioningí•˜ì—¬ ê°™ì€ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\nInverse Probability Weighting\në‹¤ìŒìœ¼ë¡œ IPWì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤. ê´€ì¸¡ì„ í†µí•´ associationì€ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ì´ë¡œë¶€í„° causationì„ ë½‘ì•„ë‚´ëŠ” ê²ƒì´ì£ . ê·¸ëŸ°ë° association == causationì´ë„ë¡ ë°ì´í„°ë¥¼ resamplingí•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.Â \n\n\\(W\\rightarrow T\\)ì˜ ì¸ê³¼ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì€ \\(T\\)ì˜ ë¶„í¬ê°€ \\(W\\)ì— ì˜í•´ ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ëœ»ì´ì£ . ìˆ˜í•™ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ìë©´ \\(P(T\\:|\\:W) \\neq P(T)\\). ê·¸ë ‡ë‹¤ë©´ ì´ë¥¼ ë’¤ì§‘ìœ¼ë©´ ì–´ë–¨ê¹Œìš”?\n\n\\(P(T\\:|\\:W) = P(T)\\)ì¸ ê²½ìš°\n\\(P(T\\:|\\:W)\\)ê°€ ìƒìˆ˜ì¸ ê²½ìš°\n\nìœ„ì™€ ê°™ì€ ê²½ìš°ì—ëŠ” \\(W\\)ê°€ \\(T\\)ì˜ ë¶„í¬ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ \\(T\\)ì™€ \\(Y\\) ì‚¬ì´ì˜ associationì€ ê³§ causationì´ê² ì£ .\n\nì •ë¦¬!\n\\(P(T|W)\\)ê°€ ìƒìˆ˜ë¼ë©´ \\(W=w\\) ê°’ì´ ë°”ë€Œì–´ë„ \\(T=t\\)ì˜ ë¶„í¬ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ë°ì´í„°ì— \\(\\frac{1}{P(t|W)}\\)ë¥¼ ê³±í•´ ë§Œë“  pseudo-populationì—ì„œëŠ” \\(W\\rightarrow (T=t)\\)ì˜ ì¸ê³¼ê°€ ëŠì–´ì§‘ë‹ˆë‹¤. (ì´ pseudo-populationì—ì„œì˜\n\\(P(T=t|W)=1\\)ì´ë‹ˆê¹Œìš”)\nê·¸ë˜ì„œ \\((T=t) \\rightarrow Y\\)ì˜ ì¸ê³¼ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´\n\\[ E[Y(t)]=E[\\frac{\\mathbb{1}(T=t)\\:Y}{P(t|W)}] \\]\në§ˆì°¬ê°€ì§€ë¡œ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤!\n\\[ \\begin{aligned} E[Y(t)] &= E[E[Y\\:|\\:t,W]] \\\\ &= \\sum_{w}{(\\sum_{y}{yP(y\\:|\\:t,w)})P(w)} \\\\ &= \\sum_{w}{\\sum_{y}{yP(y\\:|\\:t,w)}P(w)\\frac{P(t\\:|\\:w)}{P(t\\:|\\:w)}} \\\\ &= \\sum_{w}{\\sum_{y}{yP(y,t,w)}\\frac{1}{P(t\\:|\\:w)}} \\\\ &= \\sum_{w}{E[1(T=t,W=w)Y]\\frac{1}{P(t|w)}} \\\\ &= E[\\frac{1(T=t)\\:Y}{P(t|W)}]\\end{aligned} \\]\nê·¸ëŸ°ë° ì ê¹, re-weightingí•  ë•Œ ì“°ëŠ” \\(P(T=t\\:|\\:W)\\)ë¥¼ ë´…ì‹œë‹¤. ì–´ë”˜ê°€ ìµìˆ™í•œë°ìš”?\nTreatmentê°€ binaryí•˜ë‹¤ë©´\n\\[ P(T=1\\:|\\:W)=e(W) \\\\ P(T=0\\:|\\:W)=1-e(W) \\]\nì¦‰, ê²½í–¥ì ìˆ˜ë¥¼ ì´ìš©í•´ IPWë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n\n\nIPW ì ìš©í•˜ê¸°\nbinary treatmentë¥¼ ê°€ì •í–ˆì„ ë•Œ, ATEì˜ identification equationì€ ê²½í–¥ì ìˆ˜ë¥¼ ì´ìš©í•´ ì•„ë˜ì™€ ê°™ì´ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[ \\tau \\triangleq E[Y(1)-Y(0)]=E[\\frac{1(T=1)Y}{e(W)}]-E[\\frac{1(T=0)Y}{1-e(W)}] \\]\nì´ ë•Œ, ê²½í–¥ì ìˆ˜ê°€ 0ì´ë‚˜ 1ì— ì•„ì£¼ ê°€ê¹Œìš°ë©´ estimateì´ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•˜ê²Œ ë˜ì£ . ë”°ë¼ì„œ, ì ë‹¹í•œ ê°’ìœ¼ë¡œ trimì„ í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ë¬¼ë¡  ì´ ê²½ìš° biasì™€ ê°™ì€ ë¬¸ì œëŠ” ê°ì˜¤í•´ì•¼ í•©ë‹ˆë‹¤.\nìœ„ì˜ ì‹ì„ í™•ì¥í•´ CATEì— ëŒ€í•œ IPW estimatorë¥¼ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\\[ \\hat{\\tau}(x)=\\frac{1}{n_x}\\sum_{i:x_i = x}{(\\frac{1(t_i=1)y_i}{\\hat{e}(w_i)}-\\frac{1(t_i=0)y_i}{1-\\hat{e}(w_i)})} \\]\në‹¤ë§Œ ì´ ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ê°€ ë§ì§€ ì•Šì•„ varianceê°€ ì»¤ì§„ë‹¤ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤. ë” generalí•œ CATE IPW estimatorë„ ìˆì§€ë§Œ ë³¸ ê°•ì˜ì—ì„œëŠ” ë‹¤ë£¨ì§€ ì•Šê² ë‹¤ë„¤ìš”."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#ë˜-ë‹¤ë¥¸-ë°©ë²•ë“¤",
    "href": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#ë˜-ë‹¤ë¥¸-ë°©ë²•ë“¤",
    "title": "08. Estimation",
    "section": "ë˜ ë‹¤ë¥¸ ë°©ë²•ë“¤â€¦",
    "text": "ë˜ ë‹¤ë¥¸ ë°©ë²•ë“¤â€¦\nì´ë²ˆ ì¥ì—ì„œëŠ” causal effect estimationì„ ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ë‘ ê°€ì§€ ì†Œê°œí–ˆìŠµë‹ˆë‹¤.\n\n\\(\\mu(t,w) \\triangleq E[Y|t,w]\\)ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•\n\\(e(w) \\triangleq P(T=1|w)\\)ì„ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•\n\në§ˆì§€ë§‰ìœ¼ë¡œ ì—¬ê¸°ì„œ ë” ë‚˜ì•„ê°„ estimation ë°©ë²•ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n\nDoubly Robust Methods\n\\(\\mu(t,w)\\)ì™€ \\(e(w)\\)ë¥¼ ë‘˜ ë‹¤ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•. ì´ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n\n\\(\\hat{\\mu}(t,w)\\) ë˜ëŠ” \\(\\hat{e}(w)\\) ì¤‘ í•˜ë‚˜ë§Œ consistentí•´ë„ ì „ì²´ estimatorê°€ consistentí•©ë‹ˆë‹¤ (doubly robust)\nì´ë¡ ìƒìœ¼ë¡œëŠ” \\(\\tau\\)ì— ìˆ˜ë ´í•˜ëŠ” ì†ë„ê°€ COMì´ë‚˜ IPWë³´ë‹¤ ë¹ ë¦…ë‹ˆë‹¤ (\\(\\hat{\\mu} \\rightarrow \\mu\\)ì˜ ìˆ˜ë ´ ì†ë„ \\(\\times \\: \\hat{e} \\rightarrow e\\)ì˜ ìˆ˜ë ´ ì†ë„ì´ê¸° ë•Œë¬¸)\n\në‹¤ë§Œ \\(\\hat{\\mu}\\)ë‚˜ \\(\\hat{e}\\)ê°€ well-specified ë˜ì§€ ì•Šì•˜ì„ ë•Œ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ ë…¼ë€ì´ ìˆìŠµë‹ˆë‹¤.\n\n\nMatching\n\nTreatment groupê³¼ control groupì—ì„œ ë¹„ìŠ·í•œ ì‚¬ë¡€ë“¤ë§Œ ë¹„êµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. â€™ë¹„ìŠ·í•¨â€™ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëƒëŠ” ì‹¤í—˜ ì„¤ê³„ì— ë”°ë¼ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê²°ì •í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n\n\nDouble Machine Learning\n\nDouble machine learning ê¸°ë²•ì—ì„œëŠ” 3ê°€ì§€ì˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\nStage 1:\n\n\\(W\\)ë¡œë¶€í„° \\(T\\)ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ \\(\\hat{T}\\)ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸\n\\(W\\)ë¡œë¶€í„° \\(Y\\)ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ \\(\\hat{Y}\\)ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸\n\nStage 2:\n\n\\((T-\\hat{T})\\)ë¡œë¶€í„° \\((Y-\\hat{Y})\\)ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸\n\nì´ ë°©ë²•ì—ì„œëŠ” \\((T-\\hat{T})\\)ê³¼ \\((Y-\\hat{Y})\\)ë¥¼ ë³´ì•„ \\(W\\)ë¥¼ partial outí•©ë‹ˆë‹¤.\n\n\nCausal Trees and Forests\n\ndecision treeì™€ ë¹„ìŠ·í•˜ê²Œ ë°ì´í„°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë‚˜ëˆ”ìœ¼ë¡œì¨ ê°™ì€ treatment effectë¥¼ ê°€ì§„ subsetë“¤ì„ ë§Œë“œëŠ” ê¸°ë²•ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#ì°¸ê³ ìë£Œ",
    "href": "posts/Introduction_to_causal_inference_Estimation/Estimation.html#ì°¸ê³ ìë£Œ",
    "title": "08. Estimation",
    "section": "ì°¸ê³ ìë£Œ",
    "text": "ì°¸ê³ ìë£Œ\n\nThe Central Role of the Propensity Score in Observational Studies for Causal Effects"
  },
  {
    "objectID": "posts/Introduction_to_causal_inference_intro/index.html",
    "href": "posts/Introduction_to_causal_inference_intro/index.html",
    "title": "00. Casualí•˜ê²Œ Causality ì´í•´í•˜ê¸° ì†Œê°œ",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì…ë‹ˆë‹¤.\nì§€ë‚œ 3ê°œì›” ê°„ ê°€ì§œì—°êµ¬ì†Œì—ì„œ â€œCasualí•˜ê²Œ Causality ì´í•´í•˜ê¸°â€ ìŠ¤í„°ë””ê°€ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.\ní•´ë‹¹ ìŠ¤í„°ë””ë¥¼ í†µí•´ ì €í¬ê°€ ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ëª©í‘œëŠ” í¬ê²Œ 3ê°€ì§€ì…ë‹ˆë‹¤.\n\nâ—¦ Causal Inferenceì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ê°œë… í•™ìŠµ\nâ—¦ ë¬¸ì œ or ML ëª¨ë¸ì— ëŒ€í•œ ì›ì¸ ë¶„ì„ ëŠ¥ë ¥ í‚¤ìš°ê¸°\nâ—¦ ê³µë¶€í•œ ë‚´ìš©ì„ ì •ë¦¬í•´, Causal Inference ì— ëŒ€í•œ í•œêµ­ì–´ ìë£Œ ë§Œë“¤ì–´ë³´ê¸°\n\n\nì¸ê³¼ì¶”ë¡ ì„ ê³µë¶€í•˜ëŸ¬ ì˜¤ì‹  ë¶„ë“¤ì´ ì €í¬ê°€ ì‘ì„±í•œ ë¸”ë¡œê·¸ë¥¼ í†µí•´,\ní•œêµ­ì–´ ìë£Œê°€ ë§ì§€ ì•Šì€ ì¸ê³¼ì¶”ë¡ ì— ë” ì‰½ê²Œ ë‹¤ê°€ê°€ì…¨ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤!\n\n\nâ€œCasualí•˜ê²Œ Causality ì´í•´í•˜ê¸°â€ ìŠ¤í„°ë””ëŠ” Brady Nealì˜ Introduction to Causal Inference ê°•ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.\në¸”ë¡œê·¸ ê¸€ ì´ì „ì— ì „ì²´ ì±•í„°ì— ëŒ€í•œ ë…¸ì…˜ í˜ì´ì§€ ì •ë¦¬ ìë£Œë¥¼ ë³´ê³  ì‹¶ìœ¼ì‹  ë¶„ë“¤ì€ ê°€ì§œì—°êµ¬ì†Œ Causal Inference ì•„ì¹´ì´ë¸Œë¥¼ ì°¸ê³ í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!\n\n\n\nCitationBibTeX citation:@online{shin2023,\n  author = {shin, jinsoo},\n  title = {00\\textbackslash. {Casualí•˜ê²Œ} {Causality} {ì´í•´í•˜ê¸°} {ì†Œê°œ}},\n  date = {2023-11-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nshin, jinsoo. 2023. â€œ00\\. Casualí•˜ê²Œ Causality ì´í•´í•˜ê¸°\nì†Œê°œ.â€ November 14, 2023."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PseudoLab Causal Inference Team",
    "section": "",
    "text": "Randomised Controlled Trial\n\n\n\n\n\n\n\nA/B Test\n\n\nRCT\n\n\n\n\nìŠ¤í„°ë”” ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\njinsoo shin\n\n\n\n\n\n\n  \n\n\n\n\n12. Causal Discovery from Observational Data\n\n\n\n\n\n\n\nCausal Discovery\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 11 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nSangdon & Junyoung\n\n\n\n\n\n\n  \n\n\n\n\n11. Difference-in-Difference(DID)\n\n\n\n\n\n\n\nDifference-in-Difference\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 10 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nBrady Kim & Minsang Namgoong\n\n\n\n\n\n\n  \n\n\n\n\n10. Instrumental Variables\n\n\n\n\n\n\n\nInstrumental Variables\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 9 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nMinsang Namgoon & Hojae\n\n\n\n\n\n\n  \n\n\n\n\n09. Unobserved Confounding Analysis\n\n\n\n\n\n\n\nUnobserved Confounding Analysis\n\n\nSensitivity Analysis\n\n\nSensemakr\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 8 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nEunhee & Sangdon\n\n\n\n\n\n\n  \n\n\n\n\n08. Estimation\n\n\n\n\n\n\n\nEstimation\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 7 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nMinsang Namgoong\n\n\n\n\n\n\n  \n\n\n\n\n07. Nonparametric Identification\n\n\n\n\n\n\n\nNonparametric Identification\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 6 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nMinsang Namgoong\n\n\n\n\n\n\n  \n\n\n\n\n06. Randomised Experiments\n\n\n\n\n\n\n\nRandomised Experiments\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 5 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nsohee kim\n\n\n\n\n\n\n  \n\n\n\n\n05. Causal Models\n\n\n\n\n\n\n\nCausal Models\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 4 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nseongsoo kim & hojae jeong\n\n\n\n\n\n\n  \n\n\n\n\n04. Graphical Models\n\n\n\n\n\n\n\nGraphical Models\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 3 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nseongchul hong\n\n\n\n\n\n\n  \n\n\n\n\n03. Potential Outcomes\n\n\n\n\n\n\n\nPotential Outcomes\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 2 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nJinsoo shin\n\n\n\n\n\n\n  \n\n\n\n\n02. Motivation\n\n\n\n\n\n\n\nMotivation\n\n\n\n\nIntroduction to Causal Inference ê°•ì˜ chapter 1 ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\nJinsoo shin\n\n\n\n\n\n\n  \n\n\n\n\n01. Causal Inference ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë¦¬\n\n\n\n\n\n\n\npackages\n\n\nreference\n\n\n\n\nì¸ê³¼ì¶”ë¡  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\neverything\n\n\n\n\n\n\n  \n\n\n\n\n00. Casualí•˜ê²Œ Causality ì´í•´í•˜ê¸° ì†Œê°œ\n\n\n\n\n\n\n\nnews\n\n\n\n\nìŠ¤í„°ë”” ì†Œê°œ\n\n\n\n\n\n\nNov 14, 2023\n\n\njinsoo shin\n\n\n\n\n\n\nNo matching items"
  }
]