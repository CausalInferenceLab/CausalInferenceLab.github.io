---
title: "10\\. Instrumental Variables"
author: "Minsang Namgoon & Hojae"
description: |
  Introduction to Causal Inference ê°•ì˜ chapter 9 ì†Œê°œ   
date: now
categories: [Instrumental Variables]
---

ì•ˆë…•í•˜ì„¸ìš”, ê°€ì§œì—°êµ¬ì†Œ Causal Inference íŒ€ì˜ ë‚¨ê¶ë¯¼ìƒ, ì •í˜¸ì¬ì…ë‹ˆë‹¤.Â 

Introduction to Causal Inference ê°•ì˜ì˜ ì•„í™‰ë²ˆì§¸ ì±•í„°ì´ë©°, í•´ë‹¹ ì±•í„°ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

ê°•ì˜ ì˜ìƒ ë§í¬ : <https://youtu.be/B0SRWteGoOw>

ì‘ì„±ëœ ë‚´ìš© ì¤‘ ê°œì„ ì ì´ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”!

------------------------------------------------------------------------

## Contents

1.  Intro
2.  What is an Instrument?
3.  No Nonparametric Identification of the ATE
4.  Warm-Up: Binary Linear Setting
5.  Continuous Linear Setting
6.  Nonparametric Identification of the LATE
7.  IV in More General Settings

------------------------------------------------------------------------

## 1. Intro

Q : ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ ìš”ì¸ì´ ìˆì„ ë•Œ ì–´ë–»ê²Œ ì¸ê³¼ê´€ê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?

A : Frontdoor adjustment - Chap.5

-   ì¤‘ê°„ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ê³¼íš¨ê³¼ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•

A : Unconfounded children criterion - Chap.5

-   í•˜ë‚˜ì˜ conditioning setìœ¼ë¡œ ì²˜ì¹˜ë³€ìˆ˜ Tì˜ ìì† ì¤‘ì— ê²°ê³¼ë³€ìˆ˜ Yì˜ ì¡°ìƒì¸ ê²ƒë“¤ë¡œ í†µí•˜ëŠ” backdoor pathë¥¼ ëª¨ë‘ ë§‰ëŠ” ë°©ë²•

A : Some other fancy application of do-calculus - Chap.5

-   ê·¸ë˜í”„ê°€ ì•„ë‹Œ statistical quantityë¥¼ ì´ìš©í•œ ì¼ë°˜ì ì¸ ë°©ë²•

A : Set identification (bounds) - Chap.7

-   ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ ìš”ì¸ì˜ intervalì„ ì¢í˜€ë³´ëŠ” ë°©ë²•

A : Sensitivity analysis - Chap.7

-   ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì´ ì¡´ì¬í• ë•Œ ì •ëŸ‰ì ìœ¼ë¡œ ì˜í–¥ë ¥ì„ íŒë‹¨í•˜ëŠ” ë°©ë²•

**A : Instrumental Variables(â˜†)**

-   ë‹¤ë¥¸ ë³€ìˆ˜ë¡œ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ìš”ì¸ì˜ ì˜í–¥ì„ ì—†ì• ëŠ” ë°©ë²•

## 2. What is an Instrument?

ë„êµ¬ ë³€ìˆ˜ë€, ì²˜ì¹˜ë³€ìˆ˜ Tì™€ì—ëŠ” ì˜í–¥ì„ ì£¼ë©´ì„œ ê·¸ ì´ì™¸ì˜ ë³€ìˆ˜ì—ëŠ” ì˜í–¥ì„ ì£¼ê±°ë‚˜ ë°›ì§€ ì•ŠëŠ” ë³€ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

### 2.1 Assumption

**1. Relevance**

-   ë„êµ¬ë³€ìˆ˜ ZëŠ” ì²˜ì¹˜ë³€ìˆ˜ Tì— ì¸ê³¼ì  ì˜í–¥(ìƒê´€ì„± ì¡´ì¬)ì„ ë¼ì¹©ë‹ˆë‹¤.

![](img1.png)

**2. Exclusion Restriction**

::: {layout-ncol="2"}
![](img2.png)

![](img3.png)
:::

-   ê²°ê³¼ë³€ìˆ˜ Yì— ëŒ€í•œ ë„êµ¬ë³€ìˆ˜ Z ì¸ê³¼ì  ì˜í–¥ì€ ì²˜ì¹˜ë³€ìˆ˜ Tì— ì˜í•´ ì™„ì „íˆ ì¤‘ì¬ë©ë‹ˆë‹¤.
-   ë„êµ¬ ë³€ìˆ˜ Zê°€ ê²°ê³¼ë³€ìˆ˜ Yì— ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ìœ„í•´ì„  ë°˜ë“œì‹œ ì²˜ì¹˜ë³€ìˆ˜ Të¥¼ í†µí•´ì•¼ í•©ë‹ˆë‹¤.

**3. Instrumental Unconfoundedness**

-   ë„êµ¬ë³€ìˆ˜ Zì—ì„œ ê²°ê³¼ë³€ìˆ˜ Yê°„ì˜ backdoor pathsëŠ” ì—†ìŠµë‹ˆë‹¤.
-   ë„êµ¬ë³€ìˆ˜ Zì™€ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ ìš”ì¸ Uì˜ ê´€ê³„ëŠ” ì—†ìŠµë‹ˆë‹¤.

::: {layout="[[1,1], [1]]"}
![](img4.png)

![](img5.png)

![](img6.png)
:::

-   ë„êµ¬ë³€ìˆ˜ Zì™€ ê´€ì°°ë˜ì§€ ì•Šì€ êµë€ ìš”ì¸ Uì˜ ê´€ê³„ê°€ ì—†ìœ¼ë¯€ë¡œ ê´€ì°°ëœ êµë€ ë³€ìˆ˜(W)ë¥¼ ì°¨ë‹¨í•˜ì—¬ Instrumental variablesë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 3. No Nonparametric Identification of the ATE

Q : ë„êµ¬ ë³€ìˆ˜ê°€ ì¸ê³¼ê´€ê³„ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆë‹¤ë©´, ì™œ Chapter 6. Non-parametiric Identificationì—ì„œ ë³´ì§€ ì•Šì•˜ì„ê¹Œìš”?

A : ë„êµ¬ë³€ìˆ˜ëŠ” Non-parametiric Identification ë°©ë²•ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

-   ê°€ì •ì´ í•„ìš” ì—†ì„ë•Œ Non-parametiric Identificationì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ë° ë„êµ¬ë³€ìˆ˜ì—ëŠ” 3ê°€ì •ì´ ì¡´ì¬í•©ë‹ˆë‹¤.

\> (FYI) Nonparametric Identificationì„ ë§Œì¡±í•˜ëŠ” ì¡°ê±´

![](img7.png)

-   \[ì²˜ì¹˜ë³€ìˆ˜ T\]ì™€ \[ê²°ê³¼ë³€ìˆ˜ Yì˜ ancestorì´ë©´ì„œ ì²˜ì¹˜ë³€ìˆ˜ Tì˜ ìì‹ë…¸ë“œì¸ ì–´ëŠ ë…¸ë“œ\]ì™€ì˜ pathëŠ” ì°¨ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 4. Warm-Up: Binary Linear Setting

-   Assumption
    -   $Y:= \delta T + \alpha_u U$
-   Setting
    -   ì²˜ì¹˜ë³€ìˆ˜ Tì™€ ë„êµ¬ë³€ìˆ˜ ZëŠ” binary
-   Associational difference for the Z-Y relationship :

$E[Y | Z = 1] - E[Y | Z = 0]$

$= E[\delta T + \alpha_u U | Z = 1] -E[\delta T + \alpha_u U| Z = 0]$ â† exclusion restriction(2ë²ˆì§¸ê°€ì •) and linear outcome assumptions

$= \delta(E[T | Z = 1] - E[T | Z = 0]) + \alpha_u (E[U|Z=1] - E[U|Z=0])$

$= \delta(E[T | Z = 1] - E[T | Z = 0]) + \alpha_u (E[U] - E[U])$ â† instrumental unconfoundedness assumption(3ë²ˆì§¸ê°€ì •)

$= \delta(E[T | Z = 1] - E[T | Z = 0])$

-   Wald estimand :

$\delta=\frac{Cov(Y,Z) }{Cov(T,Z) }$

Relevance Assumptionìœ¼ë¡œ $E[T | Z = 1] \not= E[T | Z = 0]$ ì„ ë§Œì¡±í•©ë‹ˆë‹¤.

![](img8.png)

ë„êµ¬ë³€ìˆ˜ Zì—ì„œ ê²°ê³¼ë³€ìˆ˜ Yë¡œ ê°€ëŠ” backdoor pathëŠ” ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê° pathì˜ ì˜í–¥ì„ ì‚´í´ë´…ë‹ˆë‹¤.

ë˜í•œ Causal effectê°€ ìˆëŠ” directed pathë¥¼ ê³„ìˆ˜ë“¤ì˜ ê³±ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë¥¼ ëŒ€ì…í•˜ë©´ $\delta = \frac{\alpha_z \delta}{\alpha_z}$ ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

-   Wald estimator :

$\hat\delta=\frac{\frac{1}{n_1}\sum_{i:z_i=1}Y_i -\frac{1}{n_0}\sum_{i:z_i=0}Y_i }{\frac{1}{n_1}\sum_{i:z_i=1}T_i -\frac{1}{n_0}\sum_{i:z_i=0}T_i }$

Zâ†’Yì˜ ì˜í–¥ì„ êµ¬í•œ í›„ Zâ†’Tì˜ ì˜í–¥ì„ ë‚˜ëˆ„ì–´ì„œ ê³„ì‚°í•©ë‹ˆë‹¤.

## 5. Continuous Linear Setting

-   Assumption
    -   $Y:= \delta T + \alpha_u U$
-   Setting
    -   ì²˜ì¹˜ë³€ìˆ˜ Tì™€ ë„êµ¬ë³€ìˆ˜ ZëŠ” continuous
-   Associational difference for the Z-Y relationship :

$Cov(Y,Z) = E[YZ]E[Y]E[Z]$

$= E[(\delta T + \alpha_u U )Z] -E[\delta T + \alpha_u U]E[ Z]$ â† exclusion restriction(2ë²ˆì§¸ê°€ì •) and linear outcome assumptions

$= \delta E[TZ] + \alpha_u E[UZ] - \delta E[T]E[Z] - \alpha_u E[U]E[Z]$

$= \delta (E[TZ] - E[T]E[Z]) + \alpha_u (E[UZ] - E[U]E[Z])$

$= \delta Cov(T,Z) + \alpha_u Cov(U,Z)$

$= \delta Cov(T,Z)$ â† instrumental unconfoundedness assumption(3ë²ˆì§¸ê°€ì •)

-   Wald estimand :

$\delta=\frac{Cov(Y,Z) }{Cov(T,Z) }$

Relevance Assumptionìœ¼ë¡œ $Cov(T,Z) \not=0$ ì„ ë§Œì¡±í•©ë‹ˆë‹¤.

-   Wald estimator :

$\hat\delta=\frac{\hat{Cov}(Y,Z) }{\hat{Cov}(T,Z) }$

### **Two-stage least squares estimator**

1.  Linearly regress $T$ on $Z$ to estimate $E[T | Z ]$ . This gives us the projection of $T$ onto $Z$: $\hat T$.
2.  Linearly regress $Y$ on $\hat T$ to estimate $E[\hat T | Z ]$ . Obtain our estimate $\hat \delta$ as the fitted coefficient in front of $\hat T$.

::: {layout-ncol="2"}
![](img9.png)

![](img10.png)
:::

$\hat T$ëŠ” $U$ì— ëŒ€í•œ í•¨ìˆ˜ê°€ ì•„ë‹ˆë¯€ë¡œ $U$ì—ì„œ $T$ë¡œê°€ëŠ” pathê°€ ì‚¬ë¼ì ¸ì„œ backdoor pathê°€ ì œê±°ë©ë‹ˆë‹¤.

â€» ì˜ˆì‹œ

```         
import numpy as np
import pandas as pd
import statsmodels.formula.api as sm

np.random.seed(12345) # ë™ì¼í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ì‹œë“œ ì„¤ì •
num = 10000 # ë°ì´í„° ìˆ˜

U = np.random.normal(size = num) # Unobserved Factors
Z = np.random.normal(size = num) # Instrumental Variable

# TëŠ” Uì™€ Zì˜ ìì‹ë…¸ë“œ
T = 3.0*U + 6.0*Z + np.random.normal(size = num) # Treatment

# YëŠ” Tì™€ Uì˜ ì§€ì‹ë…¸ë“œ
Y = 15.0*U + 9.0*T + np.random.normal(size = num) # Outcome

data = pd.DataFrame({'T' : T, 'U' : U, 'Y' : Y})
```

-   ë‹¨ìˆœë¹„êµ

```         
# ë‹¨ìˆœ ë¹„êµ
sod_model = sm.ols('Y ~ T', data).fit()
# êµë€ ë³€ìˆ˜ë¥¼ ëˆ„ë½í•˜ì˜€ê¸°ì— ì¸ê³¼ íš¨ê³¼ê°€ ì˜ëª» ì¶”ì •ë¨
sod_model.summary().tables[1]
```

| Â          | coefÂ    | std err | Â t      | P\>\|t\| | \[0.025 | 0.975\] |
|-----------|---------|---------|---------|----------|---------|---------|
| Intercept | -0.1277 | 0.134   | -0.953  | 0.341    | -0.390  | 0.135   |
| T         | 9.9905  | 0.020   | 505.906 | 0.000    | 9.952   | 10.029  |

êµë€ë³€ìˆ˜ë¥¼ í†µì œí•˜ì§€ ì•Šì•˜ì„ ë•Œ ì¸ê³¼íš¨ê³¼ëŠ” 9.9905ë¡œ ë‚˜ì˜´ 0.9905ë§Œí¼ì˜ errorê°€ ë°œìƒí•©ë‹ˆë‹¤.

$Y = Intercept + T + e$

-   ë„êµ¬ë³€ìˆ˜ í™œìš©

$$ (T â†’ Y)ì˜ ì¸ê³¼íš¨ê³¼ = \frac {(Z â†’ Y) pathì˜ ì˜í–¥} {(Z â†’ T) pathì˜ ì˜í–¥} $$

```         
# first stage model(Z -> T)
Z_to_T = sm.ols('T ~ Z', data).fit()
# reduced model(Z -> Y)
Z_to_Y = sm.ols('Y ~ Z', data).fit()
```

```         
# first stage model
Z_to_T.summary().tables[1]
```

| Â          | coefÂ    | std err | Â t      | P\>\|t\| | \[0.025 | 0.975\] |
|-----------|---------|---------|---------|----------|---------|---------|
| Intercept | -0.0349 | 0.032   | -1.103  | 0.270    | -0.097  | 0.027   |
| T         | 6.0207  | 0.032   | 189.711 | 0.000    | 5.958   | 6.083   |

```         
# reduced model
Z_to_Y.summary().tables[1]
```

| Â          | coefÂ    | std err | Â t      | P\>\|t\| | \[0.025 | 0.975\] |
|-----------|---------|---------|---------|----------|---------|---------|
| Intercept | -0.4725 | 0.429   | -1.100  | 0.271    | -1.314  | 0.369   |
| T         | 54.3108 | 0.431   | 126.122 | 0.000    | 53.467  | 55.155  |

```         
Z_to_Y.params.Z/Z_to_T.params.Z
```

ê³„ì‚°ê²°ê³¼ 9.020754651013464ë¡œ ì‹¤ì œ ì¸ê³¼íš¨ê³¼ì¸ 9ì— ê·¼ì‚¬í•©ë‹ˆë‹¤.

-   2SLS

ì¸ê³¼ íš¨ê³¼ì˜ ìœ ì˜ì„±, ì‹ ë¢°êµ¬ê°„ì„ í™•ì¸í•˜ê¸° ìœ„í•´ 2SLSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```         
# first stage
t_hat = Z_to_T.predict()
data['T_hat'] =  t_hat

# second stage
That_to_Y = sm.ols('Y ~ T_hat', data).fit()
# second stage model
That_to_Y.summary().tables[1]
```

| Â          | coefÂ    | std err | Â t      | P\>\|t\| | \[0.025 | 0.975\] |
|-----------|---------|---------|---------|----------|---------|---------|
| Intercept | -0.1575 | 0.429   | -0.367  | 0.714    | -0.999  | 0.684   |
| T         | 9.0208  | 0.072   | 126.122 | 0.000    | 8.881   | 9.161   |

```         
That_to_Y.params.T_hat
```

ì¶”ì • ì¸ê³¼íš¨ê³¼ëŠ” 9.020754651013458ì´ë©°, p-valueê°€ ë§¤ìš° ì‘ì•„ ì¸ê³¼íš¨ê³¼ê°€ ìœ ì˜í•˜ë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 6. Nonparametric Identification of the LATE

ì•ì—ì„œ Linear settingì—ì„œ ATEë¥¼ ê³„ì‚°í•˜ëŠ” ë²•ì„ ë´¤ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì´ê±´ ë„ˆë¬´ ê°•ë ¥í•œ ê°€ì •ì…ë‹ˆë‹¤.

nonparametricí•œ ìƒí™©(ë¶„í¬ì— ëŒ€í•œ ê°€ì •ì´ ì—†ëŠ” ìƒí™©)ì—ì„œ ATEë¥¼ êµ¬í•  ìˆ˜ëŠ” ì—†ì„ê¹Œìš”? ì™„ì „í•œ ATEëŠ” ì•„ë‹ˆì§€ë§Œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

### IV ê´€ë ¨ Notation

ë³¸ê²©ì ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ì•ì„œ ëª‡ ê°€ì§€ í‘œê¸°ë²•ì„ ì •ë¦¬í•©ì‹œë‹¤

-   $Z$: ë„êµ¬ë³€ìˆ˜ (instrumental variable)
-   $T$: ì²˜ì¹˜ë³€ìˆ˜ (treatment variable)
-   $Y$: ê²°ê³¼, ì¢…ì†ë³€ìˆ˜ (dependent variable)

$Z \rightarrow T \rightarrow Y$ë¡œ ê°€ëŠ” ì¸ê³¼ê°€ ìˆë‹¤ê³  í•  ë•Œ,

-   $Y(1) \triangleq Y(T=1)$
-   $T(1) \triangleq T(Z=1)$
-   $Y(Z=1)$ì²˜ëŸ¼ $Z$ì— interveneí–ˆì„ ë•Œì˜ potential outcome $Y$ì˜ ê°’ì€ ë³„ë‹¤ë¥¸ ì¶•ì•½í˜•ì„ ê°€ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.

### Principal Strata

$Z$ê°€ $T$ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ì£¼ëŠëƒì— ë”°ë¼ ë°ì´í„°ë¥¼ 4ê°€ì§€ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

-   Complier: $Z$ê°€ ì‹œí‚¤ëŠ”ëŒ€ë¡œ í•˜ëŠ” ì‚¬ëŒë“¤.

$$ T(1)=T(Z=1)=1 \qquad T(0)=T(Z=0)=0 $$

-   Defier: $Z$ê°€ ì‹œí‚¤ëŠ” ê±° ë°˜ëŒ€ë¡œë§Œ í•˜ëŠ” ì²­ê°œêµ¬ë¦¬ë“¤.
-   $$ T(1)=T(Z=1)=0 \qquad T(0)=T(Z=0)=1 $$
-   Always-taker: $Z$ì— ìƒê´€ ì—†ì´ treatmentë¥¼ ë°›ëŠ” ì‚¬ëŒë“¤.
-   $$ T(1)=T(Z=1)=1 \qquad T(0)=T(Z=0)=1 $$
-   Never-taker: $Z$ì— ìƒê´€ ì—†ì´ treatmentë¥¼ ì•ˆ ë°›ëŠ” ì‚¬ëŒë“¤.
-   $$ T(1)=T(Z=1)=0 \qquad T(0)=T(Z=0)=0 $$

ğŸ’¡ ì´ ë•Œ, ìœ„ì— 2ê°œ ê·¸ë£¹ê³¼ ì•„ë˜ 2ê°œ ê·¸ë£¹ì€ ë‹¤ë¥¸ causal graphë¥¼ ê°€ì§‘ë‹ˆë‹¤.

![](img11.png)

### LATE êµ¬í•˜ê¸°

IVë¥¼ ì¨ë„, unobserved confoundingì´ ìˆë‹¤ë©´ nonparametricí•œ ìƒí™©ì—ì„œì˜ ATEë¥¼ êµ¬í•  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ì•½ê°„ì˜ ê°€ì •ì„ ì¶”ê°€í•´ì„œ **LATE (Local Average Treatment Effect)** ë˜ëŠ” **CACE (Complier Average Causal Effect)**ë¼ê³  ë¶ˆë¦¬ëŠ” ê±¸ êµ¬í•  ìˆ˜ëŠ” ìˆìŠµë‹ˆë‹¤.

ğŸ’¡ LATE (ë˜ëŠ” CACE)ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.

$\mathbb{E}[Y(T=1)-Y(T=0)\:|\:T(Z=1)=1,\:T(Z=0)=0]$

LATEë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” monotonicity(ë‹¨ì¡°ì„±)ì´ë¼ëŠ” ê°€ì •ì„ ë§ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤.

$\forall i, \space T_i(Z=1) \geq T_i(Z=0)$

ë‹¤ë¥´ê²Œ ë§í•˜ìë©´, ìš°ë¦¬ ë°ì´í„°ì— defierê°€ ì—†ë‹¤ëŠ” ê°€ì •ì…ë‹ˆë‹¤.

Relevance, exclusion restriction, instrumental unconfoundedness + monotonicity ê°€ì •ì´ ë§Œì¡±ë  ë•Œ, LATEëŠ” Wald estimandì™€ ê°™ìŠµë‹ˆë‹¤.

$\mathbb{E}[Y(1)-Y(0)\:|\:T(1)=1,\:T(0)=0]= \frac{\mathbb{E}[Y|Z=1]-\mathbb{E}[Y|Z=0]}{\mathbb{E}[T|Z=1]-\mathbb{E}[T|Z=0]}$

------------------------------------------------------------------------

-   í˜¹ì‹œ ìœ ë„ ê³¼ì •ì´ ì•Œê³  ì‹¶ë‚˜ìš”?

$Z$ê°€ $Y$ì— ë¼ì¹˜ëŠ” causal effectëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$E[Y(Z=1)-Y(Z=0)]$

ìš°ë¦¬ê°€ ì•ì„œ ë°°ì› ë˜ 4ê°€ì§€ ê·¸ë£¹ì„ ìƒê°í•´ë³¼ê¹Œìš”? $Z$ì™€ $T$ì˜ ê°’ì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì´ í’€ì–´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$\begin{aligned} \mathbb{E}[â€¦] &= \mathbb{E}[â€¦|T(1)=1,T(0)=0]\:P(T(1)=1,T(0)=0) \quad (complier) \\ &+ \mathbb{E}[â€¦|T(1)=0,T(0)=1]\:P(T(1)=0,T(0)=1) \quad (defier) \\ &+ \mathbb{E}[â€¦|T(1)=1,T(0)=1]\:P(T(1)=1,T(0)=1) \quad (always-taker) \\ &+ \mathbb{E}[â€¦|T(1)=0,T(0)=0]\:P(T(1)=0,T(0)=0) \quad (never-taker) \end{aligned}$

ì—¬ê¸°ì„œ ëª‡ ê°€ì§€ í•­ì€ ìë™ìœ¼ë¡œ ì†Œê±°ë©ë‹ˆë‹¤.

$\begin{aligned} \mathbb{E}[â€¦] &= \mathbb{E}[â€¦|T(1)=1,T(0)=0]\:P(T(1)=1,T(0)=0) \quad (complier) \\ &+ \mathbb{E}[â€¦|T(1)=0,T(0)=1]\:P(T(1)=0,T(0)=1) \quad (*defier) \\ &+ \mathbb{E}[â€¦|T(1)=1,T(0)=1]\:P(T(1)=1,T(0)=1) \quad (**always-taker) \\ &+ \mathbb{E}[â€¦|T(1)=0,T(0)=0]\:P(T(1)=0,T(0)=0) \quad (**never-taker) \end{aligned}$

\*Defierì˜ ê²½ìš°, monotonicity ê°€ì •ì— ì˜í•´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê¹”ë”í•˜ê²Œ ë¬´ì‹œ ê°€ëŠ¥í•©ë‹ˆë‹¤.

\*\*Always-takerì™€ \*\*Never-takerì˜ ê²½ìš°, $Z$ì™€ $T$ ì‚¬ì´ì— (ë‚˜ì•„ê°€ $Z$ì™€ $Y$ ì‚¬ì´ì—) causal effectê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ í•­ë“¤ë„ ê¹”ë”í•˜ê²Œ ë¬´ì‹œí•©ì‹œë‹¤.

ì´ë¥¼ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$\mathbb{E}[Y(Z=1)-Y(Z=0)|T(1)=1,T(0)=0]=\frac{\mathbb{E}[Y(Z=1)-Y(Z=0)]}{P(T(1)=1,T(0)=0)}$

ì´ ë•Œ, complierëŠ” $Z$ì˜ ê°’ê³¼ $T$ì˜ ê°’ì´ ê°™ìœ¼ë¯€ë¡œ, ì¢Œí•­ì˜ $Y(Z=0)$ê³¼ $Y(Z=1)$ë¥¼ $Y(T=0)$**ì™€** $Y(T=1)$ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  instrumental unconfoundednessì— ì˜í•´ ìš°í•­ë„ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$\mathbb{E}[Y(T=1)-Y(T=0)|T(1)=1,T(0)=0] =\frac{\mathbb{E}[Y|Z=1]-\mathbb{E}[Y|Z=0]}{P(T(1)=1,T(0)=0)}$

ì—¬ê¸°ì—ì„œ $P(T(1)=1,T(0)=0)$ë¥¼ ìì„¸íˆ ì‚´í´ë´…ì‹œë‹¤. ì „ì²´ ì§‘ë‹¨ì—ì„œ $(T=1|Z=0)$ì¸ ì§‘ë‹¨ê³¼ $(T=0|Z=1)$ì¸ ì§‘ë‹¨ì„ ì œì™¸í•œ ê²Œ $(T(1)=1, T(0)=0)$ì¸ ì§‘ë‹¨ì´ê² ì£ ? ë”°ë¼ì„œ ìš°í•­ì€ ì•„ë˜ì™€ ê°™ì´ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$\begin{aligned} &=\frac{\mathbb{E}[Y|Z=1]-\mathbb{E}[Y|Z=0]}{1-P(T=1|Z=0)-P(T=0|Z=1)} \\ &=\frac{\mathbb{E}[Y|Z=1]-\mathbb{E}[Y|Z=0]}{1-P(T=1|Z=0)-(1-P(T=1|Z=1))} \\ &=\frac{\mathbb{E}[Y|Z=1]-\mathbb{E}[Y|Z=0]}{P(T=1|Z=1)-P(T=1|Z=0)} \end{aligned}$

ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ, $T$ê°€ binaryì´ë¯€ë¡œ $T=1$ì— ëŒ€í•œ í™•ë¥ ì€ ê¸°ëŒ€ê°’ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$\begin{aligned} &=\frac{\mathbb{E}[Y|Z=1]-\mathbb{E}[Y|Z=0]}{P(T=1|Z=1)-P(T=1|Z=0)} \\ &= \frac{\mathbb{E}[Y|Z=1]-\mathbb{E}[Y|Z=0]}{\mathbb{E}[T|Z=1]-\mathbb{E}[T|Z=0]} \quad (Wald\;estimand) \end{aligned}$

------------------------------------------------------------------------

ğŸ’¡ ê´€ì ì„ ì•½ê°„ ë°”ê¿” ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

**Q. Wald estimand (**$Z\rightarrow Y$ ì¸ê³¼ / $Z\rightarrow T$ ì¸ê³¼) ëŠ” ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì¼ê¹Œìš”?

â†’ $Z$ê°€ $T$ì— ì–´ë–¤ ì‹ìœ¼ë¡œ ì˜í–¥ì„ ì£¼ëŠëƒì— ë”°ë¼, ì§‘ë‹¨ì„ 4ê°œ sub-populationìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Wald estimandëŠ” ì´ ì¤‘ complier ì§‘ë‹¨ì˜ ATE.

â†’ ì¢€ ë” ê°•í•œ ê°€ì • ($T$ì™€ $Y$ê°€ linearityí•œ ê´€ê³„) ì´ ë§Œì¡±í•  ë•ŒëŠ”, ì „ì²´ ì§‘ë‹¨ì˜ ATEë¡œ ìƒê°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

LATEì—ë„ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤

-   monotonicityê°€ í•­ìƒ ì¶©ì¡±ë˜ëŠ” ê±´ ì•„ë‹™ë‹ˆë‹¤.
-   ìƒí™©ì— ë”°ë¼, ì „ì²´ ì§‘ë‹¨ì˜ ATEê°€ í•„ìš”í•˜ì§€ Local ATEê°€ ê¶ê¸ˆí•˜ì§€ ì•Šì€ ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤.

## 7. IV in More General Settings

ì•ì„œì„œ $Y$ê°€ $T$ì— ëŒ€í•œ linear equationìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ê²½ìš°ë¥¼ ë‹¤ë¤˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì¢€ ë” í™•ì¥í•´ì„œ $Y$ê°€ $T$ì— ëŒ€í•´ ì¢€ ë” ë³µì¡í•œ í•¨ìˆ˜ë¡œ í‘œí˜„ë˜ëŠ” ê²½ìš°ë„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$ Y:=f(T,W)+U $$

ìœ„ì™€ ê°™ì´ ë‚˜íƒ€ë‚´ë©°, ë”¥ëŸ¬ë‹ ë“±ì„ ì´ìš©í•´ $f$ë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
